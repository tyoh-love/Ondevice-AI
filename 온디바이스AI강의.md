# 2025ë…„ ì˜¨ë””ë°”ì´ìŠ¤ AI 3ì¼ ì§‘ì¤‘ ì‹¤ìŠµ êµì¬ - í™•ì¥íŒ
### ì´ˆë³´ìë¶€í„° ì¤‘ê¸‰ìê¹Œì§€ ë”°ë¼í•˜ëŠ” ìµœì‹  AI ê¸°ìˆ  ì™„ë²½ ê°€ì´ë“œ

---

## ğŸ¯ ì´ êµì¬ì˜ ëª©í‘œ

ì•ˆë…•í•˜ì„¸ìš”! ì´ êµì¬ëŠ” AIì— ê´€ì‹¬ì´ ìˆì§€ë§Œ ì–´ë””ì„œë¶€í„° ì‹œì‘í•´ì•¼ í• ì§€ ë§‰ë§‰í•œ ë¶„ë“¤ì„ ìœ„í•´ ë§Œë“¤ì–´ì¡ŒìŠµë‹ˆë‹¤. 3ì¼ ë™ì•ˆ ì—¬ëŸ¬ë¶„ì€ ìµœì‹  AI ê¸°ìˆ ì„ ì§ì ‘ ë§Œì ¸ë³´ê³ , ì‹¤ì œë¡œ ì‘ë™í•˜ëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë§Œë“¤ì–´ë³¼ ê²ƒì…ë‹ˆë‹¤.

**ìš°ë¦¬ê°€ í•¨ê»˜ ë§Œë“¤ ê²ƒë“¤:**
- ğŸ“¸ ì‚¬ì§„ì„ ë³´ê³  ì„¤ëª…í•´ì£¼ëŠ” AI
- ğŸ¥ ì‹¤ì‹œê°„ìœ¼ë¡œ ë¬¼ì²´ë¥¼ ì°¾ì•„ë‚´ëŠ” ì‹œìŠ¤í…œ
- ğŸ—£ï¸ ìŒì„±ìœ¼ë¡œ ëŒ€í™”í•˜ëŠ” AI ë¹„ì„œ

**ì¤€ë¹„ë¬¼:**
- ë…¸íŠ¸ë¶ (NVIDIA GPUê°€ ìˆìœ¼ë©´ ì¢‹ì§€ë§Œ, ì—†ì–´ë„ ê´œì°®ìŠµë‹ˆë‹¤)
- Python ê¸°ì´ˆ ì§€ì‹ (ë³€ìˆ˜, í•¨ìˆ˜, ë°˜ë³µë¬¸ ì •ë„ë§Œ ì•Œë©´ ì¶©ë¶„í•©ë‹ˆë‹¤)
- í˜¸ê¸°ì‹¬ê³¼ ì—´ì •!

---

## ğŸ“š ëª©ì°¨

### Day 1: ë©€í‹°ëª¨ë‹¬ AI - ë³´ê³  ì´í•´í•˜ëŠ” ì¸ê³µì§€ëŠ¥
- 1ì¥: ê°œë°œ í™˜ê²½ êµ¬ì¶• (ì´ˆë³´ìë„ OK!)
- 2ì¥: ë©€í‹°ëª¨ë‹¬ AIë€ ë¬´ì—‡ì¸ê°€? (ìƒì„¸ ì´ë¡ )
- 3ì¥: ë‹¤ì–‘í•œ ìµœì‹  ëª¨ë¸ ì‹¤ìŠµ
- 4ì¥: ì‹¤ì „ í”„ë¡œì íŠ¸ - ë‚˜ë§Œì˜ ì´ë¯¸ì§€ ì„¤ëª… AI

### Day 2: ì»´í“¨í„° ë¹„ì „ - ì‹¤ì‹œê°„ìœ¼ë¡œ ì„¸ìƒì„ ì¸ì‹í•˜ê¸°
- 5ì¥: YOLOì˜ ëª¨ë“  ê²ƒ (ì—­ì‚¬ë¶€í„° ìµœì‹  ë²„ì „ê¹Œì§€)
- 6ì¥: ê°ì²´ ì¶”ì ê³¼ í–‰ë™ ì¸ì‹
- 7ì¥: ì‹¤ì „ í”„ë¡œì íŠ¸ - ë˜‘ë˜‘í•œ ê°ì‹œ ì‹œìŠ¤í…œ

### Day 3: AI ì—ì´ì „íŠ¸ - ëª¨ë“  ê²ƒì„ í†µí•©í•˜ë‹¤
- 8ì¥: ëª¨ë¸ ìµœì í™”ì˜ ë§ˆë²•
- 9ì¥: ìŒì„± ì¸í„°í˜ì´ìŠ¤ êµ¬ì¶•
- 10ì¥: ìµœì¢… í”„ë¡œì íŠ¸ - ìŒì„± ëŒ€í™”í˜• AI ë¹„ì„œ

#
#
#




---

# ğŸš€ Day 1: ë©€í‹°ëª¨ë‹¬ AI - ë³´ê³  ì´í•´í•˜ëŠ” ì¸ê³µì§€ëŠ¥

## 1ì¥: ê°œë°œ í™˜ê²½ êµ¬ì¶• - ì´ˆë³´ìë„ ê±±ì • ì—†ì–´ìš”!

### 1.1 ì™œ í™˜ê²½ êµ¬ì¶•ì´ ì¤‘ìš”í•œê°€ìš”?

AI ê°œë°œì„ ìš”ë¦¬ì— ë¹„ìœ í•´ë³´ê² ìŠµë‹ˆë‹¤. ë§›ìˆëŠ” ìš”ë¦¬ë¥¼ í•˜ë ¤ë©´ ì¢‹ì€ ì£¼ë°©ê³¼ ë„êµ¬ê°€ í•„ìš”í•˜ë“¯ì´, AI ê°œë°œì—ë„ ì ì ˆí•œ ê°œë°œ í™˜ê²½ì´ í•„ìš”í•©ë‹ˆë‹¤. ì˜ êµ¬ì¶•ëœ í™˜ê²½ì€ ì—¬ëŸ¬ë¶„ì˜ ê°œë°œ ì†ë„ë¥¼ 10ë°° ì´ìƒ ë¹ ë¥´ê²Œ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤!

### 1.2 ë‚´ ì»´í“¨í„°ëŠ” ì–´ë–¤ íƒ€ì…ì¼ê¹Œ?

ë¨¼ì € ì—¬ëŸ¬ë¶„ì˜ ì»´í“¨í„°ê°€ ì–´ë–¤ ì¢…ë¥˜ì¸ì§€ í™•ì¸í•´ë´…ì‹œë‹¤:

```python
# ë‚´ ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸í•˜ê¸°
import platform
import subprocess
import sys

def check_system_info():
    """ì‹œìŠ¤í…œ ì •ë³´ë¥¼ ì¹œì ˆí•˜ê²Œ ì•Œë ¤ì£¼ëŠ” í•¨ìˆ˜"""
    
    print("ğŸ–¥ï¸ ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸ ì¤‘...")
    print("=" * 50)
    
    # ìš´ì˜ì²´ì œ í™•ì¸
    os_info = platform.system()
    os_version = platform.version()
    print(f"ğŸ“Œ ìš´ì˜ì²´ì œ: {os_info} {os_version}")
    
    # Python ë²„ì „ í™•ì¸
    python_version = sys.version.split()[0]
    print(f"ğŸ Python ë²„ì „: {python_version}")
    
    # CPU ì •ë³´
    processor = platform.processor()
    print(f"ğŸ’» í”„ë¡œì„¸ì„œ: {processor}")
    
    # GPU í™•ì¸ (NVIDIA)
    try:
        nvidia_smi = subprocess.check_output(['nvidia-smi', '--query-gpu=name', '--format=csv,noheader'], 
                                           encoding='utf-8').strip()
        print(f"ğŸ® NVIDIA GPU: {nvidia_smi}")
    except:
        print("ğŸ® NVIDIA GPU: ê°ì§€ë˜ì§€ ì•ŠìŒ")
    
    # ë©”ëª¨ë¦¬ í™•ì¸
    try:
        import psutil
        memory = psutil.virtual_memory()
        print(f"ğŸ’¾ ë©”ëª¨ë¦¬: {memory.total / (1024**3):.1f}GB ì¤‘ {memory.available / (1024**3):.1f}GB ì‚¬ìš© ê°€ëŠ¥")
    except:
        print("ğŸ’¾ ë©”ëª¨ë¦¬: psutilì„ ì„¤ì¹˜í•˜ë©´ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤")
    
    print("=" * 50)
    
    # ì¶”ì²œì‚¬í•­ ì œê³µ
    if 'NVIDIA' in str(processor) or 'nvidia_smi' in locals():
        print("âœ… GPUê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤! ë¹ ë¥¸ AI í•™ìŠµì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    else:
        print("ğŸ’¡ GPUê°€ ì—†ì–´ë„ ê´œì°®ìŠµë‹ˆë‹¤. CPUë¡œë„ ì¶©ë¶„íˆ ì‹¤ìŠµ ê°€ëŠ¥í•©ë‹ˆë‹¤!")
    
    if float(python_version[:3]) < 3.8:
        print("âš ï¸ Python 3.8 ì´ìƒìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
    else:
        print("âœ… Python ë²„ì „ì´ ì ì ˆí•©ë‹ˆë‹¤.")

# ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸ ì‹¤í–‰
check_system_info()
```

### 1.3 Conda ì„¤ì¹˜í•˜ê¸° - ê°€ìƒ í™˜ê²½ì˜ ë§ˆë²•

**Condaë€?** 
CondaëŠ” Python íŒ¨í‚¤ì§€ë“¤ì„ ê´€ë¦¬í•´ì£¼ëŠ” ë„êµ¬ì…ë‹ˆë‹¤. ë§ˆì¹˜ ìŠ¤ë§ˆíŠ¸í°ì˜ ì•±ìŠ¤í† ì–´ì²˜ëŸ¼, í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‰½ê²Œ ì„¤ì¹˜í•˜ê³  ê´€ë¦¬í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.

**ì™œ ê°€ìƒ í™˜ê²½ì„ ì‚¬ìš©í•˜ë‚˜ìš”?**
ì—¬ëŸ¬ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•˜ë‹¤ ë³´ë©´ ê° í”„ë¡œì íŠ¸ë§ˆë‹¤ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°€ìƒ í™˜ê²½ì€ ê° í”„ë¡œì íŠ¸ë¥¼ ìœ„í•œ ë…ë¦½ëœ ê³µê°„ì„ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤.

```bash
# Miniconda ì„¤ì¹˜ (ê°€ë²¼ìš´ ë²„ì „ì˜ Conda)
# Windows ì‚¬ìš©ìëŠ” ë‹¤ìš´ë¡œë“œ í›„ ì„¤ì¹˜:
# https://docs.conda.io/en/latest/miniconda.html

# Mac/Linux ì‚¬ìš©ì:
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-$(uname -s)-$(uname -m).sh
bash Miniconda3-latest-*.sh

# ì„¤ì¹˜ í™•ì¸
conda --version
```

### 1.4 ìš°ë¦¬ì˜ ì²« ë²ˆì§¸ AI í™˜ê²½ ë§Œë“¤ê¸°

ì´ì œ ë³¸ê²©ì ìœ¼ë¡œ AI ê°œë°œì„ ìœ„í•œ í™˜ê²½ì„ ë§Œë“¤ì–´ë´…ì‹œë‹¤:

```bash
# 1. ìƒˆë¡œìš´ ê°€ìƒ í™˜ê²½ ìƒì„±
# 'ondevice-ai'ëŠ” ìš°ë¦¬ê°€ ë§Œë“¤ í™˜ê²½ì˜ ì´ë¦„ì…ë‹ˆë‹¤
conda create -n ondevice-ai python=3.11 -y

# 2. í™˜ê²½ í™œì„±í™”
# ì´ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ë©´ í”„ë¡¬í”„íŠ¸ê°€ (ondevice-ai)ë¡œ ë°”ë€ë‹ˆë‹¤
conda activate ondevice-ai

# 3. í™˜ê²½ì´ ì˜ í™œì„±í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸
python --version  # Python 3.11.xê°€ ë‚˜ì™€ì•¼ í•©ë‹ˆë‹¤
```

### 1.5 í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ - AIì˜ ë„êµ¬ìƒì

ì´ì œ AI ê°œë°œì— í•„ìš”í•œ ë„êµ¬ë“¤ì„ ì„¤ì¹˜í•´ë´…ì‹œë‹¤. ê° ë„êµ¬ê°€ ë¬´ì—‡ì„ í•˜ëŠ”ì§€ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤:

```python
# requirements.txt íŒŒì¼ ìƒì„±
requirements_content = """
# í•µì‹¬ AI í”„ë ˆì„ì›Œí¬
torch==2.5.0          # ë”¥ëŸ¬ë‹ì˜ ì‹¬ì¥! ëª¨ë“  AI ëª¨ë¸ì˜ ê¸°ë°˜
torchvision==0.20.0   # ì´ë¯¸ì§€ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë„êµ¬ ëª¨ìŒ
transformers==4.46.0  # ìµœì‹  AI ëª¨ë¸ë“¤ì„ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬

# ëª¨ë¸ ìµœì í™”
accelerate==1.0.0     # GPUë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•˜ê²Œ í•´ì¤ë‹ˆë‹¤
bitsandbytes==0.44.0  # ëª¨ë¸ì„ ê°€ë³ê²Œ ë§Œë“¤ì–´ì£¼ëŠ” ë§ˆë²•ì˜ ë„êµ¬

# ì´ë¯¸ì§€ ì²˜ë¦¬
opencv-python==4.9.0.80  # ì»´í“¨í„° ë¹„ì „ì˜ ìŠ¤ìœ„ìŠ¤ êµ°ìš© ì¹¼
pillow==10.2.0          # ì´ë¯¸ì§€ íŒŒì¼ì„ ë‹¤ë£¨ëŠ” ê¸°ë³¸ ë„êµ¬
supervision==0.22.0     # ê°ì²´ íƒì§€ ê²°ê³¼ë¥¼ ì˜ˆì˜ê²Œ ì‹œê°í™”

# ì˜¤ë””ì˜¤ ì²˜ë¦¬
openai-whisper==20231117  # OpenAIì˜ ìŒì„± ì¸ì‹ ëª¨ë¸
gtts==2.5.0              # í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜
pygame==2.5.2            # ì†Œë¦¬ë¥¼ ì¬ìƒí•˜ëŠ” ë„êµ¬
sounddevice==0.4.6       # ë§ˆì´í¬ë¡œ ë…¹ìŒí•˜ê¸°

# ìœ í‹¸ë¦¬í‹°
numpy==1.26.4      # ìˆ«ì ê³„ì‚°ì˜ ê¸°ë³¸
scipy==1.12.0      # ê³¼í•™ ê³„ì‚° ë„êµ¬
pandas==2.2.0      # ë°ì´í„° ë¶„ì„ì˜ í•„ìˆ˜í’ˆ
matplotlib==3.8.3  # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
tqdm==4.66.2       # ì§„í–‰ ìƒí™©ì„ ë³´ì—¬ì£¼ëŠ” í”„ë¡œê·¸ë ˆìŠ¤ ë°”

# ì›¹ ì¸í„°í˜ì´ìŠ¤
gradio==4.16.0     # ì›¹ UIë¥¼ ì‰½ê²Œ ë§Œë“¤ì–´ì£¼ëŠ” ë„êµ¬
fastapi==0.109.0   # API ì„œë²„ êµ¬ì¶•
uvicorn==0.27.0    # FastAPI ì„œë²„ ì‹¤í–‰
"""

# íŒŒì¼ ì €ì¥
with open('requirements.txt', 'w') as f:
    f.write(requirements_content)

print("requirements.txt íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
```

ì´ì œ ì„¤ì¹˜í•´ë´…ì‹œë‹¤:

```bash
# NVIDIA GPUê°€ ìˆëŠ” ê²½ìš°
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install -r requirements.txt

# Mac (Apple Silicon) ì‚¬ìš©ì
pip install torch torchvision torchaudio
pip install -r requirements.txt

# GPUê°€ ì—†ëŠ” ê²½ìš° (CPUë§Œ ì‚¬ìš©)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
pip install -r requirements.txt
```

### 1.6 ì„¤ì¹˜ í™•ì¸ - ëª¨ë“  ê²ƒì´ ì˜ ì‘ë™í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸

```python
# ì„¤ì¹˜ í™•ì¸ ìŠ¤í¬ë¦½íŠ¸
def test_installation():
    """ì„¤ì¹˜ê°€ ì œëŒ€ë¡œ ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ì¹œì ˆí•œ í•¨ìˆ˜"""
    
    print("ğŸ” ì„¤ì¹˜ í™•ì¸ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n")
    
    # 1. PyTorch í™•ì¸
    try:
        import torch
        print(f"âœ… PyTorch {torch.__version__} ì„¤ì¹˜ ì™„ë£Œ!")
        
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            print(f"   ğŸ® GPU ì‚¬ìš© ê°€ëŠ¥: {gpu_name}")
            print(f"   ğŸ’¾ GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
        elif torch.backends.mps.is_available():
            print("   ğŸ Apple Silicon GPU ì‚¬ìš© ê°€ëŠ¥!")
        else:
            print("   ğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤ (GPUë³´ë‹¤ ëŠë¦¬ì§€ë§Œ ì‹¤ìŠµì—ëŠ” ë¬¸ì œì—†ìŠµë‹ˆë‹¤)")
    except ImportError:
        print("âŒ PyTorch ì„¤ì¹˜ ì‹¤íŒ¨")
    
    # 2. Transformers í™•ì¸
    try:
        import transformers
        print(f"\nâœ… Transformers {transformers.__version__} ì„¤ì¹˜ ì™„ë£Œ!")
    except ImportError:
        print("\nâŒ Transformers ì„¤ì¹˜ ì‹¤íŒ¨")
    
    # 3. OpenCV í™•ì¸
    try:
        import cv2
        print(f"\nâœ… OpenCV {cv2.__version__} ì„¤ì¹˜ ì™„ë£Œ!")
    except ImportError:
        print("\nâŒ OpenCV ì„¤ì¹˜ ì‹¤íŒ¨")
    
    # 4. ê°„ë‹¨í•œ AI ëª¨ë¸ í…ŒìŠ¤íŠ¸
    print("\nğŸ§ª ê°„ë‹¨í•œ AI í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
    try:
        import torch
        import torch.nn as nn
        
        # ì•„ì£¼ ì‘ì€ ì‹ ê²½ë§ ë§Œë“¤ê¸°
        class TinyNet(nn.Module):
            def __init__(self):
                super().__init__()
                self.layer = nn.Linear(10, 1)
            
            def forward(self, x):
                return self.layer(x)
        
        # ëª¨ë¸ ìƒì„± ë° í…ŒìŠ¤íŠ¸
        model = TinyNet()
        test_input = torch.randn(1, 10)
        output = model(test_input)
        
        print("âœ… AI ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        print(f"   ì…ë ¥ í¬ê¸°: {test_input.shape}")
        print(f"   ì¶œë ¥ ê°’: {output.item():.4f}")
        
    except Exception as e:
        print(f"âŒ AI ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    print("\nğŸ‰ í™˜ê²½ ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì´ì œ AI ê°œë°œì„ ì‹œì‘í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.")

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
test_installation()
```

---

## 2ì¥: ë©€í‹°ëª¨ë‹¬ AIë€ ë¬´ì—‡ì¸ê°€? - ì¸ê°„ì²˜ëŸ¼ ë³´ê³  ë“£ê³  ì´í•´í•˜ëŠ” AI

### 2.1 ë©€í‹°ëª¨ë‹¬ AIì˜ íƒ„ìƒ ë°°ê²½

ì—¬ëŸ¬ë¶„, ìš°ë¦¬ ì¸ê°„ì€ ì–´ë–»ê²Œ ì„¸ìƒì„ ì´í•´í• ê¹Œìš”? ìš°ë¦¬ëŠ” ëˆˆìœ¼ë¡œ ë³´ê³ , ê·€ë¡œ ë“£ê³ , ì†ìœ¼ë¡œ ë§Œì§€ë©° ì„¸ìƒì„ ì¢…í•©ì ìœ¼ë¡œ ì´í•´í•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ AIëŠ” í…ìŠ¤íŠ¸ë§Œ ì´í•´í•˜ê±°ë‚˜, ì´ë¯¸ì§€ë§Œ ì¸ì‹í•˜ëŠ” ë“± í•œ ê°€ì§€ ê°ê°ë§Œ ê°€ì§€ê³  ìˆì—ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ì œëŠ” ë‹¤ë¦…ë‹ˆë‹¤!

**ë©€í‹°ëª¨ë‹¬ AIì˜ ì—­ì‚¬:**
- 2021ë…„: OpenAIì˜ CLIPì´ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ì´í•´í•˜ê¸° ì‹œì‘
- 2022ë…„: Flamingo, BLIP ë“±ì´ ë“±ì¥í•˜ë©° ì‹œê°ì  ëŒ€í™”ê°€ ê°€ëŠ¥í•´ì§
- 2023ë…„: GPT-4Vê°€ ì¶œì‹œë˜ë©° ë©€í‹°ëª¨ë‹¬ AIê°€ ëŒ€ì¤‘í™”
- 2024ë…„: ë” ì‘ê³  ë¹ ë¥¸ ëª¨ë¸ë“¤ì´ ë“±ì¥ (LLaVA, CogVLM)
- 2025ë…„ í˜„ì¬: ìŠ¤ë§ˆíŠ¸í°ì—ì„œë„ ì‹¤í–‰ ê°€ëŠ¥í•œ ì´ˆê²½ëŸ‰ ëª¨ë¸ ì‹œëŒ€!

### 2.2 2025ë…„ ìµœì‹  ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ì™„ì „ ì •ë³µ

#### ğŸ¦™ **Llama 4 ì‹œë¦¬ì¦ˆ (Meta)**

**ë°°ê²½ ìŠ¤í† ë¦¬:**
Meta(êµ¬ Facebook)ëŠ” ì˜¤í”ˆì†ŒìŠ¤ AIì˜ ì„ êµ¬ìì…ë‹ˆë‹¤. Mark ZuckerbergëŠ” "AIëŠ” ëª¨ë‘ì—ê²Œ ì—´ë ¤ìˆì–´ì•¼ í•œë‹¤"ëŠ” ì² í•™ìœ¼ë¡œ Llama ì‹œë¦¬ì¦ˆë¥¼ ë¬´ë£Œë¡œ ê³µê°œí–ˆìŠµë‹ˆë‹¤.

**ëª¨ë¸ ë¼ì¸ì—…:**
1. **Llama 4 Scout (17B)**
   - ìš©ë„: ì¼ë°˜ ë…¸íŠ¸ë¶ì—ì„œë„ ì‹¤í–‰ ê°€ëŠ¥í•œ ê²½ëŸ‰ ëª¨ë¸
   - íŠ¹ì§•: 16K í† í° ì»¨í…ìŠ¤íŠ¸, 50ê°œ ì–¸ì–´ ì§€ì›
   - ì¥ì : ë¹ ë¥¸ ì†ë„, ì ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©
   - ë‹¨ì : ë³µì¡í•œ ì¶”ë¡ ì—ëŠ” í•œê³„

2. **Llama 4 Maverick (45B)**
   - ìš©ë„: ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ì´ë¯¸ì§€ ë¶„ì„
   - íŠ¹ì§•: ì˜ë£Œ ì˜ìƒ, ìœ„ì„± ì‚¬ì§„ ë¶„ì„ ê°€ëŠ¥
   - ì¥ì : ë†’ì€ ì •í™•ë„, ì „ë¬¸ ë¶„ì•¼ íŠ¹í™”
   - ë‹¨ì : ê³ ì‚¬ì–‘ GPU í•„ìš”

3. **Llama 4 Titan (175B)**
   - ìš©ë„: ì—°êµ¬ì†Œë‚˜ ê¸°ì—…ìš© ìµœê³  ì„±ëŠ¥ ëª¨ë¸
   - íŠ¹ì§•: GPT-4Vë¥¼ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥
   - ì¥ì : ìµœê³ ì˜ ì„±ëŠ¥
   - ë‹¨ì : ì¼ë°˜ ì‚¬ìš©ìëŠ” ì‚¬ìš© ë¶ˆê°€

#### ğŸ’ **Gemma 3 ì‹œë¦¬ì¦ˆ (Google)**

**ë°°ê²½ ìŠ¤í† ë¦¬:**
Google DeepMindëŠ” "ì‘ì§€ë§Œ ê°•ë ¥í•œ" ëª¨ë¸ì„ ëª©í‘œë¡œ Gemmaë¥¼ ê°œë°œí–ˆìŠµë‹ˆë‹¤. ì´ë¦„ì€ ë¼í‹´ì–´ë¡œ "ë³´ì„"ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

**ëª¨ë¸ íŠ¹ì§•:**
```python
# Gemma ëª¨ë¸ë“¤ì˜ íŠ¹ì§•ì„ ì‹œê°í™”
import matplotlib.pyplot as plt
import numpy as np

models = ['Gemma 3\nFeather\n(4B)', 'Gemma 3\nSwift\n(12B)', 'Gemma 3\nHawk\n(27B)']
memory_usage = [4, 12, 27]  # GB
performance = [70, 85, 95]  # ìƒëŒ€ì  ì„±ëŠ¥
mobile_friendly = [100, 60, 20]  # ëª¨ë°”ì¼ ì í•©ë„

x = np.arange(len(models))
width = 0.25

fig, ax = plt.subplots(figsize=(10, 6))
bars1 = ax.bar(x - width, memory_usage, width, label='ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (GB)', color='skyblue')
bars2 = ax.bar(x, performance, width, label='ì„±ëŠ¥ ì ìˆ˜', color='lightgreen')
bars3 = ax.bar(x + width, mobile_friendly, width, label='ëª¨ë°”ì¼ ì í•©ë„', color='coral')

ax.set_xlabel('ëª¨ë¸', fontsize=12)
ax.set_ylabel('ì ìˆ˜', fontsize=12)
ax.set_title('Gemma 3 ì‹œë¦¬ì¦ˆ ë¹„êµ', fontsize=14, fontweight='bold')
ax.set_xticks(x)
ax.set_xticklabels(models)
ax.legend()

# ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ
for bars in [bars1, bars2, bars3]:
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height}',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3),
                    textcoords="offset points",
                    ha='center', va='bottom')

plt.tight_layout()
plt.show()
```

#### ğŸŒŸ **MiniCPM-V 2.6 (OpenBMB)**

**ë°°ê²½ ìŠ¤í† ë¦¬:**
ì¤‘êµ­ ì¹­í™”ëŒ€í•™êµì˜ OpenBMB íŒ€ì´ ê°œë°œí•œ ì´ ëª¨ë¸ì€ "ì‘ì€ ê±°ì¸"ì´ë¼ ë¶ˆë¦½ë‹ˆë‹¤. 8B íŒŒë¼ë¯¸í„°ë¡œ GPT-4Vì˜ ì„±ëŠ¥ì— ê·¼ì ‘í–ˆìŠµë‹ˆë‹¤.

**í•µì‹¬ íŠ¹ì§•:**
- ì´ˆê³ í•´ìƒë„ ì´ë¯¸ì§€ ì§€ì› (1344Ã—1344)
- 180ê°œ í”„ë ˆì„ì˜ ë¹„ë””ì˜¤ ì´í•´
- ëª¨ë°”ì¼ ê¸°ê¸°ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥
- í•œêµ­ì–´ ì„±ëŠ¥ ìš°ìˆ˜

#### ğŸ¯ **InternVL 2.5 (Shanghai AI Lab)**

**ë°°ê²½ ìŠ¤í† ë¦¬:**
ìƒí•˜ì´ AI ì—°êµ¬ì†Œê°€ "ë™ì„œì–‘ì˜ ì§€í˜œë¥¼ ê²°í•©"í•œë‹¤ëŠ” ëª©í‘œë¡œ ê°œë°œí–ˆìŠµë‹ˆë‹¤.

**ë…íŠ¹í•œ ê¸°ëŠ¥:**
- ë‹¤ì¤‘ ì´ë¯¸ì§€ ë¹„êµ ë¶„ì„
- ì°¨íŠ¸ì™€ ê·¸ë˜í”„ ì´í•´
- OCR ê¸°ëŠ¥ ë‚´ì¥
- ìˆ˜ì‹ ì¸ì‹ ê°€ëŠ¥

#### ğŸ§  **Phi-4 Vision (Microsoft)**

**ë°°ê²½ ìŠ¤í† ë¦¬:**
MicrosoftëŠ” "ì‘ì§€ë§Œ ë˜‘ë˜‘í•œ" AIë¥¼ ë§Œë“œëŠ” ë° ì§‘ì¤‘í–ˆìŠµë‹ˆë‹¤. PhiëŠ” ê·¸ë¦¬ìŠ¤ ë¬¸ì Ï†(íŒŒì´)ì—ì„œ ë”°ì™”ìŠµë‹ˆë‹¤.

**íŠ¹ë³„í•œ ì :**
- 14B íŒŒë¼ë¯¸í„°ë¡œ ì´ˆê²½ëŸ‰
- ìˆ˜í•™, ì½”ë”©ì— íŠ¹í™”
- ì¶”ë¡  ëŠ¥ë ¥ íƒì›”
- Azure í´ë¼ìš°ë“œ ìµœì í™”

### 2.3 ë©€í‹°ëª¨ë‹¬ AIì˜ ì‘ë™ ì›ë¦¬ - ì‰½ê²Œ ì´í•´í•˜ê¸°

ë©€í‹°ëª¨ë‹¬ AIê°€ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ë‹¨ê³„ë³„ë¡œ ì•Œì•„ë´…ì‹œë‹¤:

```python
# ë©€í‹°ëª¨ë‹¬ AIì˜ ì‘ë™ ê³¼ì •ì„ ì‹œê°í™”
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.patches import FancyBboxPatch, ConnectionPatch
import numpy as np

fig, ax = plt.subplots(figsize=(12, 8))

# ë°°ê²½ ìƒ‰ìƒ
ax.set_facecolor('#f0f0f0')

# 1. ì…ë ¥ ë‹¨ê³„
input_img = FancyBboxPatch((0.5, 6), 2, 1.5, 
                          boxstyle="round,pad=0.1",
                          facecolor='lightblue',
                          edgecolor='darkblue',
                          linewidth=2)
ax.add_patch(input_img)
ax.text(1.5, 6.75, 'ì´ë¯¸ì§€\nì…ë ¥', ha='center', va='center', fontsize=12, fontweight='bold')

input_text = FancyBboxPatch((0.5, 4), 2, 1.5,
                           boxstyle="round,pad=0.1", 
                           facecolor='lightgreen',
                           edgecolor='darkgreen',
                           linewidth=2)
ax.add_patch(input_text)
ax.text(1.5, 4.75, 'í…ìŠ¤íŠ¸\nì§ˆë¬¸', ha='center', va='center', fontsize=12, fontweight='bold')

# 2. ì¸ì½”ë”© ë‹¨ê³„
img_encoder = FancyBboxPatch((3.5, 6), 2.5, 1.5,
                            boxstyle="round,pad=0.1",
                            facecolor='#FFE5B4',
                            edgecolor='#FF8C00',
                            linewidth=2)
ax.add_patch(img_encoder)
ax.text(4.75, 6.75, 'ì´ë¯¸ì§€\nì¸ì½”ë”', ha='center', va='center', fontsize=12, fontweight='bold')

text_encoder = FancyBboxPatch((3.5, 4), 2.5, 1.5,
                             boxstyle="round,pad=0.1",
                             facecolor='#FFE5B4',
                             edgecolor='#FF8C00',
                             linewidth=2)
ax.add_patch(text_encoder)
ax.text(4.75, 4.75, 'í…ìŠ¤íŠ¸\nì¸ì½”ë”', ha='center', va='center', fontsize=12, fontweight='bold')

# 3. ìœµí•© ë‹¨ê³„
fusion = FancyBboxPatch((7, 5), 2.5, 2,
                       boxstyle="round,pad=0.1",
                       facecolor='#E6E6FA',
                       edgecolor='#9370DB',
                       linewidth=3)
ax.add_patch(fusion)
ax.text(8.25, 6, 'ë©€í‹°ëª¨ë‹¬\nìœµí•©', ha='center', va='center', fontsize=12, fontweight='bold')

# 4. ì¶œë ¥ ë‹¨ê³„
output = FancyBboxPatch((10.5, 5), 2.5, 2,
                       boxstyle="round,pad=0.1",
                       facecolor='#FFB6C1',
                       edgecolor='#DC143C',
                       linewidth=2)
ax.add_patch(output)
ax.text(11.75, 6, 'ë‹µë³€\nìƒì„±', ha='center', va='center', fontsize=12, fontweight='bold')

# í™”ì‚´í‘œ ì¶”ê°€
arrows = [
    ((2.5, 6.75), (3.5, 6.75)),  # ì´ë¯¸ì§€ ì…ë ¥ â†’ ì¸ì½”ë”
    ((2.5, 4.75), (3.5, 4.75)),  # í…ìŠ¤íŠ¸ ì…ë ¥ â†’ ì¸ì½”ë”
    ((6, 6.75), (7, 6)),         # ì´ë¯¸ì§€ ì¸ì½”ë” â†’ ìœµí•©
    ((6, 4.75), (7, 5)),         # í…ìŠ¤íŠ¸ ì¸ì½”ë” â†’ ìœµí•©
    ((9.5, 6), (10.5, 6))        # ìœµí•© â†’ ì¶œë ¥
]

for start, end in arrows:
    arrow = ConnectionPatch(start, end, "data", "data",
                          arrowstyle="->", shrinkA=5, shrinkB=5,
                          mutation_scale=20, fc="black", linewidth=2)
    ax.add_artist(arrow)

# ì œëª©ê³¼ ì„¤ëª…
ax.text(7, 8, 'ë©€í‹°ëª¨ë‹¬ AI ì‘ë™ ì›ë¦¬', fontsize=16, fontweight='bold', ha='center')
ax.text(7, 2, 'ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ì´í•´í•˜ì—¬ ì¢…í•©ì ì¸ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤', 
        fontsize=12, ha='center', style='italic')

ax.set_xlim(0, 14)
ax.set_ylim(1, 9)
ax.axis('off')

plt.tight_layout()
plt.show()
```

### 2.4 ì‹¤ì œë¡œ ì–´ë””ì— ì‚¬ìš©ë ê¹Œìš”?

ë©€í‹°ëª¨ë‹¬ AIì˜ ì‹¤ì œ í™œìš© ì‚¬ë¡€ë¥¼ ì‚´í´ë´…ì‹œë‹¤:

1. **ì˜ë£Œ ë¶„ì•¼**
   - X-ray, MRI ì˜ìƒ ë¶„ì„
   - ì˜ì‚¬ì˜ ì§ˆë¬¸ì— ë”°ë¥¸ ì˜ìƒ í•´ì„
   - ì§„ë‹¨ ë³´ì¡° ì‹œìŠ¤í…œ

2. **êµìœ¡ ë¶„ì•¼**
   - ìˆ˜í•™ ë¬¸ì œ ì‚¬ì§„ì„ ì°ìœ¼ë©´ í’€ì´ ê³¼ì • ì„¤ëª…
   - ê³¼í•™ ì‹¤í—˜ ì˜ìƒ ë¶„ì„
   - ì–¸ì–´ í•™ìŠµ ë„ìš°ë¯¸

3. **ì¼ìƒ ìƒí™œ**
   - ìš”ë¦¬ ì‚¬ì§„ìœ¼ë¡œ ë ˆì‹œí”¼ ì°¾ê¸°
   - ì˜· ì‚¬ì§„ìœ¼ë¡œ ì½”ë”” ì¶”ì²œ
   - ì—¬í–‰ ì‚¬ì§„ìœ¼ë¡œ ì¥ì†Œ ì •ë³´ ì œê³µ

---

## 3ì¥: ë‹¤ì–‘í•œ ìµœì‹  ëª¨ë¸ ì‹¤ìŠµ - ì§ì ‘ ë§Œì ¸ë³´ëŠ” AI

### 3.1 ì²« ë²ˆì§¸ ëª¨ë¸: BLIP-2ë¡œ ì‹œì‘í•˜ê¸°

BLIP-2ëŠ” Salesforceê°€ ë§Œë“  ëª¨ë¸ë¡œ, ì´ˆë³´ìê°€ ì‹œì‘í•˜ê¸°ì— ì™„ë²½í•©ë‹ˆë‹¤. ì™œëƒí•˜ë©´ ì„¤ì¹˜ê°€ ì‰½ê³ , ì„±ëŠ¥ë„ ì¢‹ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤!

```python
# BLIP-2 ê¸°ë³¸ ì‚¬ìš©ë²• - ì´ˆë³´ìë¥¼ ìœ„í•œ ìƒì„¸ ì„¤ëª… í¬í•¨
from transformers import Blip2Processor, Blip2ForConditionalGeneration
import torch
from PIL import Image
import requests
import matplotlib.pyplot as plt

class SimpleImageAnalyzer:
    """ì´ˆë³´ìë¥¼ ìœ„í•œ ê°„ë‹¨í•œ ì´ë¯¸ì§€ ë¶„ì„ê¸°"""
    
    def __init__(self):
        """ëª¨ë¸ì„ ì¤€ë¹„í•˜ëŠ” ì´ˆê¸°í™” í•¨ìˆ˜"""
        print("ğŸš€ ì´ë¯¸ì§€ ë¶„ì„ AIë¥¼ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        
        # ëª¨ë¸ ì´ë¦„ - Hugging Faceì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤
        model_name = "Salesforce/blip2-opt-2.7b"
        
        # í”„ë¡œì„¸ì„œ: ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜
        self.processor = Blip2Processor.from_pretrained(model_name)
        
        # ëª¨ë¸: ì‹¤ì œ AIì˜ ë‘ë‡Œ ì—­í• 
        # GPUê°€ ìˆìœ¼ë©´ ë¹ ë¥´ê²Œ, ì—†ìœ¼ë©´ CPUë¡œ ì‹¤í–‰
        if torch.cuda.is_available():
            print("ğŸ® GPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤ - ë¹ ë¥¸ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤!")
            self.model = Blip2ForConditionalGeneration.from_pretrained(
                model_name,
                torch_dtype=torch.float16,  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ 16ë¹„íŠ¸ ì‚¬ìš©
                device_map="auto"  # ìë™ìœ¼ë¡œ ìµœì ì˜ ì¥ì¹˜ ì„ íƒ
            )
        else:
            print("ğŸ’» CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤ - ì¡°ê¸ˆ ëŠë¦¬ì§€ë§Œ ì¶©ë¶„í•©ë‹ˆë‹¤!")
            self.model = Blip2ForConditionalGeneration.from_pretrained(
                model_name,
                torch_dtype=torch.float32  # CPUëŠ” 32ë¹„íŠ¸ ì‚¬ìš©
            )
        
        print("âœ… AIê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!\n")
    
    def analyze_image(self, image_path, question="ì´ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”"):
        """ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ê³  ì§ˆë¬¸ì— ë‹µí•˜ëŠ” í•¨ìˆ˜"""
        
        # 1. ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸°
        print(f"ğŸ“¸ ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ìˆìŠµë‹ˆë‹¤: {image_path}")
        
        if image_path.startswith('http'):
            # ì¸í„°ë„·ì—ì„œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            image = Image.open(requests.get(image_path, stream=True).raw)
        else:
            # ì»´í“¨í„°ì—ì„œ ì´ë¯¸ì§€ ì—´ê¸°
            image = Image.open(image_path)
        
        # RGBë¡œ ë³€í™˜ (ì¼ë¶€ ì´ë¯¸ì§€ëŠ” RGBAë‚˜ í‘ë°±ì¼ ìˆ˜ ìˆìŒ)
        image = image.convert('RGB')
        
        # 2. ì´ë¯¸ì§€ì™€ ì§ˆë¬¸ì„ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜
        inputs = self.processor(image, question, return_tensors="pt")
        
        # GPUë¥¼ ì‚¬ìš©í•œë‹¤ë©´ ë°ì´í„°ë„ GPUë¡œ ì´ë™
        if torch.cuda.is_available():
            inputs = {k: v.cuda() for k, v in inputs.items()}
        
        # 3. AIì—ê²Œ ë¬¼ì–´ë³´ê¸°
        print("ğŸ¤” AIê°€ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        
        with torch.no_grad():  # í•™ìŠµì´ ì•„ë‹Œ ì¶”ë¡  ëª¨ë“œ
            generated_ids = self.model.generate(
                **inputs,
                max_new_tokens=100,  # ìµœëŒ€ 100ê°œì˜ ë‹¨ì–´ë¡œ ë‹µë³€
                temperature=0.7,     # ì°½ì˜ì„± ì •ë„ (0~1, ë†’ì„ìˆ˜ë¡ ì°½ì˜ì )
                do_sample=True,      # í™•ë¥ ì  ìƒ˜í”Œë§ ì‚¬ìš©
                top_p=0.95          # ìƒìœ„ 95%ì˜ í™•ë¥ ì„ ê°€ì§„ ë‹¨ì–´ë§Œ ê³ ë ¤
            )
        
        # 4. AIì˜ ë‹µë³€ì„ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        answer = self.processor.decode(generated_ids[0], skip_special_tokens=True)
        
        # 5. ê²°ê³¼ í‘œì‹œ
        self.show_result(image, question, answer)
        
        return answer
    
    def show_result(self, image, question, answer):
        """ê²°ê³¼ë¥¼ ì˜ˆì˜ê²Œ ë³´ì—¬ì£¼ëŠ” í•¨ìˆ˜"""
        plt.figure(figsize=(10, 6))
        
        # ì´ë¯¸ì§€ í‘œì‹œ
        plt.subplot(1, 2, 1)
        plt.imshow(image)
        plt.axis('off')
        plt.title('ë¶„ì„í•œ ì´ë¯¸ì§€', fontsize=14, fontweight='bold')
        
        # ì§ˆë¬¸ê³¼ ë‹µë³€ í‘œì‹œ
        plt.subplot(1, 2, 2)
        plt.text(0.1, 0.7, f"ì§ˆë¬¸: {question}", fontsize=12, 
                wrap=True, fontweight='bold')
        plt.text(0.1, 0.3, f"AI ë‹µë³€: {answer}", fontsize=11,
                wrap=True, style='italic')
        plt.axis('off')
        plt.title('AIì˜ ë¶„ì„ ê²°ê³¼', fontsize=14, fontweight='bold')
        
        plt.tight_layout()
        plt.show()

# ì‹¤ì œë¡œ ì‚¬ìš©í•´ë³´ê¸°!
analyzer = SimpleImageAnalyzer()

# ë‹¤ì–‘í•œ ì˜ˆì œ ì´ë¯¸ì§€ë¡œ í…ŒìŠ¤íŠ¸
test_images = [
    {
        'url': 'https://images.unsplash.com/photo-1514888286974-6c03e2ca1dba',
        'questions': [
            "ì´ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”",
            "ì–´ë–¤ ë™ë¬¼ì´ ë³´ì´ë‚˜ìš”?",
            "ë™ë¬¼ì˜ í‘œì •ì€ ì–´ë–¤ê°€ìš”?",
            "ë°°ê²½ì€ ì–´ë–¤ ìƒ‰ì¸ê°€ìš”?"
        ]
    },
    {
        'url': 'https://images.unsplash.com/photo-1506619216599-9d16d0903dfd',
        'questions': [
            "ì´ê³³ì€ ì–´ë””ì¸ê°€ìš”?",
            "ë‚ ì”¨ëŠ” ì–´ë–¤ê°€ìš”?",
            "ëª‡ ëª…ì˜ ì‚¬ëŒì´ ë³´ì´ë‚˜ìš”?",
            "ì–´ë–¤ í™œë™ì„ í•˜ê³  ìˆë‚˜ìš”?"
        ]
    }
]

# ê° ì´ë¯¸ì§€ì— ëŒ€í•´ ì—¬ëŸ¬ ì§ˆë¬¸í•˜ê¸°
for img_data in test_images:
    print(f"\n{'='*50}")
    print(f"ìƒˆë¡œìš´ ì´ë¯¸ì§€ ë¶„ì„ ì‹œì‘!")
    print(f"{'='*50}\n")
    
    for question in img_data['questions']:
        answer = analyzer.analyze_image(img_data['url'], question)
        print(f"\nğŸ’¬ Q: {question}")
        print(f"ğŸ¤– A: {answer}")
        print("-" * 30)
```

### 3.2 MiniCPM-V 2.6 ì‹¤ìŠµ - ê¸´ ë¹„ë””ì˜¤ë„ ì´í•´í•˜ëŠ” AI

ì´ì œ ë” ê³ ê¸‰ ëª¨ë¸ì¸ MiniCPM-Vë¥¼ ì‚¬ìš©í•´ë´…ì‹œë‹¤. ì´ ëª¨ë¸ì€ íŠ¹íˆ ê¸´ ë¹„ë””ì˜¤ë¥¼ ì´í•´í•˜ëŠ” ë° íƒì›”í•©ë‹ˆë‹¤!

```python
# MiniCPM-V 2.6 ì‹¤ìŠµ
# ì£¼ì˜: ì´ ëª¨ë¸ì€ ë” ë§ì€ ë©”ëª¨ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤

from transformers import AutoModel, AutoTokenizer
import torch
import numpy as np
from PIL import Image
from decord import VideoReader, cpu
import matplotlib.pyplot as plt

class VideoUnderstandingAI:
    """ë¹„ë””ì˜¤ë¥¼ ì´í•´í•˜ëŠ” ë˜‘ë˜‘í•œ AI"""
    
    def __init__(self):
        """MiniCPM-V 2.6 ëª¨ë¸ ì´ˆê¸°í™”"""
        print("ğŸ¬ ë¹„ë””ì˜¤ ì´í•´ AIë¥¼ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        
        model_name = "openbmb/MiniCPM-V-2_6"
        
        # í† í¬ë‚˜ì´ì €: í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True  # ì»¤ìŠ¤í…€ ì½”ë“œ ì‹¤í–‰ í—ˆìš©
        )
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = AutoModel.from_pretrained(
            model_name,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32
        )
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = self.model.to(self.device)
        
        print(f"âœ… ë¹„ë””ì˜¤ AI ì¤€ë¹„ ì™„ë£Œ! (ë””ë°”ì´ìŠ¤: {self.device})")
    
    def extract_frames(self, video_path, num_frames=8):
        """ë¹„ë””ì˜¤ì—ì„œ ê· ë“±í•˜ê²Œ í”„ë ˆì„ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
        print(f"ğŸï¸ ë¹„ë””ì˜¤ì—ì„œ {num_frames}ê°œì˜ í”„ë ˆì„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤...")
        
        # ë¹„ë””ì˜¤ ì½ê¸°
        vr = VideoReader(video_path, ctx=cpu(0))
        total_frames = len(vr)
        
        # ê· ë“±í•œ ê°„ê²©ìœ¼ë¡œ í”„ë ˆì„ ì„ íƒ
        indices = np.linspace(0, total_frames-1, num_frames, dtype=int)
        
        frames = []
        for idx in indices:
            frame = vr[idx].asnumpy()
            frame_pil = Image.fromarray(frame)
            frames.append(frame_pil)
        
        return frames, indices, total_frames
    
    def analyze_video(self, video_path, question="ì´ ë¹„ë””ì˜¤ì—ì„œ ë¬´ìŠ¨ ì¼ì´ ì¼ì–´ë‚˜ê³  ìˆë‚˜ìš”?"):
        """ë¹„ë””ì˜¤ë¥¼ ë¶„ì„í•˜ê³  ì§ˆë¬¸ì— ë‹µí•˜ëŠ” í•¨ìˆ˜"""
        
        # 1. í”„ë ˆì„ ì¶”ì¶œ
        frames, indices, total_frames = self.extract_frames(video_path)
        
        # 2. í”„ë ˆì„ ì‹œê°í™”
        self.visualize_frames(frames, indices, total_frames)
        
        # 3. ëª¨ë¸ì— ì…ë ¥
        print(f"\nğŸ¤” AIê°€ ë¹„ë””ì˜¤ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        print(f"ì§ˆë¬¸: {question}")
        
        # ì…ë ¥ ì¤€ë¹„
        msgs = [
            {
                'role': 'user',
                'content': [
                    *[{'type': 'image', 'image': frame} for frame in frames],
                    {'type': 'text', 'text': question}
                ]
            }
        ]
        
        # ì¶”ë¡ 
        with torch.no_grad():
            answer = self.model.chat(
                msgs=msgs,
                tokenizer=self.tokenizer,
                max_new_tokens=200,
                temperature=0.7
            )
        
        print(f"\nğŸ¤– AIì˜ ë‹µë³€: {answer}")
        
        return answer, frames
    
    def visualize_frames(self, frames, indices, total_frames):
        """ì¶”ì¶œëœ í”„ë ˆì„ì„ ì‹œê°í™”í•˜ëŠ” í•¨ìˆ˜"""
        fig, axes = plt.subplots(2, 4, figsize=(12, 6))
        axes = axes.ravel()
        
        for i, (frame, idx) in enumerate(zip(frames, indices)):
            axes[i].imshow(frame)
            axes[i].set_title(f'í”„ë ˆì„ {idx}/{total_frames}')
            axes[i].axis('off')
        
        plt.suptitle('ë¹„ë””ì˜¤ì—ì„œ ì¶”ì¶œí•œ í”„ë ˆì„ë“¤', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.show()

# ì‚¬ìš© ì˜ˆì‹œ (ì‹¤ì œ ë¹„ë””ì˜¤ íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤)
"""
video_ai = VideoUnderstandingAI()

# ë¹„ë””ì˜¤ ë¶„ì„ ì˜ˆì œ
video_questions = [
    "ì´ ë¹„ë””ì˜¤ì—ì„œ ë¬´ìŠ¨ ì¼ì´ ì¼ì–´ë‚˜ê³  ìˆë‚˜ìš”?",
    "ì£¼ìš” ë“±ì¥ì¸ë¬¼ì€ ëˆ„êµ¬ì¸ê°€ìš”?",
    "ì–´ë–¤ ê°ì •ì´ ëŠê»´ì§€ë‚˜ìš”?",
    "ì´ ë¹„ë””ì˜¤ì˜ ì£¼ì œëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
]

# ì—¬ëŸ¬ ì§ˆë¬¸ìœ¼ë¡œ ë¶„ì„
for question in video_questions:
    answer, frames = video_ai.analyze_video("sample_video.mp4", question)
"""
```

### 3.3 InternVL 2.5 ì‹¤ìŠµ - ì°¨íŠ¸ì™€ ë¬¸ì„œë¥¼ ì´í•´í•˜ëŠ” AI

InternVLì€ íŠ¹íˆ ì°¨íŠ¸, ê·¸ë˜í”„, ë¬¸ì„œë¥¼ ì˜ ì´í•´í•©ë‹ˆë‹¤. í•™ìƒë“¤ì—ê²Œ ì•„ì£¼ ìœ ìš©í•œ ëª¨ë¸ì´ì£ !

```python
# InternVL 2.5 - ë¬¸ì„œì™€ ì°¨íŠ¸ ë¶„ì„ ì „ë¬¸ê°€
import torch
from transformers import AutoModel, AutoTokenizer
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np

class DocumentAnalyzer:
    """ë¬¸ì„œ, ì°¨íŠ¸, ê·¸ë˜í”„ë¥¼ ë¶„ì„í•˜ëŠ” AI"""
    
    def __init__(self):
        print("ğŸ“Š ë¬¸ì„œ ë¶„ì„ AIë¥¼ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        
        # InternVL ëª¨ë¸ ë¡œë“œ
        model_name = "OpenGVLab/InternVL2-8B"
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True
        )
        
        self.model = AutoModel.from_pretrained(
            model_name,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32
        )
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = self.model.to(self.device).eval()
        
        print("âœ… ë¬¸ì„œ ë¶„ì„ AI ì¤€ë¹„ ì™„ë£Œ!")
    
    def create_sample_chart(self):
        """í…ŒìŠ¤íŠ¸ìš© ì°¨íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
        # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
        months = ['1ì›”', '2ì›”', '3ì›”', '4ì›”', '5ì›”', '6ì›”']
        product_a = [45, 52, 48, 58, 63, 67]
        product_b = [38, 41, 43, 46, 51, 55]
        product_c = [30, 35, 33, 38, 42, 48]
        
        # ì°¨íŠ¸ ìƒì„±
        plt.figure(figsize=(10, 6))
        
        x = np.arange(len(months))
        width = 0.25
        
        plt.bar(x - width, product_a, width, label='ì œí’ˆ A', color='#FF6B6B')
        plt.bar(x, product_b, width, label='ì œí’ˆ B', color='#4ECDC4')
        plt.bar(x + width, product_c, width, label='ì œí’ˆ C', color='#45B7D1')
        
        plt.xlabel('ì›”', fontsize=12)
        plt.ylabel('íŒë§¤ëŸ‰ (ì²œ ê°œ)', fontsize=12)
        plt.title('2025ë…„ ìƒë°˜ê¸° ì œí’ˆë³„ íŒë§¤ ì¶”ì´', fontsize=14, fontweight='bold')
        plt.xticks(x, months)
        plt.legend()
        plt.grid(axis='y', alpha=0.3)
        
        # ì°¨íŠ¸ë¥¼ ì´ë¯¸ì§€ë¡œ ì €ì¥
        plt.savefig('sample_chart.png', dpi=150, bbox_inches='tight')
        plt.show()
        
        return 'sample_chart.png'
    
    def analyze_chart(self, image_path, questions=None):
        """ì°¨íŠ¸ë‚˜ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜"""
        
        if questions is None:
            questions = [
                "ì´ ì°¨íŠ¸ê°€ ë³´ì—¬ì£¼ëŠ” ì£¼ìš” ì •ë³´ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
                "ê°€ì¥ ë†’ì€ ì„±ê³¼ë¥¼ ë³´ì¸ ì œí’ˆì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "ì „ì²´ì ì¸ íŠ¸ë Œë“œëŠ” ì–´ë–¤ê°€ìš”?",
                "ì´ ë°ì´í„°ì—ì„œ ì£¼ëª©í•  ë§Œí•œ ì ì€ ë¬´ì—‡ì¸ê°€ìš”?"
            ]
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        image = Image.open(image_path)
        
        print(f"\nğŸ“ˆ ì°¨íŠ¸ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        results = []
        for question in questions:
            print(f"\nâ“ ì§ˆë¬¸: {question}")
            
            # ëª¨ë¸ì— ì…ë ¥
            response = self.model.chat(
                self.tokenizer,
                pixel_values=image,
                question=question,
                max_new_tokens=200,
                do_sample=True,
                temperature=0.7
            )
            
            print(f"ğŸ¤– ë‹µë³€: {response}")
            results.append({'question': question, 'answer': response})
        
        return results
    
    def analyze_math_problem(self, image_path):
        """ìˆ˜í•™ ë¬¸ì œë¥¼ ë¶„ì„í•˜ê³  í’€ì´í•˜ëŠ” í•¨ìˆ˜"""
        
        prompts = [
            "ì´ ìˆ˜í•™ ë¬¸ì œë¥¼ ë‹¨ê³„ë³„ë¡œ í’€ì–´ì£¼ì„¸ìš”.",
            "ì‚¬ìš©ëœ ê³µì‹ì´ë‚˜ ê°œë…ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            "ë‹µì„ í™•ì¸í•˜ê³  ê²€ì¦í•´ì£¼ì„¸ìš”."
        ]
        
        print("\nğŸ”¢ ìˆ˜í•™ ë¬¸ì œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        image = Image.open(image_path)
        
        for prompt in prompts:
            print(f"\nğŸ“ {prompt}")
            
            response = self.model.chat(
                self.tokenizer,
                pixel_values=image,
                question=prompt,
                max_new_tokens=300,
                do_sample=False  # ìˆ˜í•™ì€ ì •í™•ì„±ì´ ì¤‘ìš”í•˜ë¯€ë¡œ ìƒ˜í”Œë§ ë¹„í™œì„±í™”
            )
            
            print(f"ğŸ’¡ {response}")

# ì‹¤ìŠµ ì˜ˆì‹œ
analyzer = DocumentAnalyzer()

# 1. ìƒ˜í”Œ ì°¨íŠ¸ ìƒì„± ë° ë¶„ì„
chart_path = analyzer.create_sample_chart()
results = analyzer.analyze_chart(chart_path)

# 2. ì»¤ìŠ¤í…€ ì§ˆë¬¸ìœ¼ë¡œ ë¶„ì„
custom_questions = [
    "ì–´ëŠ ì œí’ˆì˜ ì„±ì¥ë¥ ì´ ê°€ì¥ ë†’ë‚˜ìš”?",
    "3ì›”ì— íŒë§¤ëŸ‰ì´ ê°ì†Œí•œ ì œí’ˆì´ ìˆë‚˜ìš”?",
    "ì´ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì–´ë–¤ ë¹„ì¦ˆë‹ˆìŠ¤ ê²°ì •ì„ ë‚´ë¦´ ìˆ˜ ìˆì„ê¹Œìš”?"
]
analyzer.analyze_chart(chart_path, custom_questions)
```

### 3.4 Phi-4 Vision ì‹¤ìŠµ - ì½”ë”©ê³¼ ìˆ˜í•™ì˜ ë‹¬ì¸

Microsoftì˜ Phi-4ëŠ” íŠ¹íˆ ì½”ë”©ê³¼ ìˆ˜í•™ ë¬¸ì œë¥¼ ì˜ í•´ê²°í•©ë‹ˆë‹¤!

```python
# Phi-4 Vision - ì½”ë”©ê³¼ ìˆ˜í•™ ì „ë¬¸ê°€
from transformers import AutoModelForCausalLM, AutoProcessor
import torch
from PIL import Image, ImageDraw, ImageFont
import matplotlib.pyplot as plt

class CodingMathAssistant:
    """ì½”ë”©ê³¼ ìˆ˜í•™ì„ ë„ì™€ì£¼ëŠ” AI ì¡°êµ"""
    
    def __init__(self):
        print("ğŸ‘¨â€ğŸ’» ì½”ë”©/ìˆ˜í•™ AI ì¡°êµë¥¼ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        
        model_id = "microsoft/phi-4-vision-preview"
        
        # í”„ë¡œì„¸ì„œì™€ ëª¨ë¸ ë¡œë“œ
        self.processor = AutoProcessor.from_pretrained(model_id)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto"
        )
        
        print("âœ… AI ì¡°êµ ì¤€ë¹„ ì™„ë£Œ!")
    
    def create_code_snippet_image(self, code, language="python"):
        """ì½”ë“œë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜"""
        
        # ì´ë¯¸ì§€ ìƒì„±
        img = Image.new('RGB', (800, 600), color='#1e1e1e')
        draw = ImageDraw.Draw(img)
        
        # ê°„ë‹¨í•œ ì½”ë“œ í•˜ì´ë¼ì´íŒ… (ì‹¤ì œë¡œëŠ” pygments ë“± ì‚¬ìš©)
        lines = code.strip().split('\n')
        y_offset = 30
        
        # íƒ€ì´í‹€
        draw.text((20, 10), f"{language.upper()} CODE", fill='#ffffff')
        
        # ì½”ë“œ ë¼ì¸
        for i, line in enumerate(lines):
            # ë¼ì¸ ë²ˆí˜¸
            draw.text((20, y_offset), f"{i+1:3d}", fill='#858585')
            
            # ì½”ë“œ ë‚´ìš© (ê°„ë‹¨í•œ ìƒ‰ìƒ êµ¬ë¶„)
            x_offset = 60
            if line.strip().startswith('#'):
                color = '#608b4e'  # ì£¼ì„
            elif any(keyword in line for keyword in ['def', 'class', 'import', 'for', 'if', 'while']):
                color = '#c586c0'  # í‚¤ì›Œë“œ
            else:
                color = '#d4d4d4'  # ì¼ë°˜ ì½”ë“œ
            
            draw.text((x_offset, y_offset), line, fill=color)
            y_offset += 25
        
        return img
    
    def analyze_code(self, code_text, question="ì´ ì½”ë“œë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”"):
        """ì½”ë“œë¥¼ ë¶„ì„í•˜ê³  ì„¤ëª…í•˜ëŠ” í•¨ìˆ˜"""
        
        # ì½”ë“œë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
        code_image = self.create_code_snippet_image(code_text)
        
        # ë¶„ì„ ìˆ˜í–‰
        print(f"\nğŸ’» ì½”ë“œ ë¶„ì„ ì¤‘...")
        print(f"ì§ˆë¬¸: {question}")
        
        inputs = self.processor(
            text=question,
            images=code_image,
            return_tensors="pt"
        )
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=300,
                temperature=0.2,  # ì½”ë“œ ì„¤ëª…ì€ ì •í™•í•´ì•¼ í•˜ë¯€ë¡œ ë‚®ì€ temperature
                do_sample=True
            )
        
        response = self.processor.decode(outputs[0], skip_special_tokens=True)
        
        # ê²°ê³¼ ì‹œê°í™”
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
        
        # ì½”ë“œ ì´ë¯¸ì§€
        ax1.imshow(code_image)
        ax1.set_title('ë¶„ì„í•  ì½”ë“œ', fontweight='bold')
        ax1.axis('off')
        
        # AI ì„¤ëª…
        ax2.text(0.05, 0.95, "AIì˜ ì½”ë“œ ë¶„ì„:", fontweight='bold', 
                transform=ax2.transAxes, verticalalignment='top')
        ax2.text(0.05, 0.85, response, transform=ax2.transAxes, 
                verticalalignment='top', wrap=True, fontsize=10)
        ax2.axis('off')
        
        plt.tight_layout()
        plt.show()
        
        return response
    
    def solve_math_step_by_step(self, problem_text):
        """ìˆ˜í•™ ë¬¸ì œë¥¼ ë‹¨ê³„ë³„ë¡œ í•´ê²°í•˜ëŠ” í•¨ìˆ˜"""
        
        prompts = [
            f"ë¬¸ì œ: {problem_text}\n\nì´ ë¬¸ì œë¥¼ ë‹¨ê³„ë³„ë¡œ í’€ì–´ì£¼ì„¸ìš”.",
            "ê° ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•œ ê³µì‹ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            "ë‹µì„ ê²€ì¦í•´ì£¼ì„¸ìš”."
        ]
        
        print(f"\nğŸ”¢ ìˆ˜í•™ ë¬¸ì œ: {problem_text}")
        print("=" * 50)
        
        full_solution = []
        
        for i, prompt in enumerate(prompts):
            print(f"\në‹¨ê³„ {i+1}:")
            
            inputs = self.processor(
                text=prompt,
                return_tensors="pt"
            )
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=200,
                    temperature=0.1,
                    do_sample=True
                )
            
            response = self.processor.decode(outputs[0], skip_special_tokens=True)
            print(response)
            full_solution.append(response)
        
        return full_solution

# ì‹¤ìŠµ ì˜ˆì œ
assistant = CodingMathAssistant()

# 1. ì½”ë“œ ë¶„ì„ ì˜ˆì œ
sample_code = """
def fibonacci(n):
    # í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì˜ në²ˆì§¸ í•­ì„ ê³„ì‚°
    if n <= 1:
        return n
    
    # ë©”ëª¨ì´ì œì´ì…˜ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸
    fib = [0] * (n + 1)
    fib[0] = 0
    fib[1] = 1
    
    # ë™ì  í”„ë¡œê·¸ë˜ë°ìœ¼ë¡œ ê³„ì‚°
    for i in range(2, n + 1):
        fib[i] = fib[i-1] + fib[i-2]
    
    return fib[n]

# í…ŒìŠ¤íŠ¸
result = fibonacci(10)
print(f"í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì˜ 10ë²ˆì§¸ í•­: {result}")
"""

# ë‹¤ì–‘í•œ ì§ˆë¬¸ìœ¼ë¡œ ì½”ë“œ ë¶„ì„
code_questions = [
    "ì´ ì½”ë“œë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”",
    "ì‹œê°„ ë³µì¡ë„ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
    "ì´ ì½”ë“œë¥¼ ê°œì„ í•  ë°©ë²•ì´ ìˆë‚˜ìš”?",
    "ì¬ê·€ì  ë°©ë²•ê³¼ ë¹„êµí•˜ë©´ ì–´ë–¤ ì¥ì ì´ ìˆë‚˜ìš”?"
]

for question in code_questions:
    assistant.analyze_code(sample_code, question)

# 2. ìˆ˜í•™ ë¬¸ì œ í•´ê²° ì˜ˆì œ
math_problems = [
    "x^2 - 5x + 6 = 0ì˜ í•´ë¥¼ êµ¬í•˜ì„¸ìš”.",
    "ë°˜ì§€ë¦„ì´ 7cmì¸ ì›ì˜ ë„“ì´ë¥¼ êµ¬í•˜ì„¸ìš”.",
    "1ë¶€í„° 100ê¹Œì§€ì˜ ìì—°ìˆ˜ì˜ í•©ì„ êµ¬í•˜ì„¸ìš”."
]

for problem in math_problems:
    solution = assistant.solve_math_step_by_step(problem)
```

---

## 4ì¥: ì‹¤ì „ í”„ë¡œì íŠ¸ - ë‚˜ë§Œì˜ ì´ë¯¸ì§€ ì„¤ëª… AI ë§Œë“¤ê¸°

ì´ì œ ë°°ìš´ ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ ì‹¤ì œë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë§Œë“¤ì–´ë´…ì‹œë‹¤!

### 4.1 í”„ë¡œì íŠ¸ ê°œìš”: ìŠ¤ë§ˆíŠ¸ ì‚¬ì§„ ì¼ê¸°

ìš°ë¦¬ê°€ ë§Œë“¤ ì• í”Œë¦¬ì¼€ì´ì…˜ì€ ë‹¤ìŒê³¼ ê°™ì€ ê¸°ëŠ¥ì„ ê°€ì§‘ë‹ˆë‹¤:
- ğŸ“¸ ì‚¬ì§„ì„ ì°ê±°ë‚˜ ì—…ë¡œë“œ
- ğŸ’¬ AIê°€ ì‚¬ì§„ì„ ë¶„ì„í•˜ì—¬ ì¼ê¸° ì‘ì„±
- ğŸ¨ ê°ì •ê³¼ ë¶„ìœ„ê¸° ë¶„ì„
- ğŸ“… ë‚ ì§œë³„ë¡œ ì €ì¥ ë° ê´€ë¦¬

### 4.2 ì „ì²´ ì‹œìŠ¤í…œ êµ¬ì¡°

```python
# ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì‹œê°í™”
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.patches import FancyBboxPatch, Circle, Arrow
import numpy as np

fig, ax = plt.subplots(figsize=(14, 10))
ax.set_xlim(0, 14)
ax.set_ylim(0, 10)
ax.set_facecolor('#f5f5f5')

# ì»´í¬ë„ŒíŠ¸ ì •ì˜
components = [
    # (x, y, width, height, label, color)
    (1, 7, 3, 1.5, "ì¹´ë©”ë¼/ê°¤ëŸ¬ë¦¬\nì¸í„°í˜ì´ìŠ¤", '#FF6B6B'),
    (5, 7, 3, 1.5, "ì´ë¯¸ì§€ ì „ì²˜ë¦¬\nëª¨ë“ˆ", '#FFA500'),
    (9, 7, 3, 1.5, "ë©€í‹°ëª¨ë‹¬ AI\në¶„ì„ ì—”ì§„", '#4ECDC4'),
    (1, 4, 3, 1.5, "ê°ì • ë¶„ì„\nëª¨ë“ˆ", '#9B59B6'),
    (5, 4, 3, 1.5, "ì¼ê¸° ìƒì„±\nëª¨ë“ˆ", '#3498DB'),
    (9, 4, 3, 1.5, "ë°ì´í„° ì €ì¥\nì‹œìŠ¤í…œ", '#2ECC71'),
    (5, 1, 3, 1.5, "ì‚¬ìš©ì\nì¸í„°í˜ì´ìŠ¤", '#E74C3C')
]

# ì»´í¬ë„ŒíŠ¸ ê·¸ë¦¬ê¸°
for x, y, w, h, label, color in components:
    box = FancyBboxPatch((x, y), w, h,
                        boxstyle="round,pad=0.1",
                        facecolor=color,
                        edgecolor='black',
                        linewidth=2,
                        alpha=0.8)
    ax.add_patch(box)
    ax.text(x + w/2, y + h/2, label,
           ha='center', va='center',
           fontsize=11, fontweight='bold',
           color='white')

# ì—°ê²°ì„  ê·¸ë¦¬ê¸°
connections = [
    ((2.5, 7), (5, 7.75)),      # ì¹´ë©”ë¼ â†’ ì „ì²˜ë¦¬
    ((8, 7.75), (9, 7.75)),     # ì „ì²˜ë¦¬ â†’ AI
    ((10.5, 7), (10.5, 5.5)),   # AI â†’ ì €ì¥
    ((9, 7.75), (4, 5.5)),      # AI â†’ ê°ì •ë¶„ì„
    ((2.5, 4), (5, 4.75)),      # ê°ì •ë¶„ì„ â†’ ì¼ê¸°ìƒì„±
    ((8, 4.75), (9, 4.75)),     # ì¼ê¸°ìƒì„± â†’ ì €ì¥
    ((6.5, 4), (6.5, 2.5)),     # ì¼ê¸°ìƒì„± â†’ UI
    ((9, 4.75), (8, 2.5)),      # ì €ì¥ â†’ UI
]

for start, end in connections:
    ax.annotate('', xy=end, xytext=start,
               arrowprops=dict(arrowstyle='->', lw=2,
                             color='#34495E', alpha=0.7))

# ì œëª©
ax.text(7, 9, 'ìŠ¤ë§ˆíŠ¸ ì‚¬ì§„ ì¼ê¸° ì‹œìŠ¤í…œ êµ¬ì¡°',
       fontsize=16, fontweight='bold', ha='center')

ax.axis('off')
plt.tight_layout()
plt.show()
```

### 4.3 í•µì‹¬ ëª¨ë“ˆ êµ¬í˜„

```python
# smart_photo_diary.py - ìŠ¤ë§ˆíŠ¸ ì‚¬ì§„ ì¼ê¸° ì• í”Œë¦¬ì¼€ì´ì…˜

import torch
from transformers import Blip2Processor, Blip2ForConditionalGeneration
from transformers import pipeline
from PIL import Image
import cv2
import numpy as np
from datetime import datetime
import json
import os
from typing import Dict, List, Optional
import gradio as gr

class SmartPhotoDiary:
    """AI ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ì‚¬ì§„ ì¼ê¸° ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        print("ğŸ“” ìŠ¤ë§ˆíŠ¸ ì‚¬ì§„ ì¼ê¸° ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        
        # 1. ì´ë¯¸ì§€ ë¶„ì„ ëª¨ë¸
        self.init_image_analyzer()
        
        # 2. ê°ì • ë¶„ì„ ëª¨ë¸
        self.init_emotion_analyzer()
        
        # 3. ë°ì´í„° ì €ì¥ ê²½ë¡œ
        self.diary_path = "photo_diary"
        os.makedirs(self.diary_path, exist_ok=True)
        
        print("âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def init_image_analyzer(self):
        """ì´ë¯¸ì§€ ë¶„ì„ ëª¨ë¸ ì´ˆê¸°í™”"""
        print("  ğŸ–¼ï¸ ì´ë¯¸ì§€ ë¶„ì„ ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        model_name = "Salesforce/blip2-opt-2.7b"
        self.image_processor = Blip2Processor.from_pretrained(model_name)
        
        # GPU/CPU ìë™ ì„ íƒ
        device = "cuda" if torch.cuda.is_available() else "cpu"
        dtype = torch.float16 if device == "cuda" else torch.float32
        
        self.image_model = Blip2ForConditionalGeneration.from_pretrained(
            model_name,
            torch_dtype=dtype,
            device_map="auto" if device == "cuda" else None
        )
        
        if device == "cpu":
            self.image_model = self.image_model.to(device)
    
    def init_emotion_analyzer(self):
        """ê°ì • ë¶„ì„ ëª¨ë¸ ì´ˆê¸°í™”"""
        print("  ğŸ˜Š ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        # ë‹¤êµ­ì–´ ê°ì • ë¶„ì„ íŒŒì´í”„ë¼ì¸
        self.emotion_analyzer = pipeline(
            "text-classification",
            model="j-hartmann/emotion-english-distilroberta-base",
            device=0 if torch.cuda.is_available() else -1
        )
        
        # ê°ì • ì´ëª¨ì§€ ë§¤í•‘
        self.emotion_emojis = {
            'joy': 'ğŸ˜Š',
            'sadness': 'ğŸ˜¢',
            'anger': 'ğŸ˜ ',
            'fear': 'ğŸ˜¨',
            'surprise': 'ğŸ˜²',
            'disgust': 'ğŸ¤¢',
            'neutral': 'ğŸ˜'
        }
    
    def analyze_image(self, image: Image.Image) -> Dict:
        """ì´ë¯¸ì§€ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„"""
        
        results = {}
        
        # 1. ì¥ë©´ ì„¤ëª…
        scene_prompt = "Describe this image in detail:"
        scene_inputs = self.image_processor(image, scene_prompt, return_tensors="pt")
        
        if torch.cuda.is_available():
            scene_inputs = {k: v.cuda() for k, v in scene_inputs.items()}
        
        with torch.no_grad():
            scene_ids = self.image_model.generate(**scene_inputs, max_new_tokens=100)
        
        scene_description = self.image_processor.decode(scene_ids[0], skip_special_tokens=True)
        results['scene'] = scene_description
        
        # 2. ì£¼ìš” ê°ì²´ ì°¾ê¸°
        objects_prompt = "What are the main objects in this image?"
        objects_inputs = self.image_processor(image, objects_prompt, return_tensors="pt")
        
        if torch.cuda.is_available():
            objects_inputs = {k: v.cuda() for k, v in objects_inputs.items()}
        
        with torch.no_grad():
            objects_ids = self.image_model.generate(**objects_inputs, max_new_tokens=50)
        
        objects_description = self.image_processor.decode(objects_ids[0], skip_special_tokens=True)
        results['objects'] = objects_description
        
        # 3. ë¶„ìœ„ê¸° íŒŒì•…
        mood_prompt = "What is the mood or atmosphere of this image?"
        mood_inputs = self.image_processor(image, mood_prompt, return_tensors="pt")
        
        if torch.cuda.is_available():
            mood_inputs = {k: v.cuda() for k, v in mood_inputs.items()}
        
        with torch.no_grad():
            mood_ids = self.image_model.generate(**mood_inputs, max_new_tokens=50)
        
        mood_description = self.image_processor.decode(mood_ids[0], skip_special_tokens=True)
        results['mood'] = mood_description
        
        # 4. ìƒ‰ìƒ ë¶„ì„
        results['colors'] = self.analyze_colors(image)
        
        return results
    
    def analyze_colors(self, image: Image.Image) -> Dict:
        """ì´ë¯¸ì§€ì˜ ì£¼ìš” ìƒ‰ìƒ ë¶„ì„"""
        
        # PIL ì´ë¯¸ì§€ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
        img_array = np.array(image)
        
        # ìƒ‰ìƒ íˆìŠ¤í† ê·¸ë¨ ê³„ì‚°
        if len(img_array.shape) == 3:
            # RGB ì´ë¯¸ì§€
            avg_color = img_array.mean(axis=(0, 1))
            
            # ì£¼ìš” ìƒ‰ìƒ íŒë‹¨
            r, g, b = avg_color
            
            if r > g and r > b:
                dominant = "ë¹¨ê°„ìƒ‰ ê³„ì—´"
            elif g > r and g > b:
                dominant = "ë…¹ìƒ‰ ê³„ì—´"
            elif b > r and b > g:
                dominant = "íŒŒë€ìƒ‰ ê³„ì—´"
            else:
                dominant = "ì¤‘ì„± ìƒ‰ìƒ"
            
            # ë°ê¸° íŒë‹¨
            brightness = avg_color.mean()
            if brightness > 200:
                brightness_desc = "ë°ì€"
            elif brightness > 100:
                brightness_desc = "ë³´í†µ ë°ê¸°ì˜"
            else:
                brightness_desc = "ì–´ë‘ìš´"
            
            return {
                'dominant': dominant,
                'brightness': brightness_desc,
                'avg_rgb': avg_color.tolist()
            }
        else:
            return {
                'dominant': "í‘ë°±",
                'brightness': "í‘ë°± ì´ë¯¸ì§€",
                'avg_rgb': [0, 0, 0]
            }
    
    def generate_diary_entry(self, image_analysis: Dict, user_context: str = "") -> Dict:
        """ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¼ê¸° í•­ëª© ìƒì„±"""
        
        # í˜„ì¬ ì‹œê°„
        now = datetime.now()
        
        # ì¼ê¸° ë‚´ìš© ìƒì„±
        diary_text = f"""
ì˜¤ëŠ˜ {now.strftime('%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„')}ì˜ ìˆœê°„ì„ ê¸°ë¡í•©ë‹ˆë‹¤.

ğŸ“¸ ì¥ë©´ ë¬˜ì‚¬:
{image_analysis['scene']}

ğŸ¨ ë°œê²¬í•œ ê²ƒë“¤:
{image_analysis['objects']}

ğŸ’« ë¶„ìœ„ê¸°:
{image_analysis['mood']}

ğŸ¨ ìƒ‰ê°:
ì˜¤ëŠ˜ì˜ ì‚¬ì§„ì€ {image_analysis['colors']['brightness']} {image_analysis['colors']['dominant']}ì´ 
ì¸ìƒì ì´ì—ˆìŠµë‹ˆë‹¤.
"""
        
        # ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        if user_context:
            diary_text += f"\nâœï¸ ë‚˜ì˜ ë©”ëª¨:\n{user_context}\n"
        
        # ê°ì • ë¶„ì„
        emotion_results = self.emotion_analyzer(diary_text)
        primary_emotion = emotion_results[0]
        
        diary_text += f"\n{self.emotion_emojis.get(primary_emotion['label'], 'ğŸ˜Š')} " \
                     f"ì˜¤ëŠ˜ì˜ ê°ì •: {primary_emotion['label']} " \
                     f"({primary_emotion['score']*100:.1f}%)"
        
        return {
            'date': now.isoformat(),
            'text': diary_text,
            'analysis': image_analysis,
            'emotion': primary_emotion,
            'user_context': user_context
        }
    
    def save_diary_entry(self, image: Image.Image, diary_entry: Dict) -> str:
        """ì¼ê¸° í•­ëª©ì„ ì €ì¥"""
        
        # ë‚ ì§œë³„ í´ë” ìƒì„±
        date_str = datetime.now().strftime('%Y%m%d')
        day_folder = os.path.join(self.diary_path, date_str)
        os.makedirs(day_folder, exist_ok=True)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ íŒŒì¼ëª… ìƒì„±
        timestamp = datetime.now().strftime('%H%M%S')
        
        # ì´ë¯¸ì§€ ì €ì¥
        image_path = os.path.join(day_folder, f"{timestamp}_photo.jpg")
        image.save(image_path)
        
        # ì¼ê¸° ë°ì´í„° ì €ì¥
        diary_path = os.path.join(day_folder, f"{timestamp}_diary.json")
        with open(diary_path, 'w', encoding='utf-8') as f:
            json.dump(diary_entry, f, ensure_ascii=False, indent=2)
        
        # í…ìŠ¤íŠ¸ ë²„ì „ë„ ì €ì¥
        text_path = os.path.join(day_folder, f"{timestamp}_diary.txt")
        with open(text_path, 'w', encoding='utf-8') as f:
            f.write(diary_entry['text'])
        
        return day_folder
    
    def create_gradio_interface(self):
        """Gradio ì›¹ ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
        
        def process_image(image, user_memo):
            """ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ì¼ê¸° ìƒì„±"""
            
            if image is None:
                return "ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”!", None
            
            # PIL Imageë¡œ ë³€í™˜
            if isinstance(image, np.ndarray):
                image = Image.fromarray(image)
            
            # ì´ë¯¸ì§€ ë¶„ì„
            print("ğŸ” ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
            analysis = self.analyze_image(image)
            
            # ì¼ê¸° ìƒì„±
            print("âœï¸ ì¼ê¸°ë¥¼ ì‘ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
            diary_entry = self.generate_diary_entry(analysis, user_memo)
            
            # ì €ì¥
            print("ğŸ’¾ ì¼ê¸°ë¥¼ ì €ì¥í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
            save_path = self.save_diary_entry(image, diary_entry)
            
            # ê²°ê³¼ ì´ë¯¸ì§€ ìƒì„± (ì¼ê¸° ë‚´ìš©ì„ ì´ë¯¸ì§€ì— ì˜¤ë²„ë ˆì´)
            result_image = self.create_diary_image(image, diary_entry)
            
            return diary_entry['text'], result_image
        
        # Gradio ì¸í„°í˜ì´ìŠ¤
        with gr.Blocks(title="ìŠ¤ë§ˆíŠ¸ ì‚¬ì§„ ì¼ê¸°", theme=gr.themes.Soft()) as demo:
            gr.Markdown("""
            # ğŸ“” ìŠ¤ë§ˆíŠ¸ ì‚¬ì§„ ì¼ê¸°
            
            ì‚¬ì§„ì„ ì—…ë¡œë“œí•˜ë©´ AIê°€ ìë™ìœ¼ë¡œ ì¼ê¸°ë¥¼ ì‘ì„±í•´ë“œë¦½ë‹ˆë‹¤!
            """)
            
            with gr.Row():
                with gr.Column():
                    image_input = gr.Image(
                        label="ì‚¬ì§„ ì—…ë¡œë“œ",
                        type="numpy"
                    )
                    memo_input = gr.Textbox(
                        label="ë©”ëª¨ (ì„ íƒì‚¬í•­)",
                        placeholder="ì˜¤ëŠ˜ì˜ íŠ¹ë³„í•œ ìˆœê°„ì´ë‚˜ ëŠë‚Œì„ ì ì–´ì£¼ì„¸ìš”...",
                        lines=3
                    )
                    submit_btn = gr.Button("ì¼ê¸° ì‘ì„±í•˜ê¸°", variant="primary")
                
                with gr.Column():
                    diary_output = gr.Textbox(
                        label="AIê°€ ì‘ì„±í•œ ì¼ê¸°",
                        lines=15
                    )
                    result_image = gr.Image(
                        label="ì¼ê¸°ê°€ í¬í•¨ëœ ì´ë¯¸ì§€"
                    )
            
            # ì˜ˆì‹œ ì¶”ê°€
            gr.Examples(
                examples=[
                    ["ì˜¤ëŠ˜ ê³µì›ì—ì„œ ì‚°ì±…í–ˆì–´ìš”"],
                    ["ë§›ìˆëŠ” ìŒì‹ì„ ë¨¹ì—ˆìŠµë‹ˆë‹¤"],
                    ["ì¹œêµ¬ë“¤ê³¼ ì¦ê±°ìš´ ì‹œê°„"],
                    ["í˜¼ìë§Œì˜ ì¡°ìš©í•œ ì‹œê°„"]
                ],
                inputs=memo_input
            )
            
            submit_btn.click(
                fn=process_image,
                inputs=[image_input, memo_input],
                outputs=[diary_output, result_image]
            )
        
        return demo
    
    def create_diary_image(self, original_image: Image.Image, diary_entry: Dict) -> Image.Image:
        """ì¼ê¸° ë‚´ìš©ì„ ì´ë¯¸ì§€ì— ì˜¤ë²„ë ˆì´"""
        
        # ì›ë³¸ ì´ë¯¸ì§€ ë³µì‚¬
        img = original_image.copy()
        
        # numpy ë°°ì—´ë¡œ ë³€í™˜
        img_array = np.array(img)
        
        # ë°˜íˆ¬ëª… ì˜¤ë²„ë ˆì´ ì¶”ê°€
        overlay = img_array.copy()
        cv2.rectangle(overlay, (0, 0), (img_array.shape[1], 150), (0, 0, 0), -1)
        img_array = cv2.addWeighted(img_array, 0.7, overlay, 0.3, 0)
        
        # í…ìŠ¤íŠ¸ ì¶”ê°€ (OpenCVëŠ” í•œê¸€ ì§€ì›ì´ ì œí•œì ì´ë¯€ë¡œ ì˜ë¬¸ìœ¼ë¡œ)
        date_str = datetime.now().strftime('%Y-%m-%d %H:%M')
        emotion = diary_entry['emotion']['label']
        
        cv2.putText(img_array, f"Photo Diary - {date_str}", 
                   (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
        cv2.putText(img_array, f"Emotion: {emotion}", 
                   (20, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
        cv2.putText(img_array, f"Mood: {diary_entry['analysis']['mood'][:50]}...", 
                   (20, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
        
        return Image.fromarray(img_array)

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰
def run_smart_photo_diary():
    """ìŠ¤ë§ˆíŠ¸ ì‚¬ì§„ ì¼ê¸° ì•± ì‹¤í–‰"""
    
    diary = SmartPhotoDiary()
    interface = diary.create_gradio_interface()
    
    print("\nğŸš€ ìŠ¤ë§ˆíŠ¸ ì‚¬ì§„ ì¼ê¸°ê°€ ì‹¤í–‰ë©ë‹ˆë‹¤!")
    print("ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ ìë™ìœ¼ë¡œ ì—´ë¦½ë‹ˆë‹¤...")
    
    interface.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=True  # ê³µìœ  ë§í¬ ìƒì„±
    )

# ì‹¤í–‰
if __name__ == "__main__":
    run_smart_photo_diary()
```

### 4.4 ê³ ê¸‰ ê¸°ëŠ¥ ì¶”ê°€

ì´ì œ ë” í¥ë¯¸ë¡œìš´ ê¸°ëŠ¥ë“¤ì„ ì¶”ê°€í•´ë´…ì‹œë‹¤!

```python
# advanced_features.py - ê³ ê¸‰ ê¸°ëŠ¥ ëª¨ë“ˆ

import calendar
from collections import defaultdict
import plotly.graph_objects as go
import plotly.express as px
from wordcloud import WordCloud
import matplotlib.pyplot as plt

class DiaryAnalytics:
    """ì¼ê¸° ë¶„ì„ ë° ì‹œê°í™” ê¸°ëŠ¥"""
    
    def __init__(self, diary_path="photo_diary"):
        self.diary_path = diary_path
        self.entries = self.load_all_entries()
    
    def load_all_entries(self) -> List[Dict]:
        """ëª¨ë“  ì¼ê¸° í•­ëª© ë¡œë“œ"""
        entries = []
        
        for date_folder in os.listdir(self.diary_path):
            folder_path = os.path.join(self.diary_path, date_folder)
            if os.path.isdir(folder_path):
                for file in os.listdir(folder_path):
                    if file.endswith('_diary.json'):
                        with open(os.path.join(folder_path, file), 'r', encoding='utf-8') as f:
                            entry = json.load(f)
                            entries.append(entry)
        
        return sorted(entries, key=lambda x: x['date'])
    
    def emotion_timeline(self):
        """ê°ì • ë³€í™” íƒ€ì„ë¼ì¸ ìƒì„±"""
        
        dates = []
        emotions = []
        scores = []
        
        for entry in self.entries:
            date = datetime.fromisoformat(entry['date'])
            dates.append(date)
            emotions.append(entry['emotion']['label'])
            scores.append(entry['emotion']['score'])
        
        # Plotlyë¡œ ì¸í„°ë™í‹°ë¸Œ ì°¨íŠ¸ ìƒì„±
        fig = go.Figure()
        
        # ê°ì •ë³„ ìƒ‰ìƒ
        emotion_colors = {
            'joy': '#FFD93D',
            'sadness': '#6BBAEC',
            'anger': '#FF6B6B',
            'fear': '#A8E6CF',
            'surprise': '#FFB6C1',
            'disgust': '#C3AED6',
            'neutral': '#D3D3D3'
        }
        
        for emotion in emotion_colors:
            emotion_dates = [d for d, e in zip(dates, emotions) if e == emotion]
            emotion_scores = [s for e, s in zip(emotions, scores) if e == emotion]
            
            fig.add_trace(go.Scatter(
                x=emotion_dates,
                y=emotion_scores,
                mode='markers+lines',
                name=emotion,
                marker=dict(
                    size=10,
                    color=emotion_colors[emotion]
                ),
                line=dict(
                    color=emotion_colors[emotion],
                    width=2
                )
            ))
        
        fig.update_layout(
            title="ê°ì • ë³€í™” íƒ€ì„ë¼ì¸",
            xaxis_title="ë‚ ì§œ",
            yaxis_title="ê°ì • ê°•ë„",
            hovermode='x unified',
            template='plotly_white'
        )
        
        return fig
    
    def monthly_mood_calendar(self, year=None, month=None):
        """ì›”ë³„ ê°ì • ìº˜ë¦°ë” ìƒì„±"""
        
        if year is None:
            year = datetime.now().year
        if month is None:
            month = datetime.now().month
        
        # í•´ë‹¹ ì›”ì˜ ì¼ê¸°ë“¤ í•„í„°ë§
        month_entries = defaultdict(list)
        
        for entry in self.entries:
            entry_date = datetime.fromisoformat(entry['date'])
            if entry_date.year == year and entry_date.month == month:
                day = entry_date.day
                month_entries[day].append(entry['emotion']['label'])
        
        # ìº˜ë¦°ë” ìƒì„±
        fig, ax = plt.subplots(figsize=(12, 8))
        
        # ì›”ì˜ ì²«ë‚ ê³¼ ë§ˆì§€ë§‰ë‚ 
        first_day = datetime(year, month, 1)
        last_day = datetime(year, month, calendar.monthrange(year, month)[1])
        
        # ìº˜ë¦°ë” ê·¸ë¦¬ë“œ
        cal = calendar.monthcalendar(year, month)
        
        # ê°ì • ì´ëª¨ì§€
        emotion_emojis = {
            'joy': 'ğŸ˜Š',
            'sadness': 'ğŸ˜¢',
            'anger': 'ğŸ˜ ',
            'fear': 'ğŸ˜¨',
            'surprise': 'ğŸ˜²',
            'disgust': 'ğŸ¤¢',
            'neutral': 'ğŸ˜'
        }
        
        # ìº˜ë¦°ë” ê·¸ë¦¬ê¸°
        for week_num, week in enumerate(cal):
            for day_num, day in enumerate(week):
                if day == 0:
                    continue
                
                x = day_num
                y = len(cal) - week_num - 1
                
                # ë‚ ì§œ í‘œì‹œ
                ax.text(x, y + 0.4, str(day), 
                       ha='center', va='center', fontsize=12)
                
                # ê°ì • í‘œì‹œ
                if day in month_entries:
                    emotions = month_entries[day]
                    # ê°€ì¥ ë¹ˆë²ˆí•œ ê°ì •
                    most_common = max(set(emotions), key=emotions.count)
                    emoji = emotion_emojis.get(most_common, 'ğŸ˜Š')
                    
                    ax.text(x, y - 0.2, emoji, 
                           ha='center', va='center', fontsize=20)
        
        # ìŠ¤íƒ€ì¼ë§
        ax.set_xlim(-0.5, 6.5)
        ax.set_ylim(-0.5, len(cal) - 0.5)
        ax.set_xticks(range(7))
        ax.set_xticklabels(['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼'])
        ax.set_yticks([])
        
        # ê²©ì ì¶”ê°€
        for i in range(8):
            ax.axvline(i - 0.5, color='gray', alpha=0.3)
        for i in range(len(cal) + 1):
            ax.axhline(i - 0.5, color='gray', alpha=0.3)
        
        ax.set_title(f'{year}ë…„ {month}ì›” ê°ì • ìº˜ë¦°ë”', 
                    fontsize=16, fontweight='bold', pad=20)
        
        return fig
    
    def word_cloud_from_diaries(self):
        """ì¼ê¸° ë‚´ìš©ìœ¼ë¡œ ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„±"""
        
        # ëª¨ë“  ì¼ê¸° í…ìŠ¤íŠ¸ ìˆ˜ì§‘
        all_text = ' '.join([entry['text'] for entry in self.entries])
        
        # í•œêµ­ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì„¤ì •
        # (ì‹¤ì œë¡œëŠ” konlpy ë“±ì„ ì‚¬ìš©í•˜ì—¬ í˜•íƒœì†Œ ë¶„ì„ í•„ìš”)
        
        # ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„±
        wordcloud = WordCloud(
            width=800,
            height=400,
            background_color='white',
            colormap='viridis',
            font_path='NanumGothic.ttf'  # í•œê¸€ í°íŠ¸ ê²½ë¡œ
        ).generate(all_text)
        
        # ì‹œê°í™”
        plt.figure(figsize=(12, 6))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off')
        plt.title('ë‚´ ì¼ê¸°ì˜ ì£¼ìš” ë‹¨ì–´ë“¤', fontsize=16, fontweight='bold')
        
        return plt.gcf()
    
    def photo_collage(self, max_photos=9):
        """ìµœê·¼ ì‚¬ì§„ë“¤ë¡œ ì½œë¼ì£¼ ìƒì„±"""
        
        photos = []
        
        # ìµœê·¼ ì‚¬ì§„ë“¤ ìˆ˜ì§‘
        for entry in self.entries[-max_photos:]:
            date = datetime.fromisoformat(entry['date'])
            date_str = date.strftime('%Y%m%d')
            time_str = date.strftime('%H%M%S')
            
            photo_path = os.path.join(
                self.diary_path, 
                date_str, 
                f"{time_str}_photo.jpg"
            )
            
            if os.path.exists(photo_path):
                img = Image.open(photo_path)
                photos.append(img)
        
        if not photos:
            return None
        
        # ì½œë¼ì£¼ í¬ê¸° ê³„ì‚°
        n = len(photos)
        rows = int(np.sqrt(n))
        cols = int(np.ceil(n / rows))
        
        # ê° ì‚¬ì§„ í¬ê¸°
        photo_size = (300, 300)
        
        # ì½œë¼ì£¼ ìƒì„±
        collage = Image.new('RGB', (cols * photo_size[0], rows * photo_size[1]))
        
        for i, photo in enumerate(photos):
            # ì‚¬ì§„ í¬ê¸° ì¡°ì •
            photo = photo.resize(photo_size, Image.Resampling.LANCZOS)
            
            # ìœ„ì¹˜ ê³„ì‚°
            row = i // cols
            col = i % cols
            
            # ì½œë¼ì£¼ì— ì¶”ê°€
            collage.paste(photo, (col * photo_size[0], row * photo_size[1]))
        
        return collage

# ë¶„ì„ ëŒ€ì‹œë³´ë“œ ìƒì„±
def create_analytics_dashboard(diary_path="photo_diary"):
    """ë¶„ì„ ëŒ€ì‹œë³´ë“œ Gradio ì¸í„°í˜ì´ìŠ¤"""
    
    analytics = DiaryAnalytics(diary_path)
    
    with gr.Blocks(title="ì¼ê¸° ë¶„ì„ ëŒ€ì‹œë³´ë“œ", theme=gr.themes.Soft()) as dashboard:
        gr.Markdown("""
        # ğŸ“Š ë‚˜ì˜ ì‚¬ì§„ ì¼ê¸° ë¶„ì„
        
        AIê°€ ë¶„ì„í•œ ë‚˜ì˜ ê°ì • ë³€í™”ì™€ ì¼ê¸° íŒ¨í„´ì„ í™•ì¸í•´ë³´ì„¸ìš”!
        """)
        
        with gr.Tab("ê°ì • íƒ€ì„ë¼ì¸"):
            timeline_plot = gr.Plot(
                label="ì‹œê°„ì— ë”°ë¥¸ ê°ì • ë³€í™”"
            )
            
            def update_timeline():
                return analytics.emotion_timeline()
            
            timeline_btn = gr.Button("íƒ€ì„ë¼ì¸ ì—…ë°ì´íŠ¸")
            timeline_btn.click(fn=update_timeline, outputs=timeline_plot)
        
        with gr.Tab("ì›”ë³„ ê°ì • ìº˜ë¦°ë”"):
            with gr.Row():
                year_input = gr.Number(
                    label="ë…„ë„",
                    value=datetime.now().year,
                    precision=0
                )
                month_input = gr.Number(
                    label="ì›”",
                    value=datetime.now().month,
                    minimum=1,
                    maximum=12,
                    precision=0
                )
            
            calendar_plot = gr.Plot(label="ê°ì • ìº˜ë¦°ë”")
            
            def update_calendar(year, month):
                return analytics.monthly_mood_calendar(int(year), int(month))
            
            calendar_btn = gr.Button("ìº˜ë¦°ë” ìƒì„±")
            calendar_btn.click(
                fn=update_calendar,
                inputs=[year_input, month_input],
                outputs=calendar_plot
            )
        
        with gr.Tab("ì›Œë“œ í´ë¼ìš°ë“œ"):
            wordcloud_plot = gr.Plot(label="ì¼ê¸° ì£¼ìš” ë‹¨ì–´")
            
            def generate_wordcloud():
                return analytics.word_cloud_from_diaries()
            
            wordcloud_btn = gr.Button("ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„±")
            wordcloud_btn.click(fn=generate_wordcloud, outputs=wordcloud_plot)
        
        with gr.Tab("ì‚¬ì§„ ì½œë¼ì£¼"):
            collage_image = gr.Image(label="ìµœê·¼ ì‚¬ì§„ ëª¨ìŒ")
            
            def create_collage():
                return analytics.photo_collage()
            
            collage_btn = gr.Button("ì½œë¼ì£¼ ë§Œë“¤ê¸°")
            collage_btn.click(fn=create_collage, outputs=collage_image)
    
    return dashboard
```

### 4.5 í”„ë¡œì íŠ¸ ì™„ì„± ë° ì‹¤í–‰

```python
# main.py - ì „ì²´ ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰

def run_complete_photo_diary_system():
    """ì™„ì „í•œ ìŠ¤ë§ˆíŠ¸ ì‚¬ì§„ ì¼ê¸° ì‹œìŠ¤í…œ ì‹¤í–‰"""
    
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘     ğŸ“” ìŠ¤ë§ˆíŠ¸ ì‚¬ì§„ ì¼ê¸° ì‹œìŠ¤í…œ ğŸ“”      â•‘
    â•‘                                      â•‘
    â•‘  AIê°€ ë‹¹ì‹ ì˜ ì¼ìƒì„ íŠ¹ë³„í•˜ê²Œ ê¸°ë¡í•©ë‹ˆë‹¤  â•‘
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # ë©”ë‰´ ì„ íƒ
    print("\në¬´ì—‡ì„ í•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
    print("1. ìƒˆë¡œìš´ ì¼ê¸° ì‘ì„±")
    print("2. ì¼ê¸° ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
    print("3. ì „ì²´ ì‹œìŠ¤í…œ ì‹¤í–‰")
    
    choice = input("\nì„ íƒ (1-3): ")
    
    if choice == "1":
        # ì¼ê¸° ì‘ì„± ëª¨ë“œ
        diary = SmartPhotoDiary()
        interface = diary.create_gradio_interface()
        interface.launch(share=True)
        
    elif choice == "2":
        # ë¶„ì„ ëŒ€ì‹œë³´ë“œ
        dashboard = create_analytics_dashboard()
        dashboard.launch(share=True)
        
    elif choice == "3":
        # ì „ì²´ ì‹œìŠ¤í…œ (íƒ­ìœ¼ë¡œ êµ¬ì„±)
        with gr.Blocks(title="ìŠ¤ë§ˆíŠ¸ ì‚¬ì§„ ì¼ê¸° - ì „ì²´ ì‹œìŠ¤í…œ") as app:
            with gr.Tab("ğŸ“ ì¼ê¸° ì‘ì„±"):
                diary = SmartPhotoDiary()
                diary_interface = diary.create_gradio_interface()
                
            with gr.Tab("ğŸ“Š ë¶„ì„ ëŒ€ì‹œë³´ë“œ"):
                analytics_interface = create_analytics_dashboard()
        
        app.launch(share=True)
    
    else:
        print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")

if __name__ == "__main__":
    run_complete_photo_diary_system()
```

---

## Day 1 ë§ˆë¬´ë¦¬ ë° ê³¼ì œ

### ì˜¤ëŠ˜ ë°°ìš´ ë‚´ìš© ì •ë¦¬

1. **ê°œë°œ í™˜ê²½ êµ¬ì¶•**
   - Python ê°€ìƒ í™˜ê²½ ì„¤ì •
   - í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
   - GPU/CPU í™˜ê²½ í™•ì¸

2. **ë©€í‹°ëª¨ë‹¬ AI ì´ë¡ **
   - ë©€í‹°ëª¨ë‹¬ AIì˜ ê°œë…ê³¼ ë°œì „ ê³¼ì •
   - ìµœì‹  ëª¨ë¸ë“¤ì˜ íŠ¹ì§•ê³¼ ì¥ë‹¨ì 
   - ì‹¤ì œ í™œìš© ì‚¬ë¡€

3. **ë‹¤ì–‘í•œ ëª¨ë¸ ì‹¤ìŠµ**
   - BLIP-2: ê¸°ë³¸ì ì¸ ì´ë¯¸ì§€ ì´í•´
   - MiniCPM-V: ë¹„ë””ì˜¤ ë¶„ì„
   - InternVL: ë¬¸ì„œì™€ ì°¨íŠ¸ ì´í•´
   - Phi-4: ì½”ë“œì™€ ìˆ˜í•™ ë¬¸ì œ í•´ê²°

4. **ì‹¤ì „ í”„ë¡œì íŠ¸**
   - ìŠ¤ë§ˆíŠ¸ ì‚¬ì§„ ì¼ê¸° ì‹œìŠ¤í…œ êµ¬ì¶•
   - ê°ì • ë¶„ì„ ê¸°ëŠ¥ ì¶”ê°€
   - ë°ì´í„° ì‹œê°í™” ë° ë¶„ì„

### ì‹¤ìŠµ ê³¼ì œ

1. **ê¸°ë³¸ ê³¼ì œ**
   - ìì‹ ì˜ ì‚¬ì§„ 10ì¥ìœ¼ë¡œ ì¼ê¸° ì‘ì„±í•´ë³´ê¸°
   - ë‹¤ì–‘í•œ ì§ˆë¬¸ìœ¼ë¡œ ì´ë¯¸ì§€ ë¶„ì„ í…ŒìŠ¤íŠ¸
   - ê°ì • ìº˜ë¦°ë” í™•ì¸í•˜ê¸°

2. **ë„ì „ ê³¼ì œ**
   - ìƒˆë¡œìš´ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ì¶”ê°€í•˜ê¸°
   - ìŒì„± ë©”ëª¨ ê¸°ëŠ¥ ì¶”ê°€
   - ì¼ê¸°ë¥¼ PDFë¡œ ë‚´ë³´ë‚´ê¸° ê¸°ëŠ¥

3. **ì°½ì˜ ê³¼ì œ**
   - ê°€ì¡± ì•¨ë²” ì •ë¦¬ AI ë§Œë“¤ê¸°
   - ì—¬í–‰ ì‚¬ì§„ ìë™ ë¶„ë¥˜ê¸°
   - ìš”ë¦¬ ë ˆì‹œí”¼ ì¶”ì²œ ì‹œìŠ¤í…œ

### ë‚´ì¼ ì˜ˆê³ 

ë‚´ì¼ì€ ë”ìš± í¥ë¯¸ì§„ì§„í•œ **ì»´í“¨í„° ë¹„ì „**ì˜ ì„¸ê³„ë¡œ ë“¤ì–´ê°‘ë‹ˆë‹¤!
- YOLOë¡œ ì‹¤ì‹œê°„ ê°ì²´ íƒì§€
- ì‚¬ëŒì˜ í–‰ë™ì„ ì¸ì‹í•˜ëŠ” AI
- ë‚˜ë§Œì˜ ìŠ¤ë§ˆíŠ¸ ë³´ì•ˆ ì‹œìŠ¤í…œ ë§Œë“¤ê¸°

ì˜¤ëŠ˜ í•˜ë£¨ ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤! ğŸ‰

---

# ğŸ¯ Day 2: ì»´í“¨í„° ë¹„ì „ - ì‹¤ì‹œê°„ìœ¼ë¡œ ì„¸ìƒì„ ì¸ì‹í•˜ê¸°

## 5ì¥: YOLOì˜ ëª¨ë“  ê²ƒ - ì—­ì‚¬ë¶€í„° ìµœì‹  ë²„ì „ê¹Œì§€

### 5.1 YOLOì˜ íƒ„ìƒ ìŠ¤í† ë¦¬

2015ë…„, ì›Œì‹±í„´ ëŒ€í•™ì˜ Joseph Redmonì´ë¼ëŠ” ë°•ì‚¬ê³¼ì • í•™ìƒì´ ìˆì—ˆìŠµë‹ˆë‹¤. ê·¸ëŠ” ê¸°ì¡´ì˜ ê°ì²´ íƒì§€ ë°©ë²•ì´ ë„ˆë¬´ ëŠë¦¬ë‹¤ê³  ìƒê°í–ˆìŠµë‹ˆë‹¤. "ì™œ ì´ë¯¸ì§€ë¥¼ ì—¬ëŸ¬ ë²ˆ ë´ì•¼ í•˜ì§€? ì‚¬ëŒì€ í•œ ë²ˆë§Œ ë´ë„ ëª¨ë“  ê±¸ ì•Œì•„ì±„ëŠ”ë°!"

ì´ëŸ° ì•„ì´ë””ì–´ì—ì„œ YOLO(You Only Look Once)ê°€ íƒ„ìƒí–ˆìŠµë‹ˆë‹¤.

### 5.2 YOLOì˜ ì§„í™” ê³¼ì •

```python
# YOLO ì§„í™” íƒ€ì„ë¼ì¸ ì‹œê°í™”
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.patches import FancyBboxPatch
import numpy as np

fig, ax = plt.subplots(figsize=(14, 8))

# YOLO ë²„ì „ ì •ë³´
yolo_versions = [
    {'version': 'YOLOv1', 'year': 2015, 'fps': 45, 'mAP': 63.4, 'creator': 'Joseph Redmon', 
     'feature': 'ì‹¤ì‹œê°„ ê°ì²´ íƒì§€ì˜ ì‹œì‘'},
    {'version': 'YOLOv2', 'year': 2016, 'fps': 67, 'mAP': 76.8, 'creator': 'Joseph Redmon',
     'feature': 'Anchor Box ë„ì…'},
    {'version': 'YOLOv3', 'year': 2018, 'fps': 65, 'mAP': 82.0, 'creator': 'Joseph Redmon',
     'feature': 'ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì˜ˆì¸¡'},
    {'version': 'YOLOv4', 'year': 2020, 'fps': 65, 'mAP': 85.4, 'creator': 'Alexey Bochkovskiy',
     'feature': 'Bag of Freebies'},
    {'version': 'YOLOv5', 'year': 2020, 'fps': 140, 'mAP': 84.0, 'creator': 'Ultralytics',
     'feature': 'PyTorch êµ¬í˜„'},
    {'version': 'YOLOv6', 'year': 2022, 'fps': 150, 'mAP': 85.5, 'creator': 'Meituan',
     'feature': 'ì‚°ì—… ìµœì í™”'},
    {'version': 'YOLOv7', 'year': 2022, 'fps': 155, 'mAP': 86.7, 'creator': 'WongKinYiu',
     'feature': 'E-ELAN êµ¬ì¡°'},
    {'version': 'YOLOv8', 'year': 2023, 'fps': 160, 'mAP': 87.0, 'creator': 'Ultralytics',
     'feature': 'Anchor-free'},
    {'version': 'YOLOv9', 'year': 2024, 'fps': 170, 'mAP': 88.0, 'creator': 'Community',
     'feature': 'PGI & GELAN'},
    {'version': 'YOLOv10', 'year': 2024, 'fps': 175, 'mAP': 88.5, 'creator': 'THU',
     'feature': 'NMS-free'},
    {'version': 'YOLO11', 'year': 2024, 'fps': 180, 'mAP': 89.0, 'creator': 'Ultralytics',
     'feature': 'C3k2 ë¸”ë¡'},
    {'version': 'YOLOv12', 'year': 2025, 'fps': 200, 'mAP': 90.5, 'creator': 'OpenAI',
     'feature': 'ViT í•˜ì´ë¸Œë¦¬ë“œ'}
]

# ë…„ë„ë³„ ìœ„ì¹˜ ê³„ì‚°
years = [v['year'] for v in yolo_versions]
positions = np.linspace(1, 10, len(yolo_versions))

# íƒ€ì„ë¼ì¸ ê·¸ë¦¬ê¸°
for i, (version, pos) in enumerate(zip(yolo_versions, positions)):
    # ë°•ìŠ¤ ê·¸ë¦¬ê¸°
    if i < 3:
        color = '#FF6B6B'  # Redmon ì‹œëŒ€
    elif i < 8:
        color = '#4ECDC4'  # ì»¤ë®¤ë‹ˆí‹° ì‹œëŒ€
    else:
        color = '#45B7D1'  # ìµœì‹  ì‹œëŒ€
    
    box = FancyBboxPatch((pos-0.4, 2), 0.8, 2,
                        boxstyle="round,pad=0.1",
                        facecolor=color,
                        edgecolor='black',
                        linewidth=2,
                        alpha=0.8)
    ax.add_patch(box)
    
    # ë²„ì „ ì •ë³´
    ax.text(pos, 3.5, version['version'], ha='center', fontweight='bold', fontsize=10)
    ax.text(pos, 3.2, f"{version['year']}", ha='center', fontsize=8)
    ax.text(pos, 2.8, f"FPS: {version['fps']}", ha='center', fontsize=8)
    ax.text(pos, 2.5, f"mAP: {version['mAP']}%", ha='center', fontsize=8)
    
    # íŠ¹ì§•
    ax.text(pos, 1.5, version['feature'], ha='center', fontsize=8, 
            wrap=True, style='italic')
    
    # ì œì‘ì
    ax.text(pos, 0.5, version['creator'], ha='center', fontsize=7, 
            color='gray')

# í™”ì‚´í‘œ ê·¸ë¦¬ê¸°
for i in range(len(positions)-1):
    ax.arrow(positions[i]+0.4, 3, positions[i+1]-positions[i]-0.8, 0,
            head_width=0.2, head_length=0.1, fc='gray', ec='gray', alpha=0.5)

# ìŠ¤íƒ€ì¼ë§
ax.set_xlim(0, 11)
ax.set_ylim(0, 5)
ax.set_title('YOLOì˜ ì§„í™”: 2015-2025', fontsize=16, fontweight='bold', pad=20)

# ë²”ë¡€
redmon_patch = mpatches.Patch(color='#FF6B6B', label='Joseph Redmon ì‹œëŒ€')
community_patch = mpatches.Patch(color='#4ECDC4', label='ì»¤ë®¤ë‹ˆí‹° ì£¼ë„')
modern_patch = mpatches.Patch(color='#45B7D1', label='ì°¨ì„¸ëŒ€ YOLO')
ax.legend(handles=[redmon_patch, community_patch, modern_patch], loc='upper right')

ax.axis('off')
plt.tight_layout()
plt.show()
```

### 5.3 YOLOê°€ íŠ¹ë³„í•œ ì´ìœ  - ê¸°ìˆ ì  í˜ì‹ 

YOLOê°€ í˜ëª…ì ì¸ ì´ìœ ë¥¼ ì‰½ê²Œ ì„¤ëª…í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤:

```python
# YOLO vs ì „í†µì  ë°©ë²• ë¹„êµ ì‹œê°í™”
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import numpy as np

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

# ì „í†µì  ë°©ë²• (R-CNN)
ax1.set_title('ì „í†µì  ë°©ë²• (R-CNN)', fontsize=14, fontweight='bold')
ax1.set_xlim(0, 10)
ax1.set_ylim(0, 10)

# ì›ë³¸ ì´ë¯¸ì§€
img_rect = patches.Rectangle((1, 1), 8, 8, linewidth=2, 
                           edgecolor='black', facecolor='lightgray')
ax1.add_patch(img_rect)
ax1.text(5, 9.5, 'ì›ë³¸ ì´ë¯¸ì§€', ha='center', fontweight='bold')

# ìˆ˜ë§ì€ ì˜ì—­ ì œì•ˆ
np.random.seed(42)
for i in range(20):
    x = np.random.uniform(1, 6)
    y = np.random.uniform(1, 6)
    w = np.random.uniform(1, 3)
    h = np.random.uniform(1, 3)
    
    rect = patches.Rectangle((x, y), w, h, linewidth=1,
                           edgecolor='red', facecolor='none',
                           alpha=0.5)
    ax1.add_patch(rect)

ax1.text(5, 0.5, '2000ê°œ ì´ìƒì˜ ì˜ì—­ì„\nê°ê° ë¶„ì„ (ëŠë¦¼!)', 
         ha='center', color='red', fontweight='bold')

# YOLO ë°©ë²•
ax2.set_title('YOLO ë°©ë²•', fontsize=14, fontweight='bold')
ax2.set_xlim(0, 10)
ax2.set_ylim(0, 10)

# ì›ë³¸ ì´ë¯¸ì§€
img_rect2 = patches.Rectangle((1, 1), 8, 8, linewidth=2,
                            edgecolor='black', facecolor='lightgray')
ax2.add_patch(img_rect2)
ax2.text(5, 9.5, 'ì›ë³¸ ì´ë¯¸ì§€', ha='center', fontweight='bold')

# ê·¸ë¦¬ë“œ
for i in range(1, 8):
    ax2.axvline(i+1, ymin=0.1, ymax=0.9, color='blue', alpha=0.3)
    ax2.axhline(i+1, xmin=0.1, xmax=0.9, color='blue', alpha=0.3)

# íƒì§€ëœ ê°ì²´
objects = [
    (2, 2, 2, 3, 'ê³ ì–‘ì´'),
    (5, 4, 2, 2, 'ê³µ'),
    (6, 7, 2, 1.5, 'ì‹ ë°œ')
]

for x, y, w, h, label in objects:
    rect = patches.Rectangle((x, y), w, h, linewidth=2,
                           edgecolor='green', facecolor='none')
    ax2.add_patch(rect)
    ax2.text(x+w/2, y+h/2, label, ha='center', va='center',
            fontweight='bold', color='green')

ax2.text(5, 0.5, 'í•œ ë²ˆì— ì „ì²´ ì´ë¯¸ì§€ ë¶„ì„\n(ë§¤ìš° ë¹ ë¦„!)', 
         ha='center', color='green', fontweight='bold')

ax1.axis('off')
ax2.axis('off')
plt.tight_layout()
plt.show()
```

### 5.4 2025ë…„ ìµœì‹  YOLO ëª¨ë¸ ìƒì„¸ ë¶„ì„

#### YOLO11 - í˜„ì¬ ê°€ì¥ ì•ˆì •ì ì¸ ì„ íƒ

```python
# YOLO11 ì•„í‚¤í…ì²˜ ì´í•´í•˜ê¸°
class YOLO11Architecture:
    """YOLO11ì˜ ì£¼ìš” êµ¬ì„± ìš”ì†Œ ì„¤ëª…"""
    
    def __init__(self):
        self.components = {
            'backbone': {
                'name': 'CSPDarknet with C3k2',
                'description': 'íŠ¹ì§• ì¶”ì¶œì„ ìœ„í•œ ë°±ë³¸ ë„¤íŠ¸ì›Œí¬',
                'innovation': 'C3k2 ë¸”ë¡ìœ¼ë¡œ ë” ê¹Šì€ ë„¤íŠ¸ì›Œí¬ êµ¬ì„± ê°€ëŠ¥'
            },
            'neck': {
                'name': 'SPPF + PAN',
                'description': 'ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ íŠ¹ì§• ìœµí•©',
                'innovation': 'ë” ë¹ ë¥¸ Spatial Pyramid Pooling'
            },
            'head': {
                'name': 'Decoupled Head',
                'description': 'ë¶„ë¥˜ì™€ ìœ„ì¹˜ ì˜ˆì¸¡ ë¶„ë¦¬',
                'innovation': 'ê° ì‘ì—…ì— ìµœì í™”ëœ ë³„ë„ ê²½ë¡œ'
            }
        }
    
    def visualize_architecture(self):
        """ì•„í‚¤í…ì²˜ ì‹œê°í™”"""
        fig, ax = plt.subplots(figsize=(12, 8))
        
        # ë°°ê²½
        ax.set_facecolor('#f0f0f0')
        
        # ì…ë ¥ ì´ë¯¸ì§€
        input_box = FancyBboxPatch((1, 6), 2, 1.5,
                                 boxstyle="round,pad=0.1",
                                 facecolor='lightblue',
                                 edgecolor='darkblue',
                                 linewidth=2)
        ax.add_patch(input_box)
        ax.text(2, 6.75, 'ì…ë ¥ ì´ë¯¸ì§€\n640Ã—640', ha='center', va='center',
               fontweight='bold')
        
        # Backbone
        backbone_box = FancyBboxPatch((4, 6), 3, 1.5,
                                    boxstyle="round,pad=0.1",
                                    facecolor='#FFE5B4',
                                    edgecolor='#FF8C00',
                                    linewidth=2)
        ax.add_patch(backbone_box)
        ax.text(5.5, 6.75, 'CSPDarknet\n+ C3k2 ë¸”ë¡', ha='center', va='center',
               fontweight='bold')
        
        # Neck
        neck_box = FancyBboxPatch((8, 6), 3, 1.5,
                                boxstyle="round,pad=0.1",
                                facecolor='#E6E6FA',
                                edgecolor='#9370DB',
                                linewidth=2)
        ax.add_patch(neck_box)
        ax.text(9.5, 6.75, 'SPPF + PAN\níŠ¹ì§• ìœµí•©', ha='center', va='center',
               fontweight='bold')
        
        # Head
        head_box = FancyBboxPatch((4, 3), 7, 2,
                                boxstyle="round,pad=0.1",
                                facecolor='#FFB6C1',
                                edgecolor='#DC143C',
                                linewidth=2)
        ax.add_patch(head_box)
        ax.text(7.5, 4, 'Decoupled Head', ha='center', va='center',
               fontweight='bold', fontsize=12)
        
        # ì¶œë ¥
        outputs = [
            (3, 0.5, 'í´ë˜ìŠ¤ ì˜ˆì¸¡'),
            (7.5, 0.5, 'ë°”ìš´ë”© ë°•ìŠ¤'),
            (12, 0.5, 'ì‹ ë¢°ë„ ì ìˆ˜')
        ]
        
        for x, y, label in outputs:
            output_box = FancyBboxPatch((x-1, y), 2, 1,
                                      boxstyle="round,pad=0.1",
                                      facecolor='lightgreen',
                                      edgecolor='darkgreen',
                                      linewidth=2)
            ax.add_patch(output_box)
            ax.text(x, y+0.5, label, ha='center', va='center')
        
        # í™”ì‚´í‘œ
        arrows = [
            ((3, 6.75), (4, 6.75)),
            ((7, 6.75), (8, 6.75)),
            ((9.5, 6), (7.5, 5)),
            ((7.5, 3), (4, 1.5)),
            ((7.5, 3), (7.5, 1.5)),
            ((7.5, 3), (11, 1.5))
        ]
        
        for start, end in arrows:
            ax.annotate('', xy=end, xytext=start,
                       arrowprops=dict(arrowstyle='->', lw=2, color='gray'))
        
        ax.set_xlim(0, 14)
        ax.set_ylim(0, 8)
        ax.set_title('YOLO11 ì•„í‚¤í…ì²˜', fontsize=16, fontweight='bold')
        ax.axis('off')
        
        plt.tight_layout()
        plt.show()

# ì•„í‚¤í…ì²˜ ì‹œê°í™”
arch = YOLO11Architecture()
arch.visualize_architecture()
```

#### YOLOv12 - ë¯¸ë˜ë¥¼ ë³´ì—¬ì£¼ëŠ” ì‹¤í—˜ì  ëª¨ë¸

YOLOv12ëŠ” 2025ë…„ ì´ˆì— ë°œí‘œëœ ìµœì‹  ëª¨ë¸ë¡œ, Vision Transformerì˜ ì¥ì ì„ í†µí•©í–ˆìŠµë‹ˆë‹¤:

1. **í•˜ì´ë¸Œë¦¬ë“œ ë°±ë³¸**: CNNê³¼ ViTì˜ ê²°í•©
2. **ì‹œê°„ì  ì¼ê´€ì„±**: ë¹„ë””ì˜¤ì—ì„œ ê°ì²´ë¥¼ ë” ì•ˆì •ì ìœ¼ë¡œ ì¶”ì 
3. **ìê¸° ì§€ë„ í•™ìŠµ**: ë¼ë²¨ ì—†ëŠ” ë°ì´í„°ë¡œë„ í•™ìŠµ ê°€ëŠ¥
4. **ì—£ì§€ ìµœì í™”**: ëª¨ë°”ì¼ ê¸°ê¸°ì—ì„œë„ ì‹¤í–‰ ê°€ëŠ¥

---

## 6ì¥: ê°ì²´ ì¶”ì ê³¼ í–‰ë™ ì¸ì‹ - ì›€ì§ì„ì„ ì´í•´í•˜ëŠ” AI

### 6.1 ê°ì²´ ì¶”ì ì´ë€?

ê°ì²´ íƒì§€ê°€ "ì§€ê¸ˆ ì´ ìˆœê°„ ë¬´ì—‡ì´ ì–´ë””ì— ìˆëŠ”ê°€?"ë¥¼ ì°¾ëŠ”ë‹¤ë©´, ê°ì²´ ì¶”ì ì€ "ì´ ë¬¼ì²´ê°€ ì‹œê°„ì— ë”°ë¼ ì–´ë–»ê²Œ ì›€ì§ì´ëŠ”ê°€?"ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.

### 6.2 ByteTrack - ê°„ë‹¨í•˜ì§€ë§Œ ê°•ë ¥í•œ ì¶”ì  ì•Œê³ ë¦¬ì¦˜

```python
# ByteTrack ì›ë¦¬ ì‹œê°í™”
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle, FancyBboxPatch
from matplotlib.patches import ConnectionPatch

def visualize_bytetrack():
    """ByteTrack ì•Œê³ ë¦¬ì¦˜ ì›ë¦¬ ì‹œê°í™”"""
    
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    # í”„ë ˆì„ t
    ax1 = axes[0]
    ax1.set_title('í”„ë ˆì„ t', fontsize=14, fontweight='bold')
    ax1.set_xlim(0, 10)
    ax1.set_ylim(0, 10)
    
    # ì´ì „ í”„ë ˆì„ì˜ ê°ì²´ë“¤
    prev_objects = [
        {'id': 1, 'pos': (2, 7), 'color': 'red', 'label': 'ì‚¬ëŒ 1'},
        {'id': 2, 'pos': (5, 4), 'color': 'blue', 'label': 'ì‚¬ëŒ 2'},
        {'id': 3, 'pos': (8, 6), 'color': 'green', 'label': 'ìë™ì°¨'}
    ]
    
    for obj in prev_objects:
        rect = Rectangle(obj['pos'], 1.5, 1.5, 
                        facecolor=obj['color'], alpha=0.5)
        ax1.add_patch(rect)
        ax1.text(obj['pos'][0]+0.75, obj['pos'][1]+0.75, 
                f"ID:{obj['id']}", ha='center', va='center',
                fontweight='bold')
    
    # í”„ë ˆì„ t+1
    ax2 = axes[1]
    ax2.set_title('í”„ë ˆì„ t+1', fontsize=14, fontweight='bold')
    ax2.set_xlim(0, 10)
    ax2.set_ylim(0, 10)
    
    # ìƒˆë¡œìš´ íƒì§€ ê²°ê³¼
    new_detections = [
        {'pos': (2.5, 6.5), 'score': 0.9},
        {'pos': (5.2, 3.8), 'score': 0.85},
        {'pos': (8.1, 5.8), 'score': 0.7},
        {'pos': (1, 2), 'score': 0.4}  # ë‚®ì€ ì‹ ë¢°ë„
    ]
    
    for i, det in enumerate(new_detections):
        color = 'gray' if det['score'] < 0.5 else 'black'
        alpha = det['score']
        rect = Rectangle(det['pos'], 1.5, 1.5,
                        facecolor=color, alpha=alpha,
                        edgecolor='black', linewidth=2)
        ax2.add_patch(rect)
        ax2.text(det['pos'][0]+0.75, det['pos'][1]+2,
                f"ì ìˆ˜:{det['score']}", ha='center',
                fontsize=8)
    
    # ë§¤ì¹­ ê²°ê³¼
    ax3 = axes[2]
    ax3.set_title('ByteTrack ë§¤ì¹­ ê²°ê³¼', fontsize=14, fontweight='bold')
    ax3.set_xlim(0, 10)
    ax3.set_ylim(0, 10)
    
    # ë§¤ì¹­ëœ ê°ì²´ë“¤
    matched = [
        {'id': 1, 'old_pos': (2, 7), 'new_pos': (2.5, 6.5), 'color': 'red'},
        {'id': 2, 'old_pos': (5, 4), 'new_pos': (5.2, 3.8), 'color': 'blue'},
        {'id': 3, 'old_pos': (8, 6), 'new_pos': (8.1, 5.8), 'color': 'green'}
    ]
    
    for match in matched:
        # ìƒˆ ìœ„ì¹˜
        rect = Rectangle(match['new_pos'], 1.5, 1.5,
                        facecolor=match['color'], alpha=0.5)
        ax3.add_patch(rect)
        ax3.text(match['new_pos'][0]+0.75, match['new_pos'][1]+0.75,
                f"ID:{match['id']}", ha='center', va='center',
                fontweight='bold')
        
        # ì´ë™ ê²½ë¡œ
        ax3.arrow(match['old_pos'][0]+0.75, match['old_pos'][1]+0.75,
                 match['new_pos'][0]-match['old_pos'][0],
                 match['new_pos'][1]-match['old_pos'][1],
                 head_width=0.2, head_length=0.1,
                 fc='gray', ec='gray', alpha=0.5)
    
    # ìƒˆë¡œìš´ ê°ì²´ (ë‚®ì€ ì‹ ë¢°ë„ì§€ë§Œ ì¶”ì  ì‹œì‘)
    new_rect = Rectangle((1, 2), 1.5, 1.5,
                        facecolor='orange', alpha=0.5,
                        edgecolor='orange', linewidth=2)
    ax3.add_patch(new_rect)
    ax3.text(1.75, 2.75, "NEW\nID:4", ha='center', va='center',
            fontweight='bold', fontsize=8)
    
    for ax in axes:
        ax.set_aspect('equal')
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

visualize_bytetrack()
```

### 6.3 í–‰ë™ ì¸ì‹ - AIê°€ ë™ì‘ì„ ì´í•´í•˜ë‹¤

í–‰ë™ ì¸ì‹ì€ ë‹¨ìˆœíˆ "ì‚¬ëŒì´ ìˆë‹¤"ë¥¼ ë„˜ì–´ "ì‚¬ëŒì´ ë¬´ì—‡ì„ í•˜ê³  ìˆëŠ”ê°€"ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.

```python
# í–‰ë™ ì¸ì‹ íŒŒì´í”„ë¼ì¸
class ActionRecognitionPipeline:
    """í–‰ë™ ì¸ì‹ì˜ ì „ì²´ ê³¼ì •ì„ ë³´ì—¬ì£¼ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.actions = {
            'walking': 'ê±·ê¸°',
            'running': 'ë›°ê¸°',
            'sitting': 'ì•‰ê¸°',
            'standing': 'ì„œê¸°',
            'waving': 'ì† í”ë“¤ê¸°',
            'jumping': 'ì í”„í•˜ê¸°',
            'dancing': 'ì¶¤ì¶”ê¸°',
            'falling': 'ë„˜ì–´ì§€ê¸°'
        }
    
    def visualize_pipeline(self):
        """í–‰ë™ ì¸ì‹ íŒŒì´í”„ë¼ì¸ ì‹œê°í™”"""
        
        fig = plt.figure(figsize=(14, 10))
        
        # 1. ë¹„ë””ì˜¤ í”„ë ˆì„
        ax1 = plt.subplot(3, 3, 1)
        ax1.set_title('1. ë¹„ë””ì˜¤ ì…ë ¥', fontweight='bold')
        
        # í”„ë ˆì„ ì‹œí€€ìŠ¤ í‘œì‹œ
        frames = np.random.rand(8, 64, 64, 3)
        frame_strip = np.hstack([frames[i] for i in range(4)])
        ax1.imshow(frame_strip)
        ax1.axis('off')
        ax1.text(0.5, -0.1, 'ì—°ì†ëœ í”„ë ˆì„ë“¤', transform=ax1.transAxes,
                ha='center')
        
        # 2. ì‚¬ëŒ íƒì§€
        ax2 = plt.subplot(3, 3, 2)
        ax2.set_title('2. ì‚¬ëŒ íƒì§€', fontweight='bold')
        
        # ë°”ìš´ë”© ë°•ìŠ¤ê°€ ìˆëŠ” í”„ë ˆì„
        frame_with_box = np.ones((64, 64, 3))
        ax2.imshow(frame_with_box)
        rect = Rectangle((15, 10), 35, 45, fill=False, 
                        edgecolor='red', linewidth=3)
        ax2.add_patch(rect)
        ax2.text(32, 32, 'ì‚¬ëŒ', ha='center', va='center',
                fontweight='bold', color='red')
        ax2.axis('off')
        
        # 3. í¬ì¦ˆ ì¶”ì •
        ax3 = plt.subplot(3, 3, 3)
        ax3.set_title('3. í¬ì¦ˆ ì¶”ì •', fontweight='bold')
        
        # ìŠ¤ì¼ˆë ˆí†¤ ê·¸ë¦¬ê¸°
        ax3.set_xlim(0, 64)
        ax3.set_ylim(64, 0)
        
        # ê´€ì ˆ ìœ„ì¹˜
        joints = {
            'head': (32, 15),
            'neck': (32, 20),
            'right_shoulder': (25, 22),
            'left_shoulder': (39, 22),
            'right_elbow': (20, 30),
            'left_elbow': (44, 30),
            'right_hand': (18, 38),
            'left_hand': (46, 38),
            'hip': (32, 35),
            'right_knee': (28, 45),
            'left_knee': (36, 45),
            'right_foot': (26, 55),
            'left_foot': (38, 55)
        }
        
        # ê´€ì ˆ ì—°ê²°
        connections = [
            ('head', 'neck'),
            ('neck', 'right_shoulder'),
            ('neck', 'left_shoulder'),
            ('right_shoulder', 'right_elbow'),
            ('right_elbow', 'right_hand'),
            ('left_shoulder', 'left_elbow'),
            ('left_elbow', 'left_hand'),
            ('neck', 'hip'),
            ('hip', 'right_knee'),
            ('hip', 'left_knee'),
            ('right_knee', 'right_foot'),
            ('left_knee', 'left_foot')
        ]
        
        # ìŠ¤ì¼ˆë ˆí†¤ ê·¸ë¦¬ê¸°
        for joint, pos in joints.items():
            ax3.plot(pos[0], pos[1], 'o', color='blue', markersize=8)
        
        for conn in connections:
            start = joints[conn[0]]
            end = joints[conn[1]]
            ax3.plot([start[0], end[0]], [start[1], end[1]], 
                    'b-', linewidth=2)
        
        ax3.axis('off')
        
        # 4. ì‹œê°„ì  íŠ¹ì§• ì¶”ì¶œ
        ax4 = plt.subplot(3, 3, 4)
        ax4.set_title('4. ì‹œê°„ì  íŠ¹ì§•', fontweight='bold')
        
        # ì‹œê°„ì— ë”°ë¥¸ ê´€ì ˆ ì›€ì§ì„
        t = np.linspace(0, 2*np.pi, 50)
        y1 = np.sin(t) * 10 + 30
        y2 = np.sin(t + np.pi/4) * 8 + 30
        
        ax4.plot(t, y1, label='ì˜¤ë¥¸íŒ” ì›€ì§ì„')
        ax4.plot(t, y2, label='ì™¼íŒ” ì›€ì§ì„')
        ax4.set_xlabel('ì‹œê°„')
        ax4.set_ylabel('ìœ„ì¹˜')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        # 5. íŠ¹ì§• ë²¡í„°
        ax5 = plt.subplot(3, 3, 5)
        ax5.set_title('5. íŠ¹ì§• ë²¡í„°', fontweight='bold')
        
        # íŠ¹ì§• ë²¡í„° ì‹œê°í™”
        features = np.random.rand(10, 10)
        im = ax5.imshow(features, cmap='viridis')
        ax5.set_xlabel('íŠ¹ì§• ì°¨ì›')
        ax5.set_ylabel('ì‹œê°„ ìŠ¤í…')
        plt.colorbar(im, ax=ax5)
        
        # 6. í–‰ë™ ë¶„ë¥˜
        ax6 = plt.subplot(3, 3, 6)
        ax6.set_title('6. í–‰ë™ ë¶„ë¥˜', fontweight='bold')
        
        # ë¶„ë¥˜ ê²°ê³¼
        actions = ['ê±·ê¸°', 'ë›°ê¸°', 'ì•‰ê¸°', 'ì„œê¸°', 'ì† í”ë“¤ê¸°']
        probabilities = [0.05, 0.02, 0.03, 0.85, 0.05]
        
        bars = ax6.bar(actions, probabilities)
        bars[3].set_color('green')  # ê°€ì¥ ë†’ì€ í™•ë¥ 
        ax6.set_ylabel('í™•ë¥ ')
        ax6.set_ylim(0, 1)
        
        # ìµœì¢… ê²°ê³¼
        ax7 = plt.subplot(3, 1, 3)
        ax7.set_title('ìµœì¢… ê²°ê³¼: ì„œìˆê¸° (85% í™•ì‹ ë„)', 
                     fontsize=16, fontweight='bold')
        ax7.text(0.5, 0.5, 'ğŸ§ ì‚¬ëŒì´ ì„œ ìˆìŠµë‹ˆë‹¤', 
                ha='center', va='center', fontsize=20)
        ax7.axis('off')
        
        plt.tight_layout()
        plt.show()

# íŒŒì´í”„ë¼ì¸ ì‹œê°í™” ì‹¤í–‰
pipeline = ActionRecognitionPipeline()
pipeline.visualize_pipeline()
```

### 6.4 ìµœì‹  í–‰ë™ ì¸ì‹ ëª¨ë¸ë“¤

#### VideoMAE V2 - ë§ˆìŠ¤í¬ í•™ìŠµì˜ ë§ˆë²•

```python
class VideoMAEExplainer:
    """VideoMAEì˜ ì‘ë™ ì›ë¦¬ë¥¼ ì„¤ëª…í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def explain_masking(self):
        """ë§ˆìŠ¤í‚¹ í•™ìŠµ ì›ë¦¬ ì„¤ëª…"""
        
        fig, axes = plt.subplots(2, 3, figsize=(12, 8))
        
        # ì›ë³¸ ë¹„ë””ì˜¤ í”„ë ˆì„
        ax1 = axes[0, 0]
        ax1.set_title('ì›ë³¸ ë¹„ë””ì˜¤', fontweight='bold')
        original = np.ones((64, 64, 3))
        ax1.imshow(original)
        ax1.axis('off')
        
        # ë§ˆìŠ¤í‚¹ëœ í”„ë ˆì„
        ax2 = axes[0, 1]
        ax2.set_title('90% ë§ˆìŠ¤í‚¹', fontweight='bold')
        masked = original.copy()
        mask = np.random.random((64, 64)) > 0.9
        masked[~mask] = 0
        ax2.imshow(masked)
        ax2.axis('off')
        
        # ëª¨ë¸ ì˜ˆì¸¡
        ax3 = axes[0, 2]
        ax3.set_title('ëª¨ë¸ì˜ ì˜ˆì¸¡', fontweight='bold')
        predicted = np.random.rand(64, 64, 3)
        ax3.imshow(predicted)
        ax3.axis('off')
        
        # í•™ìŠµ ê³¼ì •
        ax4 = axes[1, 0]
        ax4.set_title('í•™ìŠµ ê³¼ì •', fontweight='bold')
        epochs = range(1, 101)
        loss = 10 * np.exp(-np.array(epochs) / 20) + np.random.normal(0, 0.1, 100)
        ax4.plot(epochs, loss)
        ax4.set_xlabel('ì—í­')
        ax4.set_ylabel('ì†ì‹¤')
        ax4.grid(True, alpha=0.3)
        
        # íŠ¹ì§• í•™ìŠµ
        ax5 = axes[1, 1]
        ax5.set_title('í•™ìŠµëœ íŠ¹ì§•', fontweight='bold')
        features = [
            'ì›€ì§ì„ íŒ¨í„´',
            'ê°ì²´ í˜•íƒœ',
            'ì‹œê°„ì  ì¼ê´€ì„±',
            'ê³µê°„ì  ê´€ê³„',
            'ìƒ‰ìƒ ë¶„í¬'
        ]
        importance = [0.9, 0.8, 0.95, 0.7, 0.6]
        
        bars = ax5.barh(features, importance)
        ax5.set_xlim(0, 1)
        ax5.set_xlabel('ì¤‘ìš”ë„')
        
        # ì‘ìš© ë¶„ì•¼
        ax6 = axes[1, 2]
        ax6.set_title('ì‘ìš© ë¶„ì•¼', fontweight='bold')
        applications = """
        â€¢ ìŠ¤í¬ì¸  ë¶„ì„
        â€¢ ë³´ì•ˆ ê°ì‹œ
        â€¢ ì˜ë£Œ ì§„ë‹¨
        â€¢ ë¡œë´‡ ì œì–´
        â€¢ ê²Œì„ ì¸í„°í˜ì´ìŠ¤
        """
        ax6.text(0.1, 0.5, applications, fontsize=12, va='center')
        ax6.axis('off')
        
        plt.suptitle('VideoMAE: ìê¸° ì§€ë„ í•™ìŠµìœ¼ë¡œ ë¹„ë””ì˜¤ ì´í•´í•˜ê¸°', 
                    fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.show()

explainer = VideoMAEExplainer()
explainer.explain_masking()
```

---

## 7ì¥: ì‹¤ì „ í”„ë¡œì íŠ¸ - ë˜‘ë˜‘í•œ ê°ì‹œ ì‹œìŠ¤í…œ ë§Œë“¤ê¸°

### 7.1 í”„ë¡œì íŠ¸ ê°œìš”: AI ë³´ì•ˆ ì‹œìŠ¤í…œ

ìš°ë¦¬ê°€ ë§Œë“¤ ì‹œìŠ¤í…œì˜ íŠ¹ì§•:
- ğŸ‘€ ì‹¤ì‹œê°„ ê°ì²´ íƒì§€ ë° ì¶”ì 
- ğŸƒ ì´ìƒ í–‰ë™ ê°ì§€
- ğŸ“± ëª¨ë°”ì¼ ì•Œë¦¼
- ğŸ“Š í†µê³„ ëŒ€ì‹œë³´ë“œ
- ğŸ¥ ì˜ìƒ ì €ì¥ ë° ê²€ìƒ‰

### 7.2 ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì„¤ê³„

```python
# smart_security_system.py - ìŠ¤ë§ˆíŠ¸ ë³´ì•ˆ ì‹œìŠ¤í…œ

import cv2
import torch
import numpy as np
from ultralytics import YOLO
import supervision as sv
from collections import defaultdict, deque
import time
from datetime import datetime
import json
import os
import threading
import queue
from typing import Dict, List, Tuple, Optional
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.image import MIMEImage

class SmartSecuritySystem:
    """AI ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ë³´ì•ˆ ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: Dict = None):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        print("ğŸ”’ ìŠ¤ë§ˆíŠ¸ ë³´ì•ˆ ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        
        # ì„¤ì •
        self.config = config or self.get_default_config()
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.init_models()
        
        # ì¶”ì ê¸° ì´ˆê¸°í™”
        self.tracker = sv.ByteTrack()
        
        # ë°ì´í„° ì €ì¥ì†Œ
        self.init_storage()
        
        # ì•Œë¦¼ ì‹œìŠ¤í…œ
        self.init_notification_system()
        
        print("âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def get_default_config(self) -> Dict:
        """ê¸°ë³¸ ì„¤ì •ê°’"""
        return {
            'yolo_model': 'yolo11m.pt',
            'confidence_threshold': 0.5,
            'max_tracked_objects': 50,
            'alert_cooldown': 30,  # ì´ˆ
            'recording_path': 'security_recordings',
            'suspicious_behaviors': {
                'loitering': {'duration': 60, 'area': 100},  # 60ì´ˆ ì´ìƒ ë¨¸ë¬´ë¥´ê¸°
                'running': {'speed_threshold': 5.0},  # ë¹ ë¥¸ ì›€ì§ì„
                'intrusion': {'restricted_zones': []},  # ì œí•œ êµ¬ì—­ ì¹¨ì…
                'crowding': {'max_people': 10},  # ê³¼ë°€
                'abandoned_object': {'duration': 300}  # 5ë¶„ ì´ìƒ ë°©ì¹˜ëœ ë¬¼ì²´
            },
            'notification': {
                'email': None,
                'webhook': None
            }
        }
    
    def init_models(self):
        """AI ëª¨ë¸ ì´ˆê¸°í™”"""
        print("  ğŸ§  AI ëª¨ë¸ì„ ë¡œë”©í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        
        # YOLO ëª¨ë¸
        self.yolo = YOLO(self.config['yolo_model'])
        
        # í–‰ë™ ë¶„ë¥˜ê¸° (ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜)
        self.behavior_analyzer = BehaviorAnalyzer(self.config['suspicious_behaviors'])
        
    def init_storage(self):
        """ë°ì´í„° ì €ì¥ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        self.storage_path = self.config['recording_path']
        os.makedirs(self.storage_path, exist_ok=True)
        
        # ì¶”ì  ë°ì´í„°
        self.track_history = defaultdict(lambda: {
            'positions': deque(maxlen=300),  # 10ì´ˆ @ 30fps
            'timestamps': deque(maxlen=300),
            'class': None,
            'first_seen': None,
            'last_seen': None,
            'alerts': []
        })
        
        # ì´ë²¤íŠ¸ ë¡œê·¸
        self.event_log = []
        
    def init_notification_system(self):
        """ì•Œë¦¼ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        self.alert_queue = queue.Queue()
        self.last_alert_time = defaultdict(float)
        
        # ì•Œë¦¼ ì²˜ë¦¬ ìŠ¤ë ˆë“œ
        self.notification_thread = threading.Thread(
            target=self.process_notifications,
            daemon=True
        )
        self.notification_thread.start()
    
    def process_frame(self, frame: np.ndarray, frame_id: int) -> Tuple[np.ndarray, List[Dict]]:
        """ë‹¨ì¼ í”„ë ˆì„ ì²˜ë¦¬"""
        
        # 1. ê°ì²´ íƒì§€
        results = self.yolo(frame, conf=self.config['confidence_threshold'], verbose=False)[0]
        detections = sv.Detections.from_ultralytics(results)
        
        # 2. ê°ì²´ ì¶”ì 
        tracks = self.tracker.update_with_detections(detections)
        
        # 3. ì¶”ì  ë°ì´í„° ì—…ë°ì´íŠ¸
        current_time = time.time()
        events = []
        
        for i in range(len(tracks)):
            track_id = tracks.tracker_id[i]
            class_id = tracks.class_id[i]
            class_name = self.yolo.names[class_id]
            bbox = tracks.xyxy[i]
            
            # ì¤‘ì‹¬ì  ê³„ì‚°
            center = ((bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2)
            
            # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            history = self.track_history[track_id]
            history['positions'].append(center)
            history['timestamps'].append(current_time)
            history['class'] = class_name
            
            if history['first_seen'] is None:
                history['first_seen'] = current_time
                events.append({
                    'type': 'new_object',
                    'track_id': track_id,
                    'class': class_name,
                    'time': current_time
                })
            
            history['last_seen'] = current_time
        
        # 4. í–‰ë™ ë¶„ì„
        suspicious_activities = self.behavior_analyzer.analyze(
            self.track_history, current_time
        )
        
        # 5. ì•Œë¦¼ ìƒì„±
        for activity in suspicious_activities:
            if self.should_send_alert(activity):
                self.alert_queue.put({
                    'activity': activity,
                    'frame': frame.copy(),
                    'time': current_time
                })
                events.append(activity)
        
        # 6. ì‹œê°í™”
        annotated_frame = self.visualize_results(
            frame, tracks, suspicious_activities
        )
        
        return annotated_frame, events
    
    def should_send_alert(self, activity: Dict) -> bool:
        """ì•Œë¦¼ì„ ë³´ë‚¼ì§€ ê²°ì •"""
        alert_type = activity['type']
        current_time = time.time()
        
        # ì¿¨ë‹¤ìš´ í™•ì¸
        if current_time - self.last_alert_time[alert_type] < self.config['alert_cooldown']:
            return False
        
        self.last_alert_time[alert_type] = current_time
        return True
    
    def visualize_results(self, frame: np.ndarray, tracks: sv.Detections, 
                         activities: List[Dict]) -> np.ndarray:
        """ê²°ê³¼ ì‹œê°í™”"""
        annotated = frame.copy()
        
        # ë°•ìŠ¤ ê·¸ë¦¬ê¸°
        box_annotator = sv.BoxAnnotator()
        annotated = box_annotator.annotate(scene=annotated, detections=tracks)
        
        # ì¶”ì  IDì™€ í´ë˜ìŠ¤ í‘œì‹œ
        for i in range(len(tracks)):
            track_id = tracks.tracker_id[i]
            class_id = tracks.class_id[i]
            class_name = self.yolo.names[class_id]
            bbox = tracks.xyxy[i]
            
            # ë¼ë²¨
            label = f"ID:{track_id} {class_name}"
            
            # ì˜ì‹¬ í–‰ë™ì´ ìˆëŠ” ê²½ìš° ê°•ì¡°
            is_suspicious = any(
                activity.get('track_id') == track_id 
                for activity in activities
            )
            
            color = (0, 0, 255) if is_suspicious else (0, 255, 0)
            
            cv2.putText(
                annotated,
                label,
                (int(bbox[0]), int(bbox[1] - 10)),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.5,
                color,
                2
            )
        
        # ìƒíƒœ í‘œì‹œ
        self.draw_status(annotated, len(tracks), activities)
        
        return annotated
    
    def draw_status(self, frame: np.ndarray, num_objects: int, activities: List[Dict]):
        """ìƒíƒœ ì •ë³´ í‘œì‹œ"""
        h, w = frame.shape[:2]
        
        # ìƒë‹¨ ì •ë³´ ë°”
        overlay = frame.copy()
        cv2.rectangle(overlay, (0, 0), (w, 50), (0, 0, 0), -1)
        frame[:50] = cv2.addWeighted(overlay[:50], 0.7, frame[:50], 0.3, 0)
        
        # ì‹œê°„
        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        cv2.putText(frame, current_time, (10, 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        # ê°ì²´ ìˆ˜
        cv2.putText(frame, f"Objects: {num_objects}", (300, 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        # ê²½ê³ 
        if activities:
            alert_text = f"ALERT: {activities[0]['type']}"
            cv2.putText(frame, alert_text, (500, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
    
    def process_notifications(self):
        """ì•Œë¦¼ ì²˜ë¦¬ ìŠ¤ë ˆë“œ"""
        while True:
            try:
                alert = self.alert_queue.get(timeout=1)
                self.send_notification(alert)
            except queue.Empty:
                continue
    
    def send_notification(self, alert: Dict):
        """ì•Œë¦¼ ì „ì†¡"""
        print(f"\nğŸš¨ ê²½ê³ : {alert['activity']['type']}")
        print(f"   ì‹œê°„: {datetime.fromtimestamp(alert['time'])}")
        print(f"   ì„¸ë¶€ì‚¬í•­: {alert['activity'].get('details', 'N/A')}")
        
        # ì´ë©”ì¼ ì•Œë¦¼ (ì„¤ì •ëœ ê²½ìš°)
        if self.config['notification']['email']:
            self.send_email_alert(alert)
        
        # ì›¹í›… ì•Œë¦¼ (ì„¤ì •ëœ ê²½ìš°)
        if self.config['notification']['webhook']:
            self.send_webhook_alert(alert)
        
        # ìŠ¤í¬ë¦°ìƒ· ì €ì¥
        self.save_alert_screenshot(alert)
    
    def save_alert_screenshot(self, alert: Dict):
        """ê²½ê³  ìŠ¤í¬ë¦°ìƒ· ì €ì¥"""
        timestamp = datetime.fromtimestamp(alert['time']).strftime('%Y%m%d_%H%M%S')
        filename = f"{alert['activity']['type']}_{timestamp}.jpg"
        filepath = os.path.join(self.storage_path, filename)
        
        cv2.imwrite(filepath, alert['frame'])
        print(f"   ğŸ“¸ ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {filepath}")
    
    def run_live_monitoring(self, source=0):
        """ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹¤í–‰"""
        print("\nğŸ¥ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        print("'q' í‚¤ë¥¼ ëˆŒëŸ¬ ì¢…ë£Œ")
        print("'s' í‚¤ë¥¼ ëˆŒëŸ¬ ìŠ¤í¬ë¦°ìƒ· ì €ì¥")
        print("'r' í‚¤ë¥¼ ëˆŒëŸ¬ ë…¹í™” ì‹œì‘/ì¤‘ì§€")
        
        cap = cv2.VideoCapture(source)
        
        # ë¹„ë””ì˜¤ ì†ì„±
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        # ë…¹í™” ì„¤ì •
        recording = False
        video_writer = None
        
        frame_id = 0
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # í”„ë ˆì„ ì²˜ë¦¬
            annotated_frame, events = self.process_frame(frame, frame_id)
            
            # ì´ë²¤íŠ¸ ë¡œê¹…
            for event in events:
                self.log_event(event)
            
            # ë…¹í™”
            if recording and video_writer:
                video_writer.write(annotated_frame)
            
            # í™”ë©´ í‘œì‹œ
            cv2.imshow('Smart Security System', annotated_frame)
            
            # í‚¤ ì…ë ¥ ì²˜ë¦¬
            key = cv2.waitKey(1) & 0xFF
            
            if key == ord('q'):
                break
            elif key == ord('s'):
                self.save_screenshot(annotated_frame)
            elif key == ord('r'):
                if not recording:
                    # ë…¹í™” ì‹œì‘
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    video_path = os.path.join(self.storage_path, f"recording_{timestamp}.mp4")
                    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
                    video_writer = cv2.VideoWriter(video_path, fourcc, fps, (width, height))
                    recording = True
                    print(f"ğŸ”´ ë…¹í™” ì‹œì‘: {video_path}")
                else:
                    # ë…¹í™” ì¤‘ì§€
                    video_writer.release()
                    video_writer = None
                    recording = False
                    print("â¹ï¸ ë…¹í™” ì¤‘ì§€")
            
            frame_id += 1
        
        # ì •ë¦¬
        cap.release()
        if video_writer:
            video_writer.release()
        cv2.destroyAllWindows()
        
        # ìµœì¢… ë¦¬í¬íŠ¸
        self.generate_report()
    
    def log_event(self, event: Dict):
        """ì´ë²¤íŠ¸ ë¡œê¹…"""
        event['timestamp'] = datetime.now().isoformat()
        self.event_log.append(event)
        
        # ì£¼ê¸°ì ìœ¼ë¡œ íŒŒì¼ì— ì €ì¥
        if len(self.event_log) % 100 == 0:
            self.save_event_log()
    
    def save_event_log(self):
        """ì´ë²¤íŠ¸ ë¡œê·¸ ì €ì¥"""
        log_path = os.path.join(self.storage_path, 'event_log.json')
        with open(log_path, 'w', encoding='utf-8') as f:
            json.dump(self.event_log, f, ensure_ascii=False, indent=2)
    
    def generate_report(self):
        """ëª¨ë‹ˆí„°ë§ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("\nğŸ“Š ëª¨ë‹ˆí„°ë§ ë¦¬í¬íŠ¸")
        print("=" * 50)
        
        # ì „ì²´ í†µê³„
        total_objects = len(self.track_history)
        print(f"ì´ ì¶”ì  ê°ì²´ ìˆ˜: {total_objects}")
        
        # í´ë˜ìŠ¤ë³„ í†µê³„
        class_counts = defaultdict(int)
        for track_data in self.track_history.values():
            if track_data['class']:
                class_counts[track_data['class']] += 1
        
        print("\ní´ë˜ìŠ¤ë³„ ê°ì²´ ìˆ˜:")
        for class_name, count in sorted(class_counts.items()):
            print(f"  - {class_name}: {count}")
        
        # ê²½ê³  í†µê³„
        alert_counts = defaultdict(int)
        for event in self.event_log:
            if event.get('type') in ['loitering', 'running', 'intrusion']:
                alert_counts[event['type']] += 1
        
        print("\nê²½ê³  ìœ í˜•ë³„ íšŸìˆ˜:")
        for alert_type, count in sorted(alert_counts.items()):
            print(f"  - {alert_type}: {count}")
        
        print("=" * 50)


class BehaviorAnalyzer:
    """í–‰ë™ ë¶„ì„ê¸°"""
    
    def __init__(self, config: Dict):
        self.config = config
    
    def analyze(self, track_history: Dict, current_time: float) -> List[Dict]:
        """ì˜ì‹¬ìŠ¤ëŸ¬ìš´ í–‰ë™ ë¶„ì„"""
        suspicious_activities = []
        
        for track_id, history in track_history.items():
            if len(history['positions']) < 10:
                continue
            
            # 1. ë°°íšŒ ê°ì§€ (Loitering)
            loitering = self.detect_loitering(history, current_time)
            if loitering:
                suspicious_activities.append({
                    'type': 'loitering',
                    'track_id': track_id,
                    'details': loitering,
                    'time': current_time
                })
            
            # 2. ë¹ ë¥¸ ì›€ì§ì„ ê°ì§€ (Running)
            running = self.detect_running(history)
            if running:
                suspicious_activities.append({
                    'type': 'running',
                    'track_id': track_id,
                    'details': running,
                    'time': current_time
                })
            
            # 3. ë°©ì¹˜ëœ ë¬¼ì²´ ê°ì§€
            if history['class'] in ['backpack', 'suitcase', 'handbag']:
                abandoned = self.detect_abandoned_object(history, current_time)
                if abandoned:
                    suspicious_activities.append({
                        'type': 'abandoned_object',
                        'track_id': track_id,
                        'details': abandoned,
                        'time': current_time
                    })
        
        return suspicious_activities
    
    def detect_loitering(self, history: Dict, current_time: float) -> Optional[Dict]:
        """ë°°íšŒ ê°ì§€"""
        duration = current_time - history['first_seen']
        
        if duration < self.config['loitering']['duration']:
            return None
        
        # ì´ë™ ë²”ìœ„ ê³„ì‚°
        positions = list(history['positions'])
        if len(positions) < 2:
            return None
        
        xs = [p[0] for p in positions]
        ys = [p[1] for p in positions]
        
        x_range = max(xs) - min(xs)
        y_range = max(ys) - min(ys)
        area = x_range * y_range
        
        if area < self.config['loitering']['area']:
            return {
                'duration': duration,
                'area': area,
                'message': f"{duration:.1f}ì´ˆ ë™ì•ˆ ì‘ì€ ì˜ì—­ì— ë¨¸ë¬¼ëŸ¬ ìˆìŒ"
            }
        
        return None
    
    def detect_running(self, history: Dict) -> Optional[Dict]:
        """ë¹ ë¥¸ ì›€ì§ì„ ê°ì§€"""
        positions = list(history['positions'])
        timestamps = list(history['timestamps'])
        
        if len(positions) < 5:
            return None
        
        # ìµœê·¼ 5í”„ë ˆì„ì˜ ì†ë„ ê³„ì‚°
        speeds = []
        for i in range(len(positions) - 5, len(positions) - 1):
            dx = positions[i+1][0] - positions[i][0]
            dy = positions[i+1][1] - positions[i][1]
            dt = timestamps[i+1] - timestamps[i]
            
            if dt > 0:
                speed = np.sqrt(dx**2 + dy**2) / dt
                speeds.append(speed)
        
        avg_speed = np.mean(speeds) if speeds else 0
        
        if avg_speed > self.config['running']['speed_threshold']:
            return {
                'speed': avg_speed,
                'message': f"ë¹ ë¥¸ ì†ë„ë¡œ ì´ë™ ì¤‘ ({avg_speed:.1f} pixels/s)"
            }
        
        return None
    
    def detect_abandoned_object(self, history: Dict, current_time: float) -> Optional[Dict]:
        """ë°©ì¹˜ëœ ë¬¼ì²´ ê°ì§€"""
        # ë§ˆì§€ë§‰ìœ¼ë¡œ ë³¸ ì‹œê°„ í™•ì¸
        time_since_last_movement = current_time - history['last_seen']
        
        if time_since_last_movement > 1.0:  # 1ì´ˆ ì´ìƒ ì—…ë°ì´íŠ¸ ì—†ìŒ
            # ì •ì§€ ìƒíƒœ í™•ì¸
            positions = list(history['positions'])[-10:]
            if len(positions) < 10:
                return None
            
            # ìœ„ì¹˜ ë³€í™” ê³„ì‚°
            position_std = np.std([p[0] for p in positions]) + np.std([p[1] for p in positions])
            
            if position_std < 5.0:  # ê±°ì˜ ì›€ì§ì„ ì—†ìŒ
                stationary_duration = current_time - history['first_seen']
                
                if stationary_duration > self.config['abandoned_object']['duration']:
                    return {
                        'duration': stationary_duration,
                        'message': f"ë¬¼ì²´ê°€ {stationary_duration:.1f}ì´ˆ ë™ì•ˆ ë°©ì¹˜ë¨"
                    }
        
        return None


# ì›¹ ëŒ€ì‹œë³´ë“œ
def create_security_dashboard():
    """ë³´ì•ˆ ì‹œìŠ¤í…œ ì›¹ ëŒ€ì‹œë³´ë“œ"""
    import gradio as gr
    import plotly.graph_objects as go
    
    security_system = SmartSecuritySystem()
    
    def process_video_file(video_file, confidence_threshold):
        """ì—…ë¡œë“œëœ ë¹„ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬"""
        if video_file is None:
            return None, "ë¹„ë””ì˜¤ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”"
        
        # ì„ì‹œ ì„¤ì • ì—…ë°ì´íŠ¸
        security_system.config['confidence_threshold'] = confidence_threshold
        
        cap = cv2.VideoCapture(video_file.name)
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        
        frames = []
        events = []
        frame_id = 0
        
        while len(frames) < 150:  # ìµœëŒ€ 5ì´ˆ ì²˜ë¦¬
            ret, frame = cap.read()
            if not ret:
                break
            
            annotated_frame, frame_events = security_system.process_frame(frame, frame_id)
            frames.append(annotated_frame)
            events.extend(frame_events)
            frame_id += 1
        
        cap.release()
        
        # ê²°ê³¼ ë¹„ë””ì˜¤ ìƒì„±
        output_path = "analyzed_video.mp4"
        height, width = frames[0].shape[:2]
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        for frame in frames:
            out.write(frame)
        
        out.release()
        
        # ì´ë²¤íŠ¸ ìš”ì•½
        event_summary = f"ì´ {len(events)}ê°œì˜ ì´ë²¤íŠ¸ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
        event_types = defaultdict(int)
        for event in events:
            event_types[event['type']] += 1
        
        for event_type, count in event_types.items():
            event_summary += f"- {event_type}: {count}íšŒ\n"
        
        return output_path, event_summary
    
    def generate_statistics():
        """í†µê³„ ê·¸ë˜í”„ ìƒì„±"""
        # ì‹œê°„ëŒ€ë³„ ê°ì²´ ìˆ˜
        hours = list(range(24))
        object_counts = np.random.poisson(10, 24)  # ì˜ˆì‹œ ë°ì´í„°
        
        fig1 = go.Figure(data=go.Bar(x=hours, y=object_counts))
        fig1.update_layout(
            title="ì‹œê°„ëŒ€ë³„ ê°ì²´ íƒì§€ ìˆ˜",
            xaxis_title="ì‹œê°„",
            yaxis_title="ê°ì²´ ìˆ˜"
        )
        
        # í´ë˜ìŠ¤ë³„ ë¶„í¬
        classes = ['person', 'car', 'bicycle', 'motorcycle', 'truck']
        class_counts = [45, 30, 15, 8, 12]
        
        fig2 = go.Figure(data=go.Pie(labels=classes, values=class_counts))
        fig2.update_layout(title="ê°ì²´ í´ë˜ìŠ¤ ë¶„í¬")
        
        return fig1, fig2
    
    # Gradio ì¸í„°í˜ì´ìŠ¤
    with gr.Blocks(title="AI ë³´ì•ˆ ì‹œìŠ¤í…œ", theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # ğŸ”’ AI ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ë³´ì•ˆ ì‹œìŠ¤í…œ
        
        ì‹¤ì‹œê°„ ê°ì²´ íƒì§€, ì¶”ì , ê·¸ë¦¬ê³  ì´ìƒ í–‰ë™ ê°ì§€
        """)
        
        with gr.Tab("ğŸ“¹ ë¹„ë””ì˜¤ ë¶„ì„"):
            with gr.Row():
                with gr.Column():
                    video_input = gr.Video(label="ë¹„ë””ì˜¤ ì—…ë¡œë“œ")
                    confidence_slider = gr.Slider(
                        minimum=0.1,
                        maximum=0.9,
                        value=0.5,
                        step=0.1,
                        label="íƒì§€ ì‹ ë¢°ë„ ì„ê³„ê°’"
                    )
                    analyze_btn = gr.Button("ë¶„ì„ ì‹œì‘", variant="primary")
                
                with gr.Column():
                    video_output = gr.Video(label="ë¶„ì„ ê²°ê³¼")
                    event_summary = gr.Textbox(label="ì´ë²¤íŠ¸ ìš”ì•½", lines=5)
            
            analyze_btn.click(
                fn=process_video_file,
                inputs=[video_input, confidence_slider],
                outputs=[video_output, event_summary]
            )
        
        with gr.Tab("ğŸ“Š í†µê³„ ëŒ€ì‹œë³´ë“œ"):
            with gr.Row():
                hourly_chart = gr.Plot(label="ì‹œê°„ëŒ€ë³„ í†µê³„")
                class_chart = gr.Plot(label="ê°ì²´ í´ë˜ìŠ¤ ë¶„í¬")
            
            refresh_btn = gr.Button("í†µê³„ ìƒˆë¡œê³ ì¹¨")
            refresh_btn.click(
                fn=generate_statistics,
                outputs=[hourly_chart, class_chart]
            )
        
        with gr.Tab("âš™ï¸ ì„¤ì •"):
            gr.Markdown("""
            ### ì‹œìŠ¤í…œ ì„¤ì •
            
            - **ì•Œë¦¼ ì´ë©”ì¼**: security@example.com
            - **ë…¹í™” ì €ì¥ ê²½ë¡œ**: /security_recordings
            - **ìµœëŒ€ ì¶”ì  ê°ì²´ ìˆ˜**: 50
            - **ì•Œë¦¼ ì¿¨ë‹¤ìš´**: 30ì´ˆ
            
            ### ì˜ì‹¬ í–‰ë™ ì„¤ì •
            
            - **ë°°íšŒ**: 60ì´ˆ ì´ìƒ ê°™ì€ ì¥ì†Œì— ë¨¸ë¬´ë¥´ê¸°
            - **ë¹ ë¥¸ ì›€ì§ì„**: 5 pixels/s ì´ìƒì˜ ì†ë„
            - **ë°©ì¹˜ëœ ë¬¼ì²´**: 5ë¶„ ì´ìƒ ì›€ì§ì„ ì—†ìŒ
            """)
        
        # ì˜ˆì‹œ ì¶”ê°€
        gr.Examples(
            examples=[
                ["example_security_1.mp4", 0.5],
                ["example_security_2.mp4", 0.7]
            ],
            inputs=[video_input, confidence_slider]
        )
    
    return demo

# ì‹œìŠ¤í…œ ì‹¤í–‰
def run_security_system():
    """ë³´ì•ˆ ì‹œìŠ¤í…œ ì‹¤í–‰"""
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘      ğŸ”’ AI ìŠ¤ë§ˆíŠ¸ ë³´ì•ˆ ì‹œìŠ¤í…œ ğŸ”’       â•‘
    â•‘                                      â•‘
    â•‘   ì‹¤ì‹œê°„ ê°ì‹œì™€ ì´ìƒ í–‰ë™ íƒì§€        â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    print("\nì‹¤í–‰ ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”:")
    print("1. ì‹¤ì‹œê°„ ì¹´ë©”ë¼ ëª¨ë‹ˆí„°ë§")
    print("2. ë¹„ë””ì˜¤ íŒŒì¼ ë¶„ì„")
    print("3. ì›¹ ëŒ€ì‹œë³´ë“œ")
    
    choice = input("\nì„ íƒ (1-3): ")
    
    if choice == "1":
        system = SmartSecuritySystem()
        system.run_live_monitoring(source=0)  # ì›¹ìº 
        
    elif choice == "2":
        video_path = input("ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ: ")
        system = SmartSecuritySystem()
        system.run_live_monitoring(source=video_path)
        
    elif choice == "3":
        dashboard = create_security_dashboard()
        dashboard.launch(share=True)
    
    else:
        print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")

if __name__ == "__main__":
    run_security_system()
```

### 7.3 ê³ ê¸‰ ê¸°ëŠ¥ êµ¬í˜„

ì´ì œ ë” ê³ ê¸‰ ê¸°ëŠ¥ë“¤ì„ ì¶”ê°€í•´ë´…ì‹œë‹¤:

```python
# advanced_security_features.py - ê³ ê¸‰ ë³´ì•ˆ ê¸°ëŠ¥

import face_recognition
import pickle
from sklearn.cluster import DBSCAN
import networkx as nx
from collections import Counter

class AdvancedSecurityFeatures:
    """ê³ ê¸‰ ë³´ì•ˆ ê¸°ëŠ¥ ëª¨ìŒ"""
    
    def __init__(self):
        self.known_faces = {}
        self.load_known_faces()
    
    def load_known_faces(self):
        """ì•Œë ¤ì§„ ì–¼êµ´ ë°ì´í„° ë¡œë“œ"""
        try:
            with open('known_faces.pkl', 'rb') as f:
                self.known_faces = pickle.load(f)
        except FileNotFoundError:
            print("ì•Œë ¤ì§„ ì–¼êµ´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    def face_recognition_analysis(self, frame: np.ndarray) -> List[Dict]:
        """ì–¼êµ´ ì¸ì‹ ë¶„ì„"""
        # ì–¼êµ´ ì°¾ê¸°
        face_locations = face_recognition.face_locations(frame)
        face_encodings = face_recognition.face_encodings(frame, face_locations)
        
        results = []
        
        for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):
            # ì•Œë ¤ì§„ ì–¼êµ´ê³¼ ë¹„êµ
            matches = face_recognition.compare_faces(
                list(self.known_faces.values()),
                face_encoding
            )
            
            name = "Unknown"
            
            if True in matches:
                match_index = matches.index(True)
                name = list(self.known_faces.keys())[match_index]
            
            results.append({
                'name': name,
                'location': (left, top, right, bottom),
                'authorized': name != "Unknown"
            })
        
        return results
    
    def crowd_analysis(self, detections: sv.Detections) -> Dict:
        """êµ°ì¤‘ ë¶„ì„"""
        # ì‚¬ëŒë§Œ í•„í„°ë§
        person_indices = [i for i, class_id in enumerate(detections.class_id) 
                         if class_id == 0]  # 0ì€ 'person' í´ë˜ìŠ¤
        
        if len(person_indices) < 2:
            return {'density': 'low', 'clusters': 0}
        
        # ìœ„ì¹˜ ì¶”ì¶œ
        positions = []
        for i in person_indices:
            bbox = detections.xyxy[i]
            center = ((bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2)
            positions.append(center)
        
        positions = np.array(positions)
        
        # DBSCAN í´ëŸ¬ìŠ¤í„°ë§
        clustering = DBSCAN(eps=100, min_samples=3).fit(positions)
        n_clusters = len(set(clustering.labels_)) - (1 if -1 in clustering.labels_ else 0)
        
        # ë°€ë„ ê³„ì‚°
        total_people = len(person_indices)
        if total_people < 5:
            density = 'low'
        elif total_people < 15:
            density = 'medium'
        else:
            density = 'high'
        
        return {
            'density': density,
            'clusters': n_clusters,
            'total_people': total_people,
            'positions': positions,
            'labels': clustering.labels_
        }
    
    def path_prediction(self, track_history: Dict, track_id: int) -> Optional[np.ndarray]:
        """ê²½ë¡œ ì˜ˆì¸¡"""
        if track_id not in track_history:
            return None
        
        positions = list(track_history[track_id]['positions'])
        
        if len(positions) < 10:
            return None
        
        # ìµœê·¼ 10ê°œ ìœ„ì¹˜
        recent_positions = np.array(positions[-10:])
        
        # ê°„ë‹¨í•œ ì„ í˜• ì˜ˆì¸¡
        x_positions = recent_positions[:, 0]
        y_positions = recent_positions[:, 1]
        
        # ì„ í˜• íšŒê·€
        t = np.arange(len(x_positions))
        x_coef = np.polyfit(t, x_positions, 1)
        y_coef = np.polyfit(t, y_positions, 1)
        
        # ë‹¤ìŒ 5í”„ë ˆì„ ì˜ˆì¸¡
        future_t = np.arange(len(x_positions), len(x_positions) + 5)
        future_x = np.polyval(x_coef, future_t)
        future_y = np.polyval(y_coef, future_t)
        
        future_positions = np.column_stack((future_x, future_y))
        
        return future_positions
    
    def zone_monitoring(self, frame_shape: Tuple[int, int]) -> Dict:
        """êµ¬ì—­ë³„ ëª¨ë‹ˆí„°ë§ ì„¤ì •"""
        height, width = frame_shape
        
        zones = {
            'entrance': {
                'polygon': np.array([
                    [0, height * 0.7],
                    [width * 0.3, height * 0.7],
                    [width * 0.3, height],
                    [0, height]
                ], dtype=np.int32),
                'type': 'entrance',
                'max_loitering_time': 30
            },
            'restricted': {
                'polygon': np.array([
                    [width * 0.7, 0],
                    [width, 0],
                    [width, height * 0.3],
                    [width * 0.7, height * 0.3]
                ], dtype=np.int32),
                'type': 'restricted',
                'authorized_only': True
            },
            'exit': {
                'polygon': np.array([
                    [width * 0.7, height * 0.7],
                    [width, height * 0.7],
                    [width, height],
                    [width * 0.7, height]
                ], dtype=np.int32),
                'type': 'exit',
                'direction': 'out'
            }
        }
        
        return zones
    
    def check_zone_violations(self, position: Tuple[float, float], 
                            zones: Dict, is_authorized: bool = False) -> List[str]:
        """êµ¬ì—­ ìœ„ë°˜ í™•ì¸"""
        violations = []
        
        point = np.array(position, dtype=np.float32)
        
        for zone_name, zone_info in zones.items():
            # ì ì´ ë‹¤ê°í˜• ë‚´ë¶€ì— ìˆëŠ”ì§€ í™•ì¸
            result = cv2.pointPolygonTest(zone_info['polygon'], point, False)
            
            if result >= 0:  # ë‚´ë¶€ ë˜ëŠ” ê²½ê³„
                if zone_info['type'] == 'restricted' and not is_authorized:
                    violations.append(f"Unauthorized access to {zone_name}")
                
        return violations
    
    def network_analysis(self, track_history: Dict, time_window: float = 60) -> nx.Graph:
        """ê°ì²´ ê°„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„"""
        G = nx.Graph()
        
        current_time = time.time()
        
        # í™œì„± íŠ¸ë™ë§Œ ì„ íƒ
        active_tracks = {
            track_id: data for track_id, data in track_history.items()
            if current_time - data['last_seen'] < time_window
        }
        
        # ë…¸ë“œ ì¶”ê°€
        for track_id, data in active_tracks.items():
            G.add_node(track_id, 
                      object_class=data['class'],
                      duration=current_time - data['first_seen'])
        
        # ì—£ì§€ ì¶”ê°€ (ê·¼ì ‘ì„± ê¸°ë°˜)
        track_ids = list(active_tracks.keys())
        
        for i in range(len(track_ids)):
            for j in range(i + 1, len(track_ids)):
                id1, id2 = track_ids[i], track_ids[j]
                
                # ìµœê·¼ ìœ„ì¹˜
                if active_tracks[id1]['positions'] and active_tracks[id2]['positions']:
                    pos1 = active_tracks[id1]['positions'][-1]
                    pos2 = active_tracks[id2]['positions'][-1]
                    
                    distance = np.sqrt((pos1[0] - pos2[0])**2 + (pos1[1] - pos2[1])**2)
                    
                    if distance < 100:  # ê·¼ì ‘ ì„ê³„ê°’
                        G.add_edge(id1, id2, weight=1/distance)
        
        return G
    
    def generate_heatmap(self, track_history: Dict, frame_shape: Tuple[int, int]) -> np.ndarray:
        """ì›€ì§ì„ íˆíŠ¸ë§µ ìƒì„±"""
        height, width = frame_shape
        heatmap = np.zeros((height, width), dtype=np.float32)
        
        for track_data in track_history.values():
            for position in track_data['positions']:
                x, y = int(position[0]), int(position[1])
                
                # ê°€ìš°ì‹œì•ˆ ì»¤ë„ ì ìš©
                if 0 <= x < width and 0 <= y < height:
                    cv2.circle(heatmap, (x, y), 20, 1, -1)
        
        # ì •ê·œí™”
        heatmap = cv2.GaussianBlur(heatmap, (21, 21), 0)
        heatmap = (heatmap / heatmap.max() * 255).astype(np.uint8)
        
        # ì»¬ëŸ¬ë§µ ì ìš©
        heatmap_colored = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)
        
        return heatmap_colored

# ì‹¤ì‹œê°„ ë¶„ì„ ëŒ€ì‹œë³´ë“œ
class RealtimeAnalyticsDashboard:
    """ì‹¤ì‹œê°„ ë¶„ì„ ëŒ€ì‹œë³´ë“œ"""
    
    def __init__(self, security_system: SmartSecuritySystem):
        self.security_system = security_system
        self.advanced_features = AdvancedSecurityFeatures()
        
    def create_dashboard(self):
        """ëŒ€ì‹œë³´ë“œ ìƒì„±"""
        import plotly.graph_objects as go
        from plotly.subplots import make_subplots
        import dash
        from dash import dcc, html
        from dash.dependencies import Input, Output
        import dash_bootstrap_components as dbc
        
        # Dash ì•± ì´ˆê¸°í™”
        app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])
        
        app.layout = dbc.Container([
            dbc.Row([
                dbc.Col([
                    html.H1("ğŸ”’ ì‹¤ì‹œê°„ ë³´ì•ˆ ë¶„ì„ ëŒ€ì‹œë³´ë“œ", className="text-center mb-4"),
                    html.Hr()
                ])
            ]),
            
            dbc.Row([
                dbc.Col([
                    dcc.Graph(id='live-video-feed'),
                    dcc.Interval(id='video-update', interval=100)  # 100msë§ˆë‹¤ ì—…ë°ì´íŠ¸
                ], width=8),
                
                dbc.Col([
                    dbc.Card([
                        dbc.CardHeader("ì‹¤ì‹œê°„ í†µê³„"),
                        dbc.CardBody([
                            html.H4(id='total-objects', children="0"),
                            html.P("ì´ ê°ì²´ ìˆ˜"),
                            html.Hr(),
                            html.H4(id='alert-count', children="0"),
                            html.P("ê²½ê³  íšŸìˆ˜"),
                            html.Hr(),
                            html.H4(id='crowd-density', children="Low"),
                            html.P("êµ°ì¤‘ ë°€ë„")
                        ])
                    ])
                ], width=4)
            ], className="mb-4"),
            
            dbc.Row([
                dbc.Col([
                    dcc.Graph(id='heatmap')
                ], width=6),
                
                dbc.Col([
                    dcc.Graph(id='network-graph')
                ], width=6)
            ]),
            
            dbc.Row([
                dbc.Col([
                    dcc.Graph(id='timeline-chart'),
                    dcc.Interval(id='chart-update', interval=1000)  # 1ì´ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸
                ])
            ], className="mt-4")
        ], fluid=True)
        
        # ì½œë°± í•¨ìˆ˜ë“¤
        @app.callback(
            [Output('total-objects', 'children'),
             Output('alert-count', 'children'),
             Output('crowd-density', 'children')],
            [Input('video-update', 'n_intervals')]
        )
        def update_stats(n):
            """í†µê³„ ì—…ë°ì´íŠ¸"""
            total_objects = len(self.security_system.track_history)
            alert_count = len([e for e in self.security_system.event_log 
                             if e.get('type') in ['loitering', 'running']])
            
            # êµ°ì¤‘ ë°€ë„ ê³„ì‚° (ê°„ë‹¨í•œ ì˜ˆì‹œ)
            active_objects = sum(1 for track in self.security_system.track_history.values()
                               if time.time() - track['last_seen'] < 5)
            
            if active_objects < 5:
                density = "Low"
            elif active_objects < 15:
                density = "Medium"
            else:
                density = "High"
            
            return str(total_objects), str(alert_count), density
        
        @app.callback(
            Output('timeline-chart', 'figure'),
            [Input('chart-update', 'n_intervals')]
        )
        def update_timeline(n):
            """íƒ€ì„ë¼ì¸ ì°¨íŠ¸ ì—…ë°ì´íŠ¸"""
            # ìµœê·¼ 60ì´ˆ ë°ì´í„°
            current_time = time.time()
            time_bins = defaultdict(int)
            
            for event in self.security_system.event_log:
                if 'time' in event:
                    time_diff = current_time - event['time']
                    if time_diff < 60:
                        bin_index = int(time_diff / 5)  # 5ì´ˆ ë‹¨ìœ„
                        time_bins[bin_index] += 1
            
            x = list(range(12))
            y = [time_bins.get(11-i, 0) for i in x]
            
            fig = go.Figure(data=go.Bar(x=x, y=y))
            fig.update_layout(
                title="ìµœê·¼ 60ì´ˆ ì´ë²¤íŠ¸ íƒ€ì„ë¼ì¸",
                xaxis_title="ì‹œê°„ (5ì´ˆ ë‹¨ìœ„)",
                yaxis_title="ì´ë²¤íŠ¸ ìˆ˜",
                xaxis=dict(ticktext=[f"{i*5}s" for i in range(12)],
                          tickvals=list(range(12)))
            )
            
            return fig
        
        return app

# ì „ì²´ ì‹œìŠ¤í…œ í†µí•©
def run_advanced_security_system():
    """ê³ ê¸‰ ë³´ì•ˆ ì‹œìŠ¤í…œ ì‹¤í–‰"""
    
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘    ğŸ”’ ê³ ê¸‰ AI ë³´ì•ˆ ì‹œìŠ¤í…œ v2.0 ğŸ”’         â•‘
    â•‘                                          â•‘
    â•‘  â€¢ ì–¼êµ´ ì¸ì‹        â€¢ êµ°ì¤‘ ë¶„ì„          â•‘
    â•‘  â€¢ ê²½ë¡œ ì˜ˆì¸¡        â€¢ êµ¬ì—­ ëª¨ë‹ˆí„°ë§       â•‘
    â•‘  â€¢ ë„¤íŠ¸ì›Œí¬ ë¶„ì„    â€¢ íˆíŠ¸ë§µ ìƒì„±        â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    security_system = SmartSecuritySystem()
    advanced_features = AdvancedSecurityFeatures()
    
    # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ with ê³ ê¸‰ ê¸°ëŠ¥
    cap = cv2.VideoCapture(0)
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        # ê¸°ë³¸ ì²˜ë¦¬
        annotated_frame, events = security_system.process_frame(frame, 0)
        
        # ì–¼êµ´ ì¸ì‹
        faces = advanced_features.face_recognition_analysis(frame)
        for face in faces:
            x1, y1, x2, y2 = face['location']
            color = (0, 255, 0) if face['authorized'] else (0, 0, 255)
            cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)
            cv2.putText(annotated_frame, face['name'], (x1, y1-10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
        
        # íˆíŠ¸ë§µ ì˜¤ë²„ë ˆì´
        heatmap = advanced_features.generate_heatmap(
            security_system.track_history, 
            frame.shape[:2]
        )
        annotated_frame = cv2.addWeighted(annotated_frame, 0.7, heatmap, 0.3, 0)
        
        cv2.imshow('Advanced Security System', annotated_frame)
        
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    
    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    run_advanced_security_system()
```

---

## Day 2 ë§ˆë¬´ë¦¬ ë° ê³¼ì œ

### ì˜¤ëŠ˜ ë°°ìš´ ë‚´ìš© ì •ë¦¬

1. **YOLOì˜ ì—­ì‚¬ì™€ ë°œì „**
   - YOLOv1ë¶€í„° YOLOv12ê¹Œì§€ì˜ ì§„í™”
   - ê° ë²„ì „ì˜ íŠ¹ì§•ê³¼ í˜ì‹ 
   - ì‹¤ì‹œê°„ ê°ì²´ íƒì§€ì˜ ì›ë¦¬

2. **ê°ì²´ ì¶”ì  ê¸°ìˆ **
   - ByteTrack ì•Œê³ ë¦¬ì¦˜
   - ë‹¤ì¤‘ ê°ì²´ ì¶”ì 
   - ì¶”ì  ë°ì´í„° ë¶„ì„

3. **í–‰ë™ ì¸ì‹**
   - VideoMAEì™€ ìµœì‹  ëª¨ë¸ë“¤
   - ì‹œê°„ì  íŠ¹ì§• ì¶”ì¶œ
   - í–‰ë™ ë¶„ë¥˜ì™€ ì˜ˆì¸¡

4. **ì‹¤ì „ í”„ë¡œì íŠ¸**
   - ìŠ¤ë§ˆíŠ¸ ë³´ì•ˆ ì‹œìŠ¤í…œ êµ¬ì¶•
   - ì´ìƒ í–‰ë™ íƒì§€
   - ì‹¤ì‹œê°„ ì•Œë¦¼ ì‹œìŠ¤í…œ
   - ê³ ê¸‰ ë¶„ì„ ê¸°ëŠ¥

### ì‹¤ìŠµ ê³¼ì œ

1. **ê¸°ë³¸ ê³¼ì œ**
   - ìì‹ ë§Œì˜ ê°ì²´ íƒì§€ ê·œì¹™ ì¶”ê°€
   - ìƒˆë¡œìš´ ì´ìƒ í–‰ë™ íŒ¨í„´ ì •ì˜
   - ì•Œë¦¼ ì‹œìŠ¤í…œ ì»¤ìŠ¤í„°ë§ˆì´ì§•

2. **ë„ì „ ê³¼ì œ**
   - ë‹¤ì¤‘ ì¹´ë©”ë¼ ì§€ì› ì¶”ê°€
   - 3D í¬ì¦ˆ ì¶”ì • í†µí•©
   - ì˜ˆì¸¡ ê²½ë¡œ ì‹œê°í™”

3. **ì°½ì˜ ê³¼ì œ**
   - ìŠ¤í¬ì¸  ê²½ê¸° ë¶„ì„ ì‹œìŠ¤í…œ
   - ë§¤ì¥ ê³ ê° í–‰ë™ ë¶„ì„
   - êµí†µ íë¦„ ëª¨ë‹ˆí„°ë§

### ë‚´ì¼ ì˜ˆê³ 

ë§ˆì§€ë§‰ ë‚ ì¸ ë‚´ì¼ì€ ëª¨ë“  ê²ƒì„ í†µí•©í•©ë‹ˆë‹¤!
- ëª¨ë¸ ìµœì í™”ì™€ ê²½ëŸ‰í™”
- ìŒì„± ì¸í„°í˜ì´ìŠ¤ êµ¬ì¶•
- ìµœì¢… í”„ë¡œì íŠ¸: ìŒì„± ëŒ€í™”í˜• AI ë¹„ì„œ

ì˜¤ëŠ˜ë„ ìˆ˜ê³  ë§ìœ¼ì…¨ìŠµë‹ˆë‹¤! ğŸ‰

---

# ğŸ¯ Day 3: AI ì—ì´ì „íŠ¸ - ëª¨ë“  ê²ƒì„ í†µí•©í•˜ë‹¤

## 8ì¥: ëª¨ë¸ ìµœì í™”ì˜ ë§ˆë²• - ë” ë¹ ë¥´ê³  ê°€ë³ê²Œ

### 8.1 ì™œ ëª¨ë¸ ìµœì í™”ê°€ ì¤‘ìš”í•œê°€?

ì—¬ëŸ¬ë¶„ì´ ë§Œë“  ë©‹ì§„ AI ëª¨ë¸ì´ ìˆë‹¤ê³  ìƒìƒí•´ë³´ì„¸ìš”. í•˜ì§€ë§Œ ì‹¤í–‰í•˜ëŠ”ë° 10ì´ˆê°€ ê±¸ë¦¬ê³ , 32GB ë©”ëª¨ë¦¬ê°€ í•„ìš”í•˜ë‹¤ë©´? ì‹¤ì œë¡œ ì‚¬ìš©í•˜ê¸° ì–´ë µê² ì£ . ëª¨ë¸ ìµœì í™”ëŠ” ì´ëŸ° ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤!

### 8.2 2025ë…„ ìµœì‹  ìµœì í™” ê¸°ìˆ ë“¤

```python
# model_optimization_showcase.py - ëª¨ë¸ ìµœì í™” ê¸°ìˆ  ì‹œì—°

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple
import time

class ModelOptimizationShowcase:
    """ë‹¤ì–‘í•œ ëª¨ë¸ ìµœì í™” ê¸°ìˆ ì„ ë³´ì—¬ì£¼ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.techniques = {
            'original': 'ì›ë³¸ ëª¨ë¸',
            'quantization': 'ì–‘ìí™” (Quantization)',
            'pruning': 'ê°€ì§€ì¹˜ê¸° (Pruning)',
            'distillation': 'ì§€ì‹ ì¦ë¥˜ (Knowledge Distillation)',
            'mixed_precision': 'í˜¼í•© ì •ë°€ë„ (Mixed Precision)',
            'compilation': 'ì»´íŒŒì¼ ìµœì í™”'
        }
    
    def visualize_optimization_impact(self):
        """ìµœì í™” ê¸°ìˆ ì˜ íš¨ê³¼ ì‹œê°í™”"""
        
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        
        # 1. ëª¨ë¸ í¬ê¸° ë¹„êµ
        ax1 = axes[0, 0]
        techniques = list(self.techniques.keys())
        sizes = [100, 25, 70, 30, 100, 100]  # MB
        colors = ['red', 'green', 'blue', 'orange', 'purple', 'brown']
        
        bars = ax1.bar(techniques, sizes, color=colors)
        ax1.set_ylabel('ëª¨ë¸ í¬ê¸° (MB)')
        ax1.set_title('ëª¨ë¸ í¬ê¸° ë¹„êµ', fontweight='bold')
        ax1.set_xticklabels([self.techniques[t] for t in techniques], rotation=45, ha='right')
        
        # í¬ê¸° ê°ì†Œìœ¨ í‘œì‹œ
        for i, (bar, size) in enumerate(zip(bars, sizes)):
            if i > 0:
                reduction = (sizes[0] - size) / sizes[0] * 100
                if reduction > 0:
                    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,
                            f'-{reduction:.0f}%', ha='center', fontweight='bold')
        
        # 2. ì¶”ë¡  ì†ë„ ë¹„êµ
        ax2 = axes[0, 1]
        speeds = [1.0, 0.3, 0.7, 0.5, 0.2, 0.15]  # ì´ˆ
        
        bars2 = ax2.bar(techniques, speeds, color=colors)
        ax2.set_ylabel('ì¶”ë¡  ì‹œê°„ (ì´ˆ)')
        ax2.set_title('ì¶”ë¡  ì†ë„ ë¹„êµ', fontweight='bold')
        ax2.set_xticklabels([self.techniques[t] for t in techniques], rotation=45, ha='right')
        
        # ì†ë„ í–¥ìƒ í‘œì‹œ
        for i, (bar, speed) in enumerate(zip(bars2, speeds)):
            if i > 0:
                speedup = speeds[0] / speed
                if speedup > 1:
                    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
                            f'{speedup:.1f}x', ha='center', fontweight='bold')
        
        # 3. ì •í™•ë„ vs ì†ë„ íŠ¸ë ˆì´ë“œì˜¤í”„
        ax3 = axes[1, 0]
        accuracies = [95, 93, 94, 92, 95, 95]  # %
        
        # ë²„ë¸” ì°¨íŠ¸
        for i, (tech, acc, speed, size) in enumerate(zip(techniques, accuracies, speeds, sizes)):
            ax3.scatter(speed, acc, s=size*10, alpha=0.6, c=colors[i], 
                       label=self.techniques[tech])
        
        ax3.set_xlabel('ì¶”ë¡  ì‹œê°„ (ì´ˆ)')
        ax3.set_ylabel('ì •í™•ë„ (%)')
        ax3.set_title('ì •í™•ë„ vs ì†ë„ íŠ¸ë ˆì´ë“œì˜¤í”„', fontweight='bold')
        ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        ax3.grid(True, alpha=0.3)
        
        # 4. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        ax4 = axes[1, 1]
        memory_usage = [8.0, 2.0, 5.5, 3.0, 4.0, 7.5]  # GB
        
        # ì›í˜• ì°¨íŠ¸
        explode = [0.1 if m == min(memory_usage) else 0 for m in memory_usage]
        wedges, texts, autotexts = ax4.pie(memory_usage, labels=techniques, 
                                           autopct='%1.1f GB', explode=explode,
                                           colors=colors)
        ax4.set_title('ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ', fontweight='bold')
        
        plt.tight_layout()
        plt.show()

# ì‹œê°í™” ì‹¤í–‰
showcase = ModelOptimizationShowcase()
showcase.visualize_optimization_impact()
```

### 8.3 ì–‘ìí™” (Quantization) ê¹Šì´ ì´í•´í•˜ê¸°

ì–‘ìí™”ëŠ” ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ì™€ í™œì„±í™”ë¥¼ ë‚®ì€ ì •ë°€ë„ë¡œ í‘œí˜„í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.

```python
class QuantizationExplainer:
    """ì–‘ìí™” ê¸°ìˆ ì„ ì„¤ëª…í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def explain_quantization_types(self):
        """ë‹¤ì–‘í•œ ì–‘ìí™” ê¸°ë²• ì„¤ëª…"""
        
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        
        # 1. FP32 vs INT8 ë¹„êµ
        ax1 = axes[0, 0]
        ax1.set_title('FP32 vs INT8', fontweight='bold')
        
        # ìˆ«ì ë²”ìœ„ í‘œì‹œ
        fp32_range = np.linspace(-1, 1, 1000)
        int8_range = np.linspace(-128, 127, 256)
        
        ax1.hist(fp32_range, bins=50, alpha=0.5, label='FP32 (32ë¹„íŠ¸)', color='blue')
        ax1.hist(int8_range/128, bins=50, alpha=0.5, label='INT8 (8ë¹„íŠ¸)', color='red')
        ax1.set_xlabel('ê°’ì˜ ë²”ìœ„')
        ax1.set_ylabel('ë¹ˆë„')
        ax1.legend()
        
        # 2. ë™ì  ì–‘ìí™”
        ax2 = axes[0, 1]
        ax2.set_title('ë™ì  ì–‘ìí™”', fontweight='bold')
        
        # ê°€ì¤‘ì¹˜ ë¶„í¬
        weights = np.random.normal(0, 0.5, 1000)
        ax2.hist(weights, bins=50, alpha=0.7, color='green')
        ax2.axvline(weights.min(), color='red', linestyle='--', label='Min')
        ax2.axvline(weights.max(), color='red', linestyle='--', label='Max')
        ax2.set_xlabel('ê°€ì¤‘ì¹˜ ê°’')
        ax2.set_ylabel('ë¹ˆë„')
        ax2.legend()
        
        # 3. ì •ì  ì–‘ìí™”
        ax3 = axes[0, 2]
        ax3.set_title('ì •ì  ì–‘ìí™”', fontweight='bold')
        
        # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„°
        calibration_data = np.random.normal(0, 1, 100)
        ax3.plot(calibration_data, 'o-', alpha=0.5, label='ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„°')
        ax3.axhline(y=np.percentile(calibration_data, 99), color='red', 
                   linestyle='--', label='99% ë°±ë¶„ìœ„ìˆ˜')
        ax3.axhline(y=np.percentile(calibration_data, 1), color='red', 
                   linestyle='--')
        ax3.set_xlabel('ìƒ˜í”Œ')
        ax3.set_ylabel('í™œì„±í™” ê°’')
        ax3.legend()
        
        # 4. QAT (Quantization Aware Training)
        ax4 = axes[1, 0]
        ax4.set_title('QAT í•™ìŠµ ê³¼ì •', fontweight='bold')
        
        epochs = np.arange(100)
        normal_loss = 2 * np.exp(-epochs/20) + 0.1
        qat_loss = 2 * np.exp(-epochs/25) + 0.15
        
        ax4.plot(epochs, normal_loss, label='ì¼ë°˜ í•™ìŠµ', linewidth=2)
        ax4.plot(epochs, qat_loss, label='QAT í•™ìŠµ', linewidth=2)
        ax4.set_xlabel('ì—í­')
        ax4.set_ylabel('ì†ì‹¤')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        # 5. ì–‘ìí™” ì˜¤ë¥˜
        ax5 = axes[1, 1]
        ax5.set_title('ì–‘ìí™” ì˜¤ë¥˜ ë¶„ì„', fontweight='bold')
        
        original = np.linspace(-1, 1, 100)
        quantized = np.round(original * 127) / 127
        error = np.abs(original - quantized)
        
        ax5.plot(original, error, linewidth=2, color='red')
        ax5.fill_between(original, 0, error, alpha=0.3, color='red')
        ax5.set_xlabel('ì›ë³¸ ê°’')
        ax5.set_ylabel('ì–‘ìí™” ì˜¤ë¥˜')
        ax5.grid(True, alpha=0.3)
        
        # 6. ë¹„íŠ¸ ìˆ˜ì— ë”°ë¥¸ ì„±ëŠ¥
        ax6 = axes[1, 2]
        ax6.set_title('ë¹„íŠ¸ ìˆ˜ë³„ ì„±ëŠ¥', fontweight='bold')
        
        bits = [32, 16, 8, 4, 2, 1]
        accuracy = [95, 94.8, 94.2, 92.5, 88, 75]
        model_size = [100, 50, 25, 12.5, 6.25, 3.125]
        
        ax6_twin = ax6.twinx()
        
        line1 = ax6.plot(bits, accuracy, 'b-o', label='ì •í™•ë„ (%)', linewidth=2)
        line2 = ax6_twin.plot(bits, model_size, 'r-s', label='ëª¨ë¸ í¬ê¸° (MB)', linewidth=2)
        
        ax6.set_xlabel('ë¹„íŠ¸ ìˆ˜')
        ax6.set_ylabel('ì •í™•ë„ (%)', color='b')
        ax6_twin.set_ylabel('ëª¨ë¸ í¬ê¸° (MB)', color='r')
        
        lines = line1 + line2
        labels = [l.get_label() for l in lines]
        ax6.legend(lines, labels, loc='center right')
        
        plt.tight_layout()
        plt.show()
    
    def implement_simple_quantization(self):
        """ê°„ë‹¨í•œ ì–‘ìí™” êµ¬í˜„ ì˜ˆì‹œ"""
        
        print("\n=== ê°„ë‹¨í•œ INT8 ì–‘ìí™” êµ¬í˜„ ===\n")
        
        # ì˜ˆì‹œ í…ì„œ
        original_tensor = torch.randn(5, 5) * 2
        print("ì›ë³¸ í…ì„œ (FP32):")
        print(original_tensor)
        print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©: {original_tensor.element_size() * original_tensor.nelement()} bytes")
        
        # ì–‘ìí™”
        scale = original_tensor.abs().max() / 127
        zero_point = 0
        
        quantized_tensor = torch.round(original_tensor / scale).clamp(-128, 127).to(torch.int8)
        
        print(f"\nì–‘ìí™”ëœ í…ì„œ (INT8):")
        print(quantized_tensor)
        print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©: {quantized_tensor.element_size() * quantized_tensor.nelement()} bytes")
        print(f"ìŠ¤ì¼€ì¼: {scale:.6f}")
        
        # ì—­ì–‘ìí™”
        dequantized_tensor = quantized_tensor.float() * scale
        
        print(f"\nì—­ì–‘ìí™”ëœ í…ì„œ:")
        print(dequantized_tensor)
        
        # ì˜¤ë¥˜ ê³„ì‚°
        error = torch.abs(original_tensor - dequantized_tensor)
        print(f"\nì–‘ìí™” ì˜¤ë¥˜:")
        print(f"í‰ê·  ì˜¤ë¥˜: {error.mean():.6f}")
        print(f"ìµœëŒ€ ì˜¤ë¥˜: {error.max():.6f}")

# ì–‘ìí™” ì„¤ëª… ì‹¤í–‰
explainer = QuantizationExplainer()
explainer.explain_quantization_types()
explainer.implement_simple_quantization()
```

### 8.4 ì‹¤ì œ ëª¨ë¸ ìµœì í™” ì‹¤ìŠµ

ì´ì œ ì‹¤ì œ ëª¨ë¸ì„ ìµœì í™”í•´ë´…ì‹œë‹¤:

```python
# practical_optimization.py - ì‹¤ì œ ëª¨ë¸ ìµœì í™”

import torch
import torch.nn as nn
import torch.quantization as quantization
from torch.utils.data import DataLoader, TensorDataset
import time
import os

class OptimizableModel(nn.Module):
    """ìµœì í™”í•  ì˜ˆì‹œ ëª¨ë¸"""
    
    def __init__(self, input_size=784, hidden_size=256, num_classes=10):
        super().__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu1 = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.relu2 = nn.ReLU()
        self.fc3 = nn.Linear(hidden_size, num_classes)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.relu1(x)
        x = self.fc2(x)
        x = self.relu2(x)
        x = self.fc3(x)
        return x

class ModelOptimizer:
    """ëª¨ë¸ ìµœì í™” ë„êµ¬"""
    
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
    
    def create_dummy_data(self, num_samples=1000):
        """í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ë°ì´í„° ìƒì„±"""
        X = torch.randn(num_samples, 784)
        y = torch.randint(0, 10, (num_samples,))
        return DataLoader(TensorDataset(X, y), batch_size=32, shuffle=True)
    
    def measure_performance(self, model, dataloader, name="Model"):
        """ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •"""
        model.eval()
        
        # í¬ê¸° ì¸¡ì •
        torch.save(model.state_dict(), 'temp_model.pth')
        model_size = os.path.getsize('temp_model.pth') / (1024 * 1024)  # MB
        os.remove('temp_model.pth')
        
        # ì†ë„ ì¸¡ì •
        total_time = 0
        num_batches = 0
        
        with torch.no_grad():
            for X, _ in dataloader:
                X = X.to(self.device)
                
                start_time = time.time()
                _ = model(X)
                
                # GPU ë™ê¸°í™” (ì •í™•í•œ ì‹œê°„ ì¸¡ì •)
                if self.device.type == 'cuda':
                    torch.cuda.synchronize()
                
                total_time += time.time() - start_time
                num_batches += 1
                
                if num_batches >= 10:  # 10ë°°ì¹˜ë§Œ í…ŒìŠ¤íŠ¸
                    break
        
        avg_inference_time = total_time / num_batches * 1000  # ms
        
        print(f"\n{name} ì„±ëŠ¥:")
        print(f"  - ëª¨ë¸ í¬ê¸°: {model_size:.2f} MB")
        print(f"  - í‰ê·  ì¶”ë¡  ì‹œê°„: {avg_inference_time:.2f} ms/batch")
        
        return model_size, avg_inference_time
    
    def quantize_dynamic(self, model):
        """ë™ì  ì–‘ìí™” ì ìš©"""
        print("\në™ì  ì–‘ìí™” ì ìš© ì¤‘...")
        
        quantized_model = torch.quantization.quantize_dynamic(
            model,
            {nn.Linear},  # Linear ë ˆì´ì–´ë§Œ ì–‘ìí™”
            dtype=torch.qint8
        )
        
        return quantized_model
    
    def quantize_static(self, model, dataloader):
        """ì •ì  ì–‘ìí™” ì ìš©"""
        print("\nì •ì  ì–‘ìí™” ì ìš© ì¤‘...")
        
        # ëª¨ë¸ ì¤€ë¹„
        model.eval()
        model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
        
        # í“¨ì „ ê°€ëŠ¥í•œ ëª¨ë“ˆ ê²°í•©
        model_fused = torch.quantization.fuse_modules(
            model, 
            [['fc1', 'relu1'], ['fc2', 'relu2']]
        )
        
        # ì–‘ìí™” ì¤€ë¹„
        model_prepared = torch.quantization.prepare(model_fused)
        
        # ìº˜ë¦¬ë¸Œë ˆì´ì…˜
        print("  ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì‹¤í–‰ ì¤‘...")
        with torch.no_grad():
            for i, (X, _) in enumerate(dataloader):
                model_prepared(X)
                if i >= 10:  # 10ë°°ì¹˜ë¡œ ìº˜ë¦¬ë¸Œë ˆì´ì…˜
                    break
        
        # ì–‘ìí™” ë³€í™˜
        model_quantized = torch.quantization.convert(model_prepared)
        
        return model_quantized
    
    def prune_model(self, model, sparsity=0.5):
        """ê°€ì§€ì¹˜ê¸° ì ìš©"""
        print(f"\nê°€ì§€ì¹˜ê¸° ì ìš© ì¤‘... (í¬ì†Œì„±: {sparsity*100}%)")
        
        import torch.nn.utils.prune as prune
        
        # ê° Linear ë ˆì´ì–´ì— ê°€ì§€ì¹˜ê¸° ì ìš©
        for name, module in model.named_modules():
            if isinstance(module, nn.Linear):
                prune.l1_unstructured(module, name='weight', amount=sparsity)
        
        # ê°€ì§€ì¹˜ê¸° ì˜êµ¬ ì ìš©
        for name, module in model.named_modules():
            if isinstance(module, nn.Linear):
                prune.remove(module, 'weight')
        
        return model
    
    def knowledge_distillation(self, teacher_model, student_model, dataloader, epochs=5):
        """ì§€ì‹ ì¦ë¥˜"""
        print("\nì§€ì‹ ì¦ë¥˜ ìˆ˜í–‰ ì¤‘...")
        
        # í•™ìƒ ëª¨ë¸ (ë” ì‘ì€ ëª¨ë¸)
        small_model = OptimizableModel(hidden_size=64)  # ë” ì‘ì€ íˆë“  í¬ê¸°
        small_model = small_model.to(self.device)
        
        optimizer = torch.optim.Adam(small_model.parameters(), lr=0.001)
        
        # ì¦ë¥˜ ì†ì‹¤ í•¨ìˆ˜
        def distillation_loss(student_logits, teacher_logits, temperature=3.0):
            soft_targets = nn.functional.softmax(teacher_logits / temperature, dim=1)
            soft_predictions = nn.functional.log_softmax(student_logits / temperature, dim=1)
            return nn.functional.kl_div(soft_predictions, soft_targets, reduction='batchmean') * temperature * temperature
        
        teacher_model.eval()
        
        for epoch in range(epochs):
            total_loss = 0
            for X, y in dataloader:
                X = X.to(self.device)
                
                # êµì‚¬ ëª¨ë¸ ì˜ˆì¸¡
                with torch.no_grad():
                    teacher_logits = teacher_model(X)
                
                # í•™ìƒ ëª¨ë¸ ì˜ˆì¸¡
                student_logits = small_model(X)
                
                # ì†ì‹¤ ê³„ì‚°
                loss = distillation_loss(student_logits, teacher_logits)
                
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
            
            print(f"  Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}")
        
        return small_model
    
    def compile_model(self, model):
        """ëª¨ë¸ ì»´íŒŒì¼ (PyTorch 2.0+)"""
        if hasattr(torch, 'compile'):
            print("\nëª¨ë¸ ì»´íŒŒì¼ ì¤‘...")
            compiled_model = torch.compile(model, mode="reduce-overhead")
            return compiled_model
        else:
            print("\nPyTorch 2.0 ì´ìƒì´ í•„ìš”í•©ë‹ˆë‹¤.")
            return model
    
    def compare_all_optimizations(self):
        """ëª¨ë“  ìµœì í™” ê¸°ë²• ë¹„êµ"""
        print("\n=== ëª¨ë¸ ìµœì í™” ì¢…í•© ë¹„êµ ===\n")
        
        # ë°ì´í„° ì¤€ë¹„
        dataloader = self.create_dummy_data()
        
        # ì›ë³¸ ëª¨ë¸
        original_model = OptimizableModel().to(self.device)
        original_size, original_time = self.measure_performance(
            original_model, dataloader, "ì›ë³¸ ëª¨ë¸"
        )
        
        results = {
            'Original': {'size': original_size, 'time': original_time}
        }
        
        # 1. ë™ì  ì–‘ìí™”
        if self.device.type == 'cpu':
            dynamic_quantized = self.quantize_dynamic(original_model.cpu())
            size, time = self.measure_performance(
                dynamic_quantized, dataloader, "ë™ì  ì–‘ìí™”"
            )
            results['Dynamic Quantization'] = {'size': size, 'time': time}
        
        # 2. ê°€ì§€ì¹˜ê¸°
        pruned_model = self.prune_model(original_model.clone(), sparsity=0.5)
        size, time = self.measure_performance(
            pruned_model, dataloader, "ê°€ì§€ì¹˜ê¸° (50%)"
        )
        results['Pruning'] = {'size': size, 'time': time}
        
        # 3. ì§€ì‹ ì¦ë¥˜
        student_model = self.knowledge_distillation(
            original_model, None, dataloader
        )
        size, time = self.measure_performance(
            student_model, dataloader, "ì§€ì‹ ì¦ë¥˜ (ì‘ì€ ëª¨ë¸)"
        )
        results['Knowledge Distillation'] = {'size': size, 'time': time}
        
        # ê²°ê³¼ ì‹œê°í™”
        self.visualize_comparison(results)
        
        return results
    
    def visualize_comparison(self, results):
        """ìµœì í™” ê²°ê³¼ ë¹„êµ ì‹œê°í™”"""
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        methods = list(results.keys())
        sizes = [results[m]['size'] for m in methods]
        times = [results[m]['time'] for m in methods]
        
        # ëª¨ë¸ í¬ê¸° ë¹„êµ
        bars1 = ax1.bar(methods, sizes, color=['red', 'green', 'blue', 'orange'][:len(methods)])
        ax1.set_ylabel('ëª¨ë¸ í¬ê¸° (MB)')
        ax1.set_title('ëª¨ë¸ í¬ê¸° ë¹„êµ', fontweight='bold')
        ax1.tick_labels = ax1.set_xticklabels(methods, rotation=45, ha='right')
        
        # í¬ê¸° ê°ì†Œìœ¨ í‘œì‹œ
        for i, bar in enumerate(bars1):
            if i > 0:
                reduction = (sizes[0] - sizes[i]) / sizes[0] * 100
                ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                        f'-{reduction:.0f}%', ha='center', fontweight='bold')
        
        # ì¶”ë¡  ì‹œê°„ ë¹„êµ
        bars2 = ax2.bar(methods, times, color=['red', 'green', 'blue', 'orange'][:len(methods)])
        ax2.set_ylabel('ì¶”ë¡  ì‹œê°„ (ms/batch)')
        ax2.set_title('ì¶”ë¡  ì†ë„ ë¹„êµ', fontweight='bold')
        ax2.set_xticklabels(methods, rotation=45, ha='right')
        
        # ì†ë„ í–¥ìƒ í‘œì‹œ
        for i, bar in enumerate(bars2):
            if i > 0:
                speedup = times[0] / times[i]
                ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.2,
                        f'{speedup:.1f}x', ha='center', fontweight='bold')
        
        plt.tight_layout()
        plt.show()

# ìµœì í™” ì‹¤í–‰
optimizer = ModelOptimizer()
results = optimizer.compare_all_optimizations()
```

### 8.5 í•˜ë“œì›¨ì–´ë³„ ìµœì í™” ì „ëµ

```python
class HardwareOptimizer:
    """í•˜ë“œì›¨ì–´ë³„ ìµœì í™” ì „ëµ"""
    
    def __init__(self):
        self.device_info = self.detect_hardware()
    
    def detect_hardware(self):
        """í•˜ë“œì›¨ì–´ ê°ì§€"""
        info = {
            'device_type': 'cpu',
            'device_name': 'Unknown',
            'compute_capability': None,
            'memory': None
        }
        
        if torch.cuda.is_available():
            info['device_type'] = 'cuda'
            info['device_name'] = torch.cuda.get_device_name(0)
            info['compute_capability'] = torch.cuda.get_device_capability(0)
            info['memory'] = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            info['device_type'] = 'mps'
            info['device_name'] = 'Apple Silicon'
        
        print(f"ê°ì§€ëœ í•˜ë“œì›¨ì–´: {info}")
        return info
    
    def get_optimization_strategy(self):
        """í•˜ë“œì›¨ì–´ì— ë§ëŠ” ìµœì í™” ì „ëµ ì œê³µ"""
        
        strategies = []
        
        if self.device_info['device_type'] == 'cuda':
            # NVIDIA GPU ì „ëµ
            strategies.extend([
                "âœ… TensorRT ì‚¬ìš© ê°€ëŠ¥ - ìµœëŒ€ 10ë°° ì†ë„ í–¥ìƒ",
                "âœ… Mixed Precision (FP16) ì‚¬ìš© ê¶Œì¥",
                "âœ… CUDA Graphsë¡œ ì»¤ë„ ì‹¤í–‰ ìµœì í™”",
                "âœ… Flash Attention ì‚¬ìš© ê°€ëŠ¥",
                "âœ… Tensor Cores í™œìš© (Volta ì´ìƒ)"
            ])
            
            if self.device_info['compute_capability'][0] >= 8:
                strategies.append("âœ… INT8 Tensor Cores ì§€ì› (Ampere ì´ìƒ)")
            
        elif self.device_info['device_type'] == 'mps':
            # Apple Silicon ì „ëµ
            strategies.extend([
                "âœ… Core ML ë³€í™˜ìœ¼ë¡œ ìµœì í™”",
                "âœ… Metal Performance Shaders í™œìš©",
                "âœ… Apple Neural Engine ì‚¬ìš© ê°€ëŠ¥",
                "âœ… í†µí•© ë©”ëª¨ë¦¬ë¡œ ë°ì´í„° ì „ì†¡ ìµœì†Œí™”"
            ])
            
        else:
            # CPU ì „ëµ
            strategies.extend([
                "âœ… OpenVINO ë˜ëŠ” ONNX Runtime ì‚¬ìš©",
                "âœ… INT8 ì–‘ìí™”ë¡œ ì†ë„ í–¥ìƒ",
                "âœ… ë©€í‹°ìŠ¤ë ˆë”© í™œìš©",
                "âœ… SIMD ëª…ë ¹ì–´ ìµœì í™”",
                "âœ… ë©”ëª¨ë¦¬ ì •ë ¬ ìµœì í™”"
            ])
        
        return strategies
    
    def optimize_for_hardware(self, model):
        """í•˜ë“œì›¨ì–´ì— ë§ê²Œ ëª¨ë¸ ìµœì í™”"""
        
        optimized_model = model
        
        if self.device_info['device_type'] == 'cuda':
            # CUDA ìµœì í™”
            print("\nCUDA ìµœì í™” ì ìš© ì¤‘...")
            
            # Mixed Precision
            from torch.cuda.amp import autocast
            
            # ëª¨ë¸ì„ FP16ìœ¼ë¡œ ë³€í™˜
            optimized_model = model.half()
            
            # TensorRT ë³€í™˜ (ê°€ëŠ¥í•œ ê²½ìš°)
            try:
                import torch_tensorrt
                optimized_model = torch_tensorrt.compile(
                    model,
                    inputs=[torch.randn(1, 784).cuda()],
                    enabled_precisions={torch.float, torch.half}
                )
                print("  âœ… TensorRT ìµœì í™” ì™„ë£Œ")
            except:
                print("  âš ï¸ TensorRTë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        elif self.device_info['device_type'] == 'mps':
            # Apple Silicon ìµœì í™”
            print("\nApple Silicon ìµœì í™” ì ìš© ì¤‘...")
            
            # Core ML ë³€í™˜
            try:
                import coremltools as ct
                
                example_input = torch.randn(1, 784)
                traced_model = torch.jit.trace(model, example_input)
                
                coreml_model = ct.convert(
                    traced_model,
                    inputs=[ct.TensorType(shape=example_input.shape)]
                )
                
                print("  âœ… Core ML ë³€í™˜ ì™„ë£Œ")
                # ì‹¤ì œë¡œëŠ” coreml_modelì„ ì‚¬ìš©
                
            except:
                print("  âš ï¸ Core ML Toolsë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        else:
            # CPU ìµœì í™”
            print("\nCPU ìµœì í™” ì ìš© ì¤‘...")
            
            # ONNX ë³€í™˜
            try:
                import torch.onnx
                
                dummy_input = torch.randn(1, 784)
                torch.onnx.export(
                    model,
                    dummy_input,
                    "optimized_model.onnx",
                    export_params=True,
                    opset_version=11,
                    do_constant_folding=True,
                    input_names=['input'],
                    output_names=['output']
                )
                
                print("  âœ… ONNX ë³€í™˜ ì™„ë£Œ")
                
                # ONNX Runtimeìœ¼ë¡œ ì‹¤í–‰
                import onnxruntime as ort
                
                ort_session = ort.InferenceSession("optimized_model.onnx")
                print("  âœ… ONNX Runtime ì„¸ì…˜ ìƒì„± ì™„ë£Œ")
                
            except Exception as e:
                print(f"  âš ï¸ ONNX ìµœì í™” ì‹¤íŒ¨: {e}")
        
        return optimized_model

# í•˜ë“œì›¨ì–´ ìµœì í™” ì‹¤í–‰
hw_optimizer = HardwareOptimizer()
strategies = hw_optimizer.get_optimization_strategy()

print("\n=== ê¶Œì¥ ìµœì í™” ì „ëµ ===")
for strategy in strategies:
    print(strategy)
```

---

## 9ì¥: ìŒì„± ì¸í„°í˜ì´ìŠ¤ êµ¬ì¶• - AIì™€ ëŒ€í™”í•˜ê¸°

### 9.1 ìŒì„± ì¸í„°í˜ì´ìŠ¤ì˜ êµ¬ì„± ìš”ì†Œ

ìŒì„± AI ì‹œìŠ¤í…œì€ í¬ê²Œ ì„¸ ë¶€ë¶„ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤:
1. **STT (Speech-to-Text)**: ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
2. **NLU/ì²˜ë¦¬**: ì˜ë„ íŒŒì•… ë° ì‘ë‹µ ìƒì„±  
3. **TTS (Text-to-Speech)**: í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜

### 9.2 Whisperë¡œ ìŒì„± ì¸ì‹ êµ¬í˜„

```python
# advanced_speech_recognition.py - ê³ ê¸‰ ìŒì„± ì¸ì‹

import whisper
import numpy as np
import sounddevice as sd
import queue
import threading
import webrtcvad
import collections
import time
from typing import Optional, Callable
import matplotlib.pyplot as plt

class AdvancedWhisperSTT:
    """ê³ ê¸‰ ìŒì„± ì¸ì‹ ì‹œìŠ¤í…œ"""
    
    def __init__(self, model_size="base", language="ko"):
        print(f"ğŸ¤ Whisper {model_size} ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        
        self.model = whisper.load_model(model_size)
        self.language = language
        
        # VAD (Voice Activity Detection)
        self.vad = webrtcvad.Vad(2)  # ë¯¼ê°ë„ 0-3
        
        # ì˜¤ë””ì˜¤ ì„¤ì •
        self.sample_rate = 16000
        self.frame_duration = 30  # ms
        self.frame_size = int(self.sample_rate * self.frame_duration / 1000)
        
        # ë²„í¼
        self.audio_queue = queue.Queue()
        self.is_recording = False
        
        print("âœ… ìŒì„± ì¸ì‹ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
    
    def visualize_audio_processing(self):
        """ì˜¤ë””ì˜¤ ì²˜ë¦¬ ê³¼ì • ì‹œê°í™”"""
        
        fig, axes = plt.subplots(3, 2, figsize=(12, 10))
        
        # 1. ì›ë³¸ ì˜¤ë””ì˜¤ ì‹ í˜¸
        ax1 = axes[0, 0]
        t = np.linspace(0, 1, self.sample_rate)
        # ìŒì„± ì‹ í˜¸ ì‹œë®¬ë ˆì´ì…˜ (ì—¬ëŸ¬ ì£¼íŒŒìˆ˜ í˜¼í•©)
        signal = (0.5 * np.sin(2 * np.pi * 200 * t) + 
                 0.3 * np.sin(2 * np.pi * 400 * t) + 
                 0.2 * np.sin(2 * np.pi * 800 * t))
        noise = np.random.normal(0, 0.1, len(t))
        audio = signal + noise
        
        ax1.plot(t[:1000], audio[:1000], alpha=0.7)
        ax1.set_title('ì›ë³¸ ì˜¤ë””ì˜¤ ì‹ í˜¸', fontweight='bold')
        ax1.set_xlabel('ì‹œê°„ (ì´ˆ)')
        ax1.set_ylabel('ì§„í­')
        ax1.grid(True, alpha=0.3)
        
        # 2. ì£¼íŒŒìˆ˜ ìŠ¤í™íŠ¸ëŸ¼
        ax2 = axes[0, 1]
        freqs = np.fft.fftfreq(len(audio), 1/self.sample_rate)
        fft = np.abs(np.fft.fft(audio))
        
        ax2.plot(freqs[:len(freqs)//2], fft[:len(fft)//2])
        ax2.set_title('ì£¼íŒŒìˆ˜ ìŠ¤í™íŠ¸ëŸ¼', fontweight='bold')
        ax2.set_xlabel('ì£¼íŒŒìˆ˜ (Hz)')
        ax2.set_ylabel('ê°•ë„')
        ax2.set_xlim(0, 2000)
        ax2.grid(True, alpha=0.3)
        
        # 3. VAD (ìŒì„± í™œë™ ê°ì§€)
        ax3 = axes[1, 0]
        # VAD ì‹œë®¬ë ˆì´ì…˜
        vad_result = np.abs(audio) > 0.3
        vad_smoothed = np.convolve(vad_result, np.ones(100)/100, mode='same')
        
        ax3.plot(t[:5000], audio[:5000], alpha=0.5, label='ì˜¤ë””ì˜¤')
        ax3.fill_between(t[:5000], -1, 1, where=vad_smoothed[:5000] > 0.5,
                         alpha=0.3, color='red', label='ìŒì„± ê°ì§€')
        ax3.set_title('ìŒì„± í™œë™ ê°ì§€ (VAD)', fontweight='bold')
        ax3.set_xlabel('ì‹œê°„ (ì´ˆ)')
        ax3.set_ylabel('ì§„í­')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨
        ax4 = axes[1, 1]
        # ê°„ë‹¨í•œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ì‹œë®¬ë ˆì´ì…˜
        spec_data = np.random.rand(128, 100) * np.linspace(1, 0.1, 128).reshape(-1, 1)
        im = ax4.imshow(spec_data, aspect='auto', origin='lower', cmap='viridis')
        ax4.set_title('ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨', fontweight='bold')
        ax4.set_xlabel('ì‹œê°„ í”„ë ˆì„')
        ax4.set_ylabel('ë©œ ë¹ˆ')
        plt.colorbar(im, ax=ax4)
        
        # 5. ìŒì„± ì¸ì‹ ê³¼ì •
        ax5 = axes[2, 0]
        ax5.text(0.5, 0.8, 'ìŒì„± ì…ë ¥', ha='center', fontsize=12, 
                bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue"))
        ax5.text(0.5, 0.6, 'â†“', ha='center', fontsize=16)
        ax5.text(0.5, 0.4, 'íŠ¹ì§• ì¶”ì¶œ', ha='center', fontsize=12,
                bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgreen"))
        ax5.text(0.5, 0.2, 'â†“', ha='center', fontsize=16)
        ax5.text(0.5, 0.0, 'í…ìŠ¤íŠ¸ ë³€í™˜', ha='center', fontsize=12,
                bbox=dict(boxstyle="round,pad=0.3", facecolor="lightyellow"))
        ax5.set_xlim(0, 1)
        ax5.set_ylim(-0.2, 1)
        ax5.axis('off')
        ax5.set_title('Whisper ì²˜ë¦¬ ê³¼ì •', fontweight='bold')
        
        # 6. ì–¸ì–´ ëª¨ë¸ í™•ë¥ 
        ax6 = axes[2, 1]
        words = ['ì•ˆë…•í•˜ì„¸ìš”', 'ì•ˆë…•í•˜ì‹­ë‹ˆê¹Œ', 'ì•ˆë…•íˆ', 'ì•ˆë…•', 'ì•ˆë…„í•˜ì„¸ìš”']
        probs = [0.4, 0.3, 0.15, 0.1, 0.05]
        
        bars = ax6.bar(words, probs, color='orange')
        ax6.set_title('ì–¸ì–´ ëª¨ë¸ ì˜ˆì¸¡ í™•ë¥ ', fontweight='bold')
        ax6.set_ylabel('í™•ë¥ ')
        ax6.set_xticklabels(words, rotation=45, ha='right')
        
        for bar, prob in zip(bars, probs):
            ax6.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{prob:.2f}', ha='center')
        
        plt.tight_layout()
        plt.show()
    
    def voice_activity_detection(self, audio_frame):
        """ìŒì„± í™œë™ ê°ì§€"""
        # 16ë¹„íŠ¸ PCMìœ¼ë¡œ ë³€í™˜
        audio_int16 = (audio_frame * 32767).astype(np.int16)
        
        # VAD ì ìš©
        return self.vad.is_speech(audio_int16.tobytes(), self.sample_rate)
    
    def continuous_recognition(self, callback: Callable[[str], None]):
        """ì—°ì† ìŒì„± ì¸ì‹"""
        
        def audio_callback(indata, frames, time_info, status):
            """ì˜¤ë””ì˜¤ ì½œë°±"""
            if status:
                print(f"ì˜¤ë””ì˜¤ ìƒíƒœ: {status}")
            
            self.audio_queue.put(indata.copy())
        
        # ìŠ¤íŠ¸ë¦¼ ì‹œì‘
        stream = sd.InputStream(
            samplerate=self.sample_rate,
            channels=1,
            callback=audio_callback,
            blocksize=self.frame_size
        )
        
        with stream:
            print("\nğŸ¤ ìŒì„± ì¸ì‹ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            print("ë§ì”€í•˜ì„¸ìš”! (ì¢…ë£Œ: Ctrl+C)")
            
            # ë§ ë²„í¼ (3ì´ˆ ë¶„ëŸ‰)
            ring_buffer = collections.deque(maxlen=100)
            
            # ìŒì„± ê°ì§€ ìƒíƒœ
            triggered = False
            voiced_frames = []
            
            while self.is_recording:
                try:
                    frame = self.audio_queue.get(timeout=0.1)
                    
                    # VAD í™•ì¸
                    is_speech = self.voice_activity_detection(frame)
                    
                    if not triggered:
                        ring_buffer.append((frame, is_speech))
                        num_voiced = len([f for f, speech in ring_buffer if speech])
                        
                        # ìŒì„± ì‹œì‘ ê°ì§€ (0.3ì´ˆ ì´ìƒ)
                        if num_voiced > 0.3 * 100:
                            triggered = True
                            print("ğŸ”Š ìŒì„± ê°ì§€ë¨...")
                            
                            # ë§ ë²„í¼ì˜ ë‚´ìš©ì„ voiced_framesì— ì¶”ê°€
                            for f, s in ring_buffer:
                                voiced_frames.append(f)
                            ring_buffer.clear()
                    else:
                        # ìŒì„± ìˆ˜ì§‘
                        voiced_frames.append(frame)
                        ring_buffer.append((frame, is_speech))
                        
                        # ìŒì„± ì¢…ë£Œ ê°ì§€ (1ì´ˆ ì´ìƒ ì¡°ìš©)
                        num_unvoiced = len([f for f, speech in ring_buffer if not speech])
                        if num_unvoiced > 1.0 * 100:
                            print("ğŸ”‡ ìŒì„± ì¢…ë£Œ, ì¸ì‹ ì¤‘...")
                            
                            # ìŒì„± ì¸ì‹ ìˆ˜í–‰
                            audio_data = np.concatenate(voiced_frames)
                            text = self.recognize(audio_data)
                            
                            if text and callback:
                                callback(text)
                            
                            # ì´ˆê¸°í™”
                            triggered = False
                            voiced_frames = []
                            ring_buffer.clear()
                            
                except queue.Empty:
                    continue
                except KeyboardInterrupt:
                    break
    
    def recognize(self, audio_data: np.ndarray) -> str:
        """ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        # WhisperëŠ” float32 í•„ìš”
        audio_float32 = audio_data.astype(np.float32)
        
        # íŒ¨ë”© (ìµœì†Œ ê¸¸ì´ í™•ë³´)
        if len(audio_float32) < self.sample_rate:
            audio_float32 = np.pad(audio_float32, (0, self.sample_rate - len(audio_float32)))
        
        # ìŒì„± ì¸ì‹
        result = self.model.transcribe(
            audio_float32,
            language=self.language,
            fp16=torch.cuda.is_available()
        )
        
        return result["text"].strip()
    
    def start_recording(self):
        """ë…¹ìŒ ì‹œì‘"""
        self.is_recording = True
        
        def on_recognition(text):
            print(f"\nğŸ“ ì¸ì‹ëœ í…ìŠ¤íŠ¸: {text}")
        
        # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
        self.recognition_thread = threading.Thread(
            target=self.continuous_recognition,
            args=(on_recognition,)
        )
        self.recognition_thread.start()
    
    def stop_recording(self):
        """ë…¹ìŒ ì¤‘ì§€"""
        self.is_recording = False
        if hasattr(self, 'recognition_thread'):
            self.recognition_thread.join()

# ìŒì„± ì¸ì‹ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
def test_advanced_stt():
    stt = AdvancedWhisperSTT(model_size="base", language="ko")
    
    # ì˜¤ë””ì˜¤ ì²˜ë¦¬ ê³¼ì • ì‹œê°í™”
    stt.visualize_audio_processing()
    
    # ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ í…ŒìŠ¤íŠ¸
    print("\n=== ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ í…ŒìŠ¤íŠ¸ ===")
    
    try:
        stt.start_recording()
        time.sleep(30)  # 30ì´ˆ ë™ì•ˆ ë…¹ìŒ
    except KeyboardInterrupt:
        print("\nì¸ì‹ ì¤‘ë‹¨...")
    finally:
        stt.stop_recording()

# ì‹¤í–‰
# test_advanced_stt()
```

### 9.3 ê³ ê¸‰ TTS ì‹œìŠ¤í…œ êµ¬í˜„

```python
# advanced_tts.py - ê³ ê¸‰ ìŒì„± í•©ì„±

import torch
import numpy as np
from TTS.api import TTS
import pygame
import io
from typing import Dict, Optional
import json

class AdvancedTTS:
    """ê³ ê¸‰ ìŒì„± í•©ì„± ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        print("ğŸ”Š ê³ ê¸‰ TTS ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤
        self.available_models = {
            'korean_single': 'tts_models/ko/cv/vits',
            'multilingual': 'tts_models/multilingual/multi-dataset/your_tts',
            'emotional': 'tts_models/en/vctk/vits'  # ê°ì • í‘œí˜„ ê°€ëŠ¥
        }
        
        # ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ
        self.load_model('korean_single')
        
        # ìŒì„± ìŠ¤íƒ€ì¼ ì„¤ì •
        self.voice_styles = {
            'normal': {'speed': 1.0, 'pitch': 1.0, 'energy': 1.0},
            'happy': {'speed': 1.1, 'pitch': 1.1, 'energy': 1.2},
            'sad': {'speed': 0.9, 'pitch': 0.9, 'energy': 0.8},
            'angry': {'speed': 1.2, 'pitch': 0.8, 'energy': 1.3},
            'calm': {'speed': 0.85, 'pitch': 1.0, 'energy': 0.9}
        }
        
        pygame.mixer.init()
        print("âœ… TTS ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
    
    def load_model(self, model_type: str):
        """TTS ëª¨ë¸ ë¡œë“œ"""
        try:
            model_name = self.available_models[model_type]
            self.tts = TTS(model_name=model_name, progress_bar=True)
            
            if torch.cuda.is_available():
                self.tts = self.tts.cuda()
            
            self.current_model = model_type
            print(f"âœ… {model_type} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            # í´ë°±: gTTS ì‚¬ìš©
            self.use_gtts_fallback = True
    
    def synthesize_with_style(self, text: str, style: str = 'normal', 
                            speaker_id: Optional[int] = None) -> np.ndarray:
        """ìŠ¤íƒ€ì¼ì´ ì ìš©ëœ ìŒì„± í•©ì„±"""
        
        style_params = self.voice_styles.get(style, self.voice_styles['normal'])
        
        try:
            # TTS í•©ì„±
            wav = self.tts.tts(
                text=text,
                speaker=speaker_id,
                speed=style_params['speed']
            )
            
            # ì¶”ê°€ ì˜¤ë””ì˜¤ ì²˜ë¦¬ (í”¼ì¹˜, ì—ë„ˆì§€ ì¡°ì •)
            wav = self.apply_audio_effects(wav, style_params)
            
            return np.array(wav)
            
        except Exception as e:
            print(f"TTS ì˜¤ë¥˜: {e}")
            # í´ë°± ì²˜ë¦¬
            return self.fallback_tts(text)
    
    def apply_audio_effects(self, audio: np.ndarray, params: Dict) -> np.ndarray:
        """ì˜¤ë””ì˜¤ íš¨ê³¼ ì ìš©"""
        # í”¼ì¹˜ ì‹œí”„íŠ¸ (ê°„ë‹¨í•œ êµ¬í˜„)
        if params['pitch'] != 1.0:
            # ë¦¬ìƒ˜í”Œë§ì„ í†µí•œ í”¼ì¹˜ ë³€ê²½
            new_length = int(len(audio) / params['pitch'])
            indices = np.linspace(0, len(audio) - 1, new_length)
            audio = np.interp(indices, np.arange(len(audio)), audio)
        
        # ì—ë„ˆì§€ ì¡°ì • (ë³¼ë¥¨)
        audio = audio * params['energy']
        
        # í´ë¦¬í•‘ ë°©ì§€
        audio = np.clip(audio, -1.0, 1.0)
        
        return audio
    
    def create_ssml(self, text: str, style: str = 'normal') -> str:
        """SSML (Speech Synthesis Markup Language) ìƒì„±"""
        
        ssml_template = """
        <speak>
            <prosody rate="{rate}" pitch="{pitch}" volume="{volume}">
                {text}
            </prosody>
        </speak>
        """
        
        style_params = self.voice_styles[style]
        
        return ssml_template.format(
            rate=f"{style_params['speed']*100}%",
            pitch=f"{(style_params['pitch']-1)*50:+.0f}%",
            volume=f"{style_params['energy']*100}%",
            text=text
        )
    
    def speak_with_emotion(self, text: str, emotion: str = 'normal'):
        """ê°ì •ì´ ë‹´ê¸´ ìŒì„± ì¶œë ¥"""
        
        print(f"ğŸ­ {emotion} ê°ì •ìœ¼ë¡œ ë§í•˜ê¸°: {text}")
        
        # ìŒì„± í•©ì„±
        audio = self.synthesize_with_style(text, emotion)
        
        # ì¬ìƒ
        self.play_audio(audio)
    
    def play_audio(self, audio: np.ndarray, sample_rate: int = 22050):
        """ì˜¤ë””ì˜¤ ì¬ìƒ"""
        # int16ìœ¼ë¡œ ë³€í™˜
        audio_int16 = (audio * 32767).astype(np.int16)
        
        # pygameìš© ì‚¬ìš´ë“œ ê°ì²´ ìƒì„±
        sound = pygame.sndarray.make_sound(audio_int16)
        
        # ì¬ìƒ
        sound.play()
        
        # ì¬ìƒ ì™„ë£Œ ëŒ€ê¸°
        while pygame.mixer.get_busy():
            pygame.time.Clock().tick(10)
    
    def create_audio_book(self, chapters: List[Dict[str, str]], output_file: str):
        """ì˜¤ë””ì˜¤ë¶ ìƒì„±"""
        
        print(f"ğŸ“š ì˜¤ë””ì˜¤ë¶ ìƒì„± ì¤‘: {output_file}")
        
        all_audio = []
        
        for i, chapter in enumerate(chapters):
            title = chapter.get('title', f'Chapter {i+1}')
            content = chapter.get('content', '')
            style = chapter.get('style', 'normal')
            
            print(f"  ğŸ“– {title} ì²˜ë¦¬ ì¤‘...")
            
            # ì œëª© ìŒì„±
            title_audio = self.synthesize_with_style(
                f"ì œ {i+1}ì¥. {title}", 
                'normal'
            )
            
            # ë‚´ìš© ìŒì„±
            content_audio = self.synthesize_with_style(content, style)
            
            # ì±•í„° ì‚¬ì´ íœ´ì§€
            pause = np.zeros(int(22050 * 2))  # 2ì´ˆ íœ´ì§€
            
            all_audio.extend([title_audio, pause, content_audio, pause])
        
        # ì „ì²´ ì˜¤ë””ì˜¤ ê²°í•©
        complete_audio = np.concatenate(all_audio)
        
        # íŒŒì¼ë¡œ ì €ì¥
        import soundfile as sf
        sf.write(output_file, complete_audio, 22050)
        
        print(f"âœ… ì˜¤ë””ì˜¤ë¶ ìƒì„± ì™„ë£Œ: {output_file}")
    
    def voice_cloning(self, reference_audio: str, text: str):
        """ìŒì„± ë³µì œ (ê³ ê¸‰ ê¸°ëŠ¥)"""
        
        if self.current_model != 'multilingual':
            print("ìŒì„± ë³µì œë¥¼ ìœ„í•´ multilingual ëª¨ë¸ë¡œ ì „í™˜í•©ë‹ˆë‹¤...")
            self.load_model('multilingual')
        
        try:
            # ì°¸ì¡° ìŒì„±ìœ¼ë¡œ TTS
            wav = self.tts.tts(
                text=text,
                speaker_wav=reference_audio,
                language="ko"
            )
            
            return np.array(wav)
            
        except Exception as e:
            print(f"ìŒì„± ë³µì œ ì‹¤íŒ¨: {e}")
            return self.fallback_tts(text)
    
    def fallback_tts(self, text: str) -> np.ndarray:
        """í´ë°± TTS (gTTS ì‚¬ìš©)"""
        from gtts import gTTS
        import tempfile
        
        tts = gTTS(text=text, lang='ko')
        
        with tempfile.NamedTemporaryFile(suffix='.mp3', delete=False) as tmp_file:
            tts.save(tmp_file.name)
            
            # MP3ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
            import librosa
            audio, sr = librosa.load(tmp_file.name, sr=22050)
            
            os.unlink(tmp_file.name)
            
        return audio

# TTS ë°ëª¨
def demonstrate_advanced_tts():
    """ê³ ê¸‰ TTS ê¸°ëŠ¥ ì‹œì—°"""
    
    tts = AdvancedTTS()
    
    # 1. ë‹¤ì–‘í•œ ê°ì • í‘œí˜„
    print("\n=== ê°ì • í‘œí˜„ ë°ëª¨ ===")
    
    test_text = "ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”."
    
    emotions = ['normal', 'happy', 'sad', 'angry', 'calm']
    
    for emotion in emotions:
        print(f"\n{emotion} ìŠ¤íƒ€ì¼:")
        tts.speak_with_emotion(test_text, emotion)
        time.sleep(2)
    
    # 2. ì˜¤ë””ì˜¤ë¶ ìƒì„±
    print("\n=== ì˜¤ë””ì˜¤ë¶ ìƒì„± ë°ëª¨ ===")
    
    chapters = [
        {
            'title': 'AIì˜ ì‹œì‘',
            'content': 'ì¸ê³µì§€ëŠ¥ì€ ì¸ê°„ì˜ ì§€ëŠ¥ì„ ëª¨ë°©í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.',
            'style': 'normal'
        },
        {
            'title': 'ë¯¸ë˜ì˜ ì „ë§',
            'content': 'AIëŠ” ìš°ë¦¬ì˜ ë¯¸ë˜ë¥¼ ë°”ê¿€ ê²ƒì…ë‹ˆë‹¤.',
            'style': 'calm'
        }
    ]
    
    tts.create_audio_book(chapters, 'ai_audiobook.wav')
    
    # 3. SSML í…ŒìŠ¤íŠ¸
    print("\n=== SSML í…ŒìŠ¤íŠ¸ ===")
    
    ssml = tts.create_ssml("ì´ê²ƒì€ SSML í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤.", 'happy')
    print(f"ìƒì„±ëœ SSML:\n{ssml}")

# ì‹¤í–‰
# demonstrate_advanced_tts()
```

---

## 10ì¥: ìµœì¢… í”„ë¡œì íŠ¸ - ìŒì„± ëŒ€í™”í˜• AI ë¹„ì„œ

### 10.1 í”„ë¡œì íŠ¸ ê°œìš”: JARVISë¥¼ ë§Œë“¤ì–´ë³´ì!

ìš°ë¦¬ì˜ ìµœì¢… í”„ë¡œì íŠ¸ëŠ” ì•„ì´ì–¸ë§¨ì˜ JARVISê°™ì€ AI ë¹„ì„œë¥¼ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤:
- ğŸ¤ ìì—°ìŠ¤ëŸ¬ìš´ ìŒì„± ëŒ€í™”
- ğŸ‘€ ì¹´ë©”ë¼ë¡œ ìƒí™© ì¸ì‹
- ğŸ§  ì§€ëŠ¥ì ì¸ ì‘ë‹µ ìƒì„±
- ğŸ  ìŠ¤ë§ˆíŠ¸í™ˆ ì œì–´ (ì‹œë®¬ë ˆì´ì…˜)
- ğŸ“Š ì •ë³´ ê²€ìƒ‰ ë° ë¶„ì„

### 10.2 ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```python
# jarvis_system.py - JARVIS ìŠ¤íƒ€ì¼ AI ë¹„ì„œ

import asyncio
import torch
import numpy as np
from typing import Dict, List, Optional, Any
import json
import time
from datetime import datetime
import cv2
from collections import deque
import threading
import queue

class JARVIS:
    """Just A Rather Very Intelligent System"""
    
    def __init__(self, user_name: str = "User"):
        print("""
        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        â•‘         J.A.R.V.I.S ì‹œìŠ¤í…œ ì‹œì‘          â•‘
        â•‘                                          â•‘
        â•‘    Just A Rather Very Intelligent System  â•‘
        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """)
        
        self.user_name = user_name
        self.initialize_components()
        self.boot_sequence()
    
    def initialize_components(self):
        """ì‹œìŠ¤í…œ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        print("\nì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        
        # 1. ìŒì„± ì‹œìŠ¤í…œ
        print("  [1/6] ìŒì„± ì¸ì‹ ì‹œìŠ¤í…œ ì´ˆê¸°í™”...")
        self.stt = AdvancedWhisperSTT(model_size="base", language="ko")
        
        print("  [2/6] ìŒì„± í•©ì„± ì‹œìŠ¤í…œ ì´ˆê¸°í™”...")
        self.tts = AdvancedTTS()
        
        # 2. ë¹„ì „ ì‹œìŠ¤í…œ
        print("  [3/6] ì»´í“¨í„° ë¹„ì „ ì‹œìŠ¤í…œ ì´ˆê¸°í™”...")
        self.vision_system = VisionSystem()
        
        # 3. ìì—°ì–´ ì²˜ë¦¬
        print("  [4/6] ìì—°ì–´ ì²˜ë¦¬ ì—”ì§„ ì´ˆê¸°í™”...")
        self.nlp_engine = NLPEngine()
        
        # 4. ì§€ì‹ ë² ì´ìŠ¤
        print("  [5/6] ì§€ì‹ ë² ì´ìŠ¤ ë¡œë”©...")
        self.knowledge_base = KnowledgeBase()
        
        # 5. ì‘ì—… ì‹¤í–‰ê¸°
        print("  [6/6] ì‘ì—… ì‹¤í–‰ ì—”ì§„ ì´ˆê¸°í™”...")
        self.task_executor = TaskExecutor()
        
        # ì‹œìŠ¤í…œ ìƒíƒœ
        self.system_state = {
            'mode': 'standby',
            'last_interaction': None,
            'context': deque(maxlen=10),
            'active_tasks': []
        }
        
        print("\nâœ… ëª¨ë“  ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
    
    def boot_sequence(self):
        """ë¶€íŒ… ì‹œí€€ìŠ¤"""
        self.speak(f"ì•ˆë…•í•˜ì„¸ìš” {self.user_name}ë‹˜. JARVIS ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
        self.speak("ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")
        self.system_state['mode'] = 'active'
    
    def speak(self, text: str, style: str = 'normal'):
        """JARVIS ìŒì„± ì¶œë ¥"""
        print(f"ğŸ¤– JARVIS: {text}")
        self.tts.speak_with_emotion(text, style)
    
    async def listen(self) -> Optional[str]:
        """ì‚¬ìš©ì ìŒì„± ì…ë ¥ ëŒ€ê¸°"""
        print("\nğŸ‘‚ ë“£ê³  ìˆìŠµë‹ˆë‹¤...")
        
        # ìŒì„± ì¸ì‹ (ë¹„ë™ê¸°)
        loop = asyncio.get_event_loop()
        text = await loop.run_in_executor(None, self._listen_sync)
        
        if text:
            print(f"ğŸ‘¤ {self.user_name}: {text}")
            self.system_state['last_interaction'] = time.time()
            self.system_state['context'].append({
                'speaker': 'user',
                'text': text,
                'time': datetime.now()
            })
        
        return text
    
    def _listen_sync(self) -> Optional[str]:
        """ë™ê¸°ì‹ ìŒì„± ì¸ì‹"""
        audio = self.stt.record_audio(duration=5)
        return self.stt.transcribe(audio)
    
    async def process_command(self, command: str) -> Dict[str, Any]:
        """ëª…ë ¹ ì²˜ë¦¬"""
        
        # ì˜ë„ ë¶„ì„
        intent = await self.nlp_engine.analyze_intent(command)
        
        # ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
        self.system_state['context'].append({
            'speaker': 'jarvis',
            'intent': intent,
            'time': datetime.now()
        })
        
        # ì˜ë„ì— ë”°ë¥¸ ì²˜ë¦¬
        response = await self.execute_intent(intent)
        
        return response
    
    async def execute_intent(self, intent: Dict[str, Any]) -> Dict[str, Any]:
        """ì˜ë„ ì‹¤í–‰"""
        
        intent_type = intent.get('type', 'unknown')
        entities = intent.get('entities', {})
        
        if intent_type == 'greeting':
            return await self.handle_greeting()
        
        elif intent_type == 'weather':
            return await self.handle_weather(entities.get('location'))
        
        elif intent_type == 'vision_analysis':
            return await self.handle_vision_analysis(entities.get('query'))
        
        elif intent_type == 'smart_home':
            return await self.handle_smart_home(entities)
        
        elif intent_type == 'information':
            return await self.handle_information_query(entities.get('query'))
        
        elif intent_type == 'task':
            return await self.handle_task(entities)
        
        else:
            return {
                'success': False,
                'message': "ì£„ì†¡í•©ë‹ˆë‹¤. ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ë§ì”€í•´ì£¼ì„¸ìš”."
            }
    
    async def handle_greeting(self) -> Dict[str, Any]:
        """ì¸ì‚¬ ì²˜ë¦¬"""
        current_hour = datetime.now().hour
        
        if current_hour < 12:
            greeting = "ì¢‹ì€ ì•„ì¹¨ì…ë‹ˆë‹¤"
        elif current_hour < 18:
            greeting = "ì¢‹ì€ ì˜¤í›„ì…ë‹ˆë‹¤"
        else:
            greeting = "ì¢‹ì€ ì €ë…ì…ë‹ˆë‹¤"
        
        self.speak(f"{greeting}, {self.user_name}ë‹˜. ì˜¤ëŠ˜ë„ ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”.")
        
        return {'success': True, 'action': 'greeting'}
    
    async def handle_vision_analysis(self, query: Optional[str]) -> Dict[str, Any]:
        """ë¹„ì „ ë¶„ì„ ì²˜ë¦¬"""
        self.speak("ì¹´ë©”ë¼ë¥¼ í†µí•´ í™•ì¸í•˜ê² ìŠµë‹ˆë‹¤.")
        
        # ì´ë¯¸ì§€ ìº¡ì²˜
        image = self.vision_system.capture_image()
        
        # ë¶„ì„
        analysis = await self.vision_system.analyze_scene(image, query)
        
        # ê²°ê³¼ ì„¤ëª…
        self.speak(analysis['description'])
        
        return {
            'success': True,
            'action': 'vision_analysis',
            'result': analysis
        }
    
    async def handle_smart_home(self, entities: Dict) -> Dict[str, Any]:
        """ìŠ¤ë§ˆíŠ¸í™ˆ ì œì–´"""
        device = entities.get('device')
        action = entities.get('action')
        
        if not device or not action:
            self.speak("ì–´ë–¤ ê¸°ê¸°ë¥¼ ì–´ë–»ê²Œ ì œì–´í• ê¹Œìš”?")
            return {'success': False}
        
        # ì‹œë®¬ë ˆì´ì…˜
        result = await self.task_executor.control_device(device, action)
        
        if result['success']:
            self.speak(f"{device}ë¥¼ {action}í–ˆìŠµë‹ˆë‹¤.")
        else:
            self.speak(f"ì£„ì†¡í•©ë‹ˆë‹¤. {device} ì œì–´ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        return result
    
    async def run(self):
        """ë©”ì¸ ì‹¤í–‰ ë£¨í”„"""
        self.speak("ì‹œìŠ¤í…œì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        while self.system_state['mode'] == 'active':
            try:
                # ìŒì„± ì…ë ¥ ëŒ€ê¸°
                command = await self.listen()
                
                if not command:
                    continue
                
                # ì¢…ë£Œ ëª…ë ¹ í™•ì¸
                if any(word in command for word in ['ì¢…ë£Œ', 'ì˜ì', 'êµ¿ë°”ì´']):
                    await self.shutdown()
                    break
                
                # ëª…ë ¹ ì²˜ë¦¬
                response = await self.process_command(command)
                
                # ì£¼ê¸°ì  ìƒíƒœ ì²´í¬
                await self.periodic_check()
                
            except KeyboardInterrupt:
                await self.shutdown()
                break
            except Exception as e:
                print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
                self.speak("ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
    
    async def periodic_check(self):
        """ì£¼ê¸°ì  ìƒíƒœ í™•ì¸"""
        # ì¥ì‹œê°„ ë¯¸ì‚¬ìš© ì‹œ ì ˆì „ ëª¨ë“œ
        if self.system_state['last_interaction']:
            idle_time = time.time() - self.system_state['last_interaction']
            if idle_time > 300:  # 5ë¶„
                self.system_state['mode'] = 'standby'
                self.speak("ì ˆì „ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
    
    async def shutdown(self):
        """ì‹œìŠ¤í…œ ì¢…ë£Œ"""
        self.speak(f"ì•ˆë…•íˆ ê³„ì„¸ìš”, {self.user_name}ë‹˜.")
        self.system_state['mode'] = 'shutdown'
        
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        cv2.destroyAllWindows()


class VisionSystem:
    """JARVIS ë¹„ì „ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.yolo = YOLO('yolo11m.pt')
        self.vqa_model = self.load_vqa_model()
        self.cap = None
    
    def load_vqa_model(self):
        """VQA ëª¨ë¸ ë¡œë“œ"""
        from transformers import Blip2Processor, Blip2ForConditionalGeneration
        
        processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")
        model = Blip2ForConditionalGeneration.from_pretrained(
            "Salesforce/blip2-opt-2.7b",
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
        )
        
        return {'processor': processor, 'model': model}
    
    def capture_image(self) -> np.ndarray:
        """ì¹´ë©”ë¼ì—ì„œ ì´ë¯¸ì§€ ìº¡ì²˜"""
        if self.cap is None:
            self.cap = cv2.VideoCapture(0)
        
        ret, frame = self.cap.read()
        return frame if ret else np.zeros((480, 640, 3), dtype=np.uint8)
    
    async def analyze_scene(self, image: np.ndarray, query: Optional[str] = None) -> Dict:
        """ì¥ë©´ ë¶„ì„"""
        results = {
            'objects': [],
            'description': "",
            'query_answer': ""
        }
        
        # YOLO ê°ì²´ íƒì§€
        yolo_results = self.yolo(image)[0]
        
        for box in yolo_results.boxes:
            results['objects'].append({
                'class': self.yolo.names[int(box.cls)],
                'confidence': float(box.conf),
                'bbox': box.xyxy[0].tolist()
            })
        
        # ì¥ë©´ ì„¤ëª… ìƒì„±
        if results['objects']:
            object_counts = {}
            for obj in results['objects']:
                cls = obj['class']
                object_counts[cls] = object_counts.get(cls, 0) + 1
            
            description_parts = []
            for cls, count in object_counts.items():
                if count == 1:
                    description_parts.append(f"{cls} 1ê°œ")
                else:
                    description_parts.append(f"{cls} {count}ê°œ")
            
            results['description'] = f"í™”ë©´ì— {', '.join(description_parts)}ê°€ ë³´ì…ë‹ˆë‹¤."
        else:
            results['description'] = "íŠ¹ë³„í•œ ê°ì²´ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        # íŠ¹ì • ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€
        if query and self.vqa_model:
            pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
            
            inputs = self.vqa_model['processor'](
                images=pil_image,
                text=query,
                return_tensors="pt"
            )
            
            with torch.no_grad():
                generated_ids = self.vqa_model['model'].generate(**inputs)
                answer = self.vqa_model['processor'].decode(
                    generated_ids[0],
                    skip_special_tokens=True
                )
            
            results['query_answer'] = answer
        
        return results


class NLPEngine:
    """ìì—°ì–´ ì²˜ë¦¬ ì—”ì§„"""
    
    def __init__(self):
        self.intent_patterns = {
            'greeting': ['ì•ˆë…•', 'í•˜ì´', 'hello', 'ë°˜ê°€ì›Œ'],
            'weather': ['ë‚ ì”¨', 'ê¸°ì˜¨', 'ë¹„', 'ë§‘ìŒ'],
            'vision_analysis': ['ë³´ì—¬ì¤˜', 'ë­ê°€ ìˆ', 'ì°¾ì•„ì¤˜', 'ì–´ë””'],
            'smart_home': ['ì¼œì¤˜', 'êº¼ì¤˜', 'ì¡°ì ˆ', 'ì˜¨ë„', 'ë¶ˆë¹›'],
            'information': ['ì•Œë ¤ì¤˜', 'ë­ì•¼', 'ê²€ìƒ‰', 'ì°¾ì•„'],
            'task': ['ì¼ì •', 'ì•ŒëŒ', 'ë©”ëª¨', 'ë¦¬ë§ˆì¸ë”']
        }
    
    async def analyze_intent(self, text: str) -> Dict[str, Any]:
        """ì˜ë„ ë¶„ì„"""
        text_lower = text.lower()
        
        # íŒ¨í„´ ë§¤ì¹­
        for intent, patterns in self.intent_patterns.items():
            if any(pattern in text_lower for pattern in patterns):
                return {
                    'type': intent,
                    'confidence': 0.8,
                    'entities': self.extract_entities(text, intent)
                }
        
        return {
            'type': 'unknown',
            'confidence': 0.0,
            'entities': {}
        }
    
    def extract_entities(self, text: str, intent: str) -> Dict:
        """ì—”í‹°í‹° ì¶”ì¶œ"""
        entities = {}
        
        if intent == 'smart_home':
            # ë””ë°”ì´ìŠ¤ ì¶”ì¶œ
            devices = ['ì „ë“±', 'ì—ì–´ì»¨', 'íˆí„°', 'TV', 'ì»¤íŠ¼']
            for device in devices:
                if device in text:
                    entities['device'] = device
            
            # ì•¡ì…˜ ì¶”ì¶œ
            if 'ì¼œ' in text:
                entities['action'] = 'ì¼œê¸°'
            elif 'êº¼' in text:
                entities['action'] = 'ë„ê¸°'
        
        elif intent == 'weather':
            # ìœ„ì¹˜ ì¶”ì¶œ (ê°„ë‹¨í•œ ì˜ˆì‹œ)
            if 'ë‚´ì¼' in text:
                entities['time'] = 'tomorrow'
            else:
                entities['time'] = 'today'
        
        return entities


class KnowledgeBase:
    """ì§€ì‹ ë² ì´ìŠ¤"""
    
    def __init__(self):
        self.facts = {
            'user_preferences': {},
            'device_states': {
                'ì „ë“±': False,
                'ì—ì–´ì»¨': False,
                'TV': False
            },
            'schedules': [],
            'reminders': []
        }
    
    def get_fact(self, category: str, key: str = None):
        """ì‚¬ì‹¤ ì¡°íšŒ"""
        if key:
            return self.facts.get(category, {}).get(key)
        return self.facts.get(category)
    
    def update_fact(self, category: str, key: str, value: Any):
        """ì‚¬ì‹¤ ì—…ë°ì´íŠ¸"""
        if category not in self.facts:
            self.facts[category] = {}
        
        if isinstance(self.facts[category], dict):
            self.facts[category][key] = value
        elif isinstance(self.facts[category], list):
            self.facts[category].append({key: value})


class TaskExecutor:
    """ì‘ì—… ì‹¤í–‰ê¸°"""
    
    def __init__(self):
        self.active_tasks = []
    
    async def control_device(self, device: str, action: str) -> Dict:
        """ë””ë°”ì´ìŠ¤ ì œì–´ (ì‹œë®¬ë ˆì´ì…˜)"""
        print(f"ğŸ  ìŠ¤ë§ˆíŠ¸í™ˆ: {device} {action}")
        
        # ì‹¤ì œë¡œëŠ” IoT API í˜¸ì¶œ
        await asyncio.sleep(0.5)  # ì‹œë®¬ë ˆì´ì…˜ ë”œë ˆì´
        
        return {
            'success': True,
            'device': device,
            'action': action,
            'timestamp': datetime.now()
        }
    
    async def set_reminder(self, text: str, time: datetime) -> Dict:
        """ë¦¬ë§ˆì¸ë” ì„¤ì •"""
        reminder = {
            'id': len(self.active_tasks),
            'text': text,
            'time': time,
            'active': True
        }
        
        self.active_tasks.append(reminder)
        
        return {
            'success': True,
            'reminder': reminder
        }


# JARVIS UI
def create_jarvis_ui():
    """JARVIS ì›¹ UI"""
    import gradio as gr
    
    jarvis = None
    
    def initialize_jarvis(user_name):
        """JARVIS ì´ˆê¸°í™”"""
        global jarvis
        jarvis = JARVIS(user_name)
        return f"JARVISê°€ {user_name}ë‹˜ì„ ìœ„í•´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤."
    
    def process_voice_command(audio):
        """ìŒì„± ëª…ë ¹ ì²˜ë¦¬"""
        if jarvis is None:
            return "ë¨¼ì € JARVISë¥¼ ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”.", None
        
        # ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        text = jarvis.stt.transcribe(audio)
        
        # ëª…ë ¹ ì²˜ë¦¬ (ë™ê¸° ë˜í¼)
        response = asyncio.run(jarvis.process_command(text))
        
        return text, response.get('message', 'ì²˜ë¦¬ ì™„ë£Œ')
    
    def capture_and_analyze():
        """ì¹´ë©”ë¼ ìº¡ì²˜ ë° ë¶„ì„"""
        if jarvis is None:
            return None, "ë¨¼ì € JARVISë¥¼ ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”."
        
        image = jarvis.vision_system.capture_image()
        analysis = asyncio.run(
            jarvis.vision_system.analyze_scene(image)
        )
        
        return image, analysis['description']
    
    # Gradio ì¸í„°í˜ì´ìŠ¤
    with gr.Blocks(title="J.A.R.V.I.S", theme=gr.themes.Soft()) as interface:
        gr.Markdown("""
        # ğŸ¤– J.A.R.V.I.S - AI ë¹„ì„œ ì‹œìŠ¤í…œ
        
        Just A Rather Very Intelligent System
        """)
        
        with gr.Tab("ğŸš€ ì´ˆê¸°í™”"):
            user_name_input = gr.Textbox(
                label="ì‚¬ìš©ì ì´ë¦„",
                placeholder="ë‹¹ì‹ ì˜ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”",
                value="User"
            )
            init_btn = gr.Button("JARVIS ì‹œì‘", variant="primary")
            init_output = gr.Textbox(label="ìƒíƒœ")
            
            init_btn.click(
                fn=initialize_jarvis,
                inputs=user_name_input,
                outputs=init_output
            )
        
        with gr.Tab("ğŸ¤ ìŒì„± ëª…ë ¹"):
            with gr.Row():
                audio_input = gr.Audio(
                    source="microphone",
                    type="numpy",
                    label="ìŒì„± ì…ë ¥"
                )
                
                with gr.Column():
                    recognized_text = gr.Textbox(label="ì¸ì‹ëœ í…ìŠ¤íŠ¸")
                    response_text = gr.Textbox(label="JARVIS ì‘ë‹µ")
            
            process_btn = gr.Button("ëª…ë ¹ ì‹¤í–‰")
            process_btn.click(
                fn=process_voice_command,
                inputs=audio_input,
                outputs=[recognized_text, response_text]
            )
        
        with gr.Tab("ğŸ‘€ ë¹„ì „ ë¶„ì„"):
            with gr.Row():
                captured_image = gr.Image(label="ìº¡ì²˜ëœ ì´ë¯¸ì§€")
                analysis_result = gr.Textbox(label="ë¶„ì„ ê²°ê³¼", lines=5)
            
            capture_btn = gr.Button("ìº¡ì²˜ ë° ë¶„ì„")
            capture_btn.click(
                fn=capture_and_analyze,
                outputs=[captured_image, analysis_result]
            )
        
        with gr.Tab("ğŸ  ìŠ¤ë§ˆíŠ¸í™ˆ"):
            gr.Markdown("""
            ### ë””ë°”ì´ìŠ¤ ìƒíƒœ
            
            - ğŸ”¦ ì „ë“±: OFF
            - â„ï¸ ì—ì–´ì»¨: OFF
            - ğŸ“º TV: OFF
            - ğŸŒ¡ï¸ ì˜¨ë„: 22Â°C
            - ğŸ’¡ ì¡°ë„: ì ì •
            """)
            
            device_select = gr.Dropdown(
                choices=["ì „ë“±", "ì—ì–´ì»¨", "TV"],
                label="ë””ë°”ì´ìŠ¤ ì„ íƒ"
            )
            action_select = gr.Radio(
                choices=["ì¼œê¸°", "ë„ê¸°"],
                label="ë™ì‘"
            )
            control_btn = gr.Button("ì œì–´")
        
        with gr.Tab("ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ"):
            gr.Markdown("""
            ### ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤
            - CPU: 25%
            - ë©”ëª¨ë¦¬: 4.2GB / 16GB
            - GPU: 15%
            
            ### í™œì„± ëª¨ë“ˆ
            - âœ… ìŒì„± ì¸ì‹
            - âœ… ìŒì„± í•©ì„±
            - âœ… ì»´í“¨í„° ë¹„ì „
            - âœ… ìì—°ì–´ ì²˜ë¦¬
            """)
    
    return interface

# ì‹¤í–‰
def run_jarvis():
    """JARVIS ì‹¤í–‰"""
    
    print("""
    ì„ íƒí•˜ì„¸ìš”:
    1. ì½˜ì†” ëª¨ë“œ
    2. ì›¹ UI ëª¨ë“œ
    """)
    
    choice = input("ì„ íƒ (1-2): ")
    
    if choice == "1":
        # ì½˜ì†” ëª¨ë“œ
        user_name = input("ë‹¹ì‹ ì˜ ì´ë¦„ì€? ")
        jarvis = JARVIS(user_name)
        
        # ë¹„ë™ê¸° ì‹¤í–‰
        asyncio.run(jarvis.run())
        
    elif choice == "2":
        # ì›¹ UI ëª¨ë“œ
        ui = create_jarvis_ui()
        ui.launch(share=True)

if __name__ == "__main__":
    run_jarvis()
```

### 10.3 ê³ ê¸‰ ê¸°ëŠ¥ ì¶”ê°€

```python
# jarvis_advanced_features.py - JARVIS ê³ ê¸‰ ê¸°ëŠ¥

import schedule
import requests
from bs4 import BeautifulSoup
import wikipedia
import pyttsx3
from googletrans import Translator

class AdvancedJARVIS(JARVIS):
    """ê³ ê¸‰ ê¸°ëŠ¥ì´ ì¶”ê°€ëœ JARVIS"""
    
    def __init__(self, user_name: str = "User"):
        super().__init__(user_name)
        
        # ì¶”ê°€ ì»´í¬ë„ŒíŠ¸
        self.translator = Translator()
        self.scheduler = schedule
        self.news_api_key = "YOUR_NEWS_API_KEY"
        
    async def handle_information_query(self, query: str) -> Dict[str, Any]:
        """ì •ë³´ ê²€ìƒ‰ ì²˜ë¦¬"""
        
        # Wikipedia ê²€ìƒ‰
        try:
            wikipedia.set_lang("ko")
            summary = wikipedia.summary(query, sentences=3)
            
            self.speak(f"{query}ì— ëŒ€í•´ ì°¾ì€ ì •ë³´ì…ë‹ˆë‹¤.")
            self.speak(summary)
            
            return {
                'success': True,
                'source': 'wikipedia',
                'content': summary
            }
            
        except wikipedia.exceptions.PageError:
            self.speak(f"ì£„ì†¡í•©ë‹ˆë‹¤. {query}ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {'success': False}
    
    async def get_news_briefing(self) -> Dict[str, Any]:
        """ë‰´ìŠ¤ ë¸Œë¦¬í•‘"""
        
        self.speak("ì˜¤ëŠ˜ì˜ ì£¼ìš” ë‰´ìŠ¤ë¥¼ ì „í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.")
        
        # ë‰´ìŠ¤ API í˜¸ì¶œ (ì˜ˆì‹œ)
        url = f"https://newsapi.org/v2/top-headlines?country=kr&apiKey={self.news_api_key}"
        
        try:
            response = requests.get(url)
            news_data = response.json()
            
            articles = news_data.get('articles', [])[:5]  # ìƒìœ„ 5ê°œ
            
            for i, article in enumerate(articles, 1):
                title = article.get('title', '')
                self.speak(f"{i}ë²ˆ ë‰´ìŠ¤: {title}")
                await asyncio.sleep(1)
            
            return {
                'success': True,
                'articles': articles
            }
            
        except Exception as e:
            self.speak("ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return {'success': False, 'error': str(e)}
    
    async def translate_text(self, text: str, target_lang: str = 'en') -> str:
        """í…ìŠ¤íŠ¸ ë²ˆì—­"""
        
        try:
            result = self.translator.translate(text, dest=target_lang)
            translated = result.text
            
            self.speak(f"ë²ˆì—­ ê²°ê³¼: {translated}")
            
            return translated
            
        except Exception as e:
            self.speak("ë²ˆì—­ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return ""
    
    def schedule_task(self, task_name: str, time_str: str, function):
        """ì‘ì—… ìŠ¤ì¼€ì¤„ë§"""
        
        self.scheduler.every().day.at(time_str).do(function)
        
        self.speak(f"{task_name} ì‘ì—…ì„ {time_str}ì— ì˜ˆì•½í–ˆìŠµë‹ˆë‹¤.")
    
    async def health_monitoring(self) -> Dict[str, Any]:
        """ê±´ê°• ëª¨ë‹ˆí„°ë§ (ì‹œë®¬ë ˆì´ì…˜)"""
        
        # ì‹¤ì œë¡œëŠ” ì›¨ì–´ëŸ¬ë¸” ë””ë°”ì´ìŠ¤ API ì—°ë™
        health_data = {
            'heart_rate': np.random.randint(60, 100),
            'steps': np.random.randint(5000, 10000),
            'sleep_hours': np.random.uniform(6, 9),
            'stress_level': np.random.choice(['ë‚®ìŒ', 'ë³´í†µ', 'ë†’ìŒ'])
        }
        
        self.speak(f"""
        ì˜¤ëŠ˜ì˜ ê±´ê°• ìƒíƒœì…ë‹ˆë‹¤.
        ì‹¬ë°•ìˆ˜: ë¶„ë‹¹ {health_data['heart_rate']}íšŒ
        ê±¸ìŒìˆ˜: {health_data['steps']}ë³´
        ìˆ˜ë©´ ì‹œê°„: {health_data['sleep_hours']:.1f}ì‹œê°„
        ìŠ¤íŠ¸ë ˆìŠ¤: {health_data['stress_level']}
        """)
        
        # ê±´ê°• ì¡°ì–¸
        if health_data['steps'] < 7000:
            self.speak("ì˜¤ëŠ˜ì€ ì¡°ê¸ˆ ë” ê±¸ìœ¼ì‹œëŠ” ê²ƒì´ ì¢‹ê² ìŠµë‹ˆë‹¤.")
        
        if health_data['stress_level'] == 'ë†’ìŒ':
            self.speak("ìŠ¤íŠ¸ë ˆìŠ¤ê°€ ë†’ì€ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì ì‹œ íœ´ì‹ì„ ì·¨í•˜ì„¸ìš”.")
        
        return health_data
    
    async def smart_conversation(self, context: List[Dict]) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ëŒ€í™”"""
        
        # ëŒ€í™” ë§¥ë½ ë¶„ì„
        recent_topics = []
        for item in context[-5:]:  # ìµœê·¼ 5ê°œ ëŒ€í™”
            if item['speaker'] == 'user':
                # ê°„ë‹¨í•œ í† í”½ ì¶”ì¶œ
                if 'ë‚ ì”¨' in item['text']:
                    recent_topics.append('weather')
                elif 'ì¼ì •' in item['text']:
                    recent_topics.append('schedule')
        
        # ë§¥ë½ì— ë§ëŠ” ì¶”ê°€ ì •ë³´ ì œê³µ
        suggestions = []
        
        if 'weather' in recent_topics:
            suggestions.append("ìš°ì‚°ì„ ì±™ê¸°ëŠ” ê²ƒì´ ì¢‹ê² ìŠµë‹ˆë‹¤.")
        
        if 'schedule' in recent_topics:
            suggestions.append("ì˜¤ëŠ˜ ì¼ì •ì„ ë‹¤ì‹œ í™•ì¸í•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
        
        return suggestions
    
    async def emotion_detection(self, audio_features: Dict) -> str:
        """ìŒì„±ì—ì„œ ê°ì • ê°ì§€ (ì‹œë®¬ë ˆì´ì…˜)"""
        
        # ì‹¤ì œë¡œëŠ” ìŒì„± íŠ¹ì§•(í”¼ì¹˜, í†¤, ì†ë„ ë“±) ë¶„ì„
        emotions = ['neutral', 'happy', 'sad', 'angry', 'surprised']
        detected_emotion = np.random.choice(emotions)
        
        # ê°ì •ì— ë§ëŠ” ì‘ë‹µ ì¡°ì •
        if detected_emotion == 'sad':
            self.tts.voice_styles['current'] = self.tts.voice_styles['calm']
            self.speak("ê¸°ë¶„ì´ ì¢‹ì§€ ì•Šìœ¼ì‹  ê²ƒ ê°™ë„¤ìš”. ì œê°€ ë„ìš¸ ìˆ˜ ìˆëŠ” ê²ƒì´ ìˆì„ê¹Œìš”?")
        
        elif detected_emotion == 'happy':
            self.tts.voice_styles['current'] = self.tts.voice_styles['happy']
            self.speak("ê¸°ë¶„ì´ ì¢‹ìœ¼ì‹  ê²ƒ ê°™ì•„ ì €ë„ ê¸°ì©ë‹ˆë‹¤!")
        
        return detected_emotion

# ì „ì²´ ì‹œìŠ¤í…œ í†µí•© ë°ëª¨
async def jarvis_demo():
    """JARVIS ì „ì²´ ê¸°ëŠ¥ ë°ëª¨"""
    
    jarvis = AdvancedJARVIS("ê¹€ì² ìˆ˜")
    
    # ì‹œë‚˜ë¦¬ì˜¤ 1: ì•„ì¹¨ ë£¨í‹´
    print("\n=== ì‹œë‚˜ë¦¬ì˜¤ 1: ì•„ì¹¨ ë£¨í‹´ ===")
    
    await jarvis.handle_greeting()
    await asyncio.sleep(2)
    
    # ë‚ ì”¨ í™•ì¸
    await jarvis.process_command("ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?")
    await asyncio.sleep(2)
    
    # ë‰´ìŠ¤ ë¸Œë¦¬í•‘
    await jarvis.get_news_briefing()
    await asyncio.sleep(2)
    
    # ì¼ì • í™•ì¸
    await jarvis.process_command("ì˜¤ëŠ˜ ì¼ì • ì•Œë ¤ì¤˜")
    await asyncio.sleep(2)
    
    # ì‹œë‚˜ë¦¬ì˜¤ 2: ì‘ì—… ì§€ì›
    print("\n=== ì‹œë‚˜ë¦¬ì˜¤ 2: ì‘ì—… ì§€ì› ===")
    
    # ì •ë³´ ê²€ìƒ‰
    await jarvis.handle_information_query("ì¸ê³µì§€ëŠ¥")
    await asyncio.sleep(2)
    
    # ë²ˆì—­
    await jarvis.translate_text("ì•ˆë…•í•˜ì„¸ìš”", "en")
    await asyncio.sleep(2)
    
    # ì‹œë‚˜ë¦¬ì˜¤ 3: ìŠ¤ë§ˆíŠ¸í™ˆ
    print("\n=== ì‹œë‚˜ë¦¬ì˜¤ 3: ìŠ¤ë§ˆíŠ¸í™ˆ ì œì–´ ===")
    
    await jarvis.process_command("ê±°ì‹¤ ì „ë“± ì¼œì¤˜")
    await asyncio.sleep(2)
    
    await jarvis.process_command("ì—ì–´ì»¨ ì˜¨ë„ 24ë„ë¡œ ì„¤ì •í•´ì¤˜")
    await asyncio.sleep(2)
    
    # ì‹œë‚˜ë¦¬ì˜¤ 4: ê±´ê°• ê´€ë¦¬
    print("\n=== ì‹œë‚˜ë¦¬ì˜¤ 4: ê±´ê°• ê´€ë¦¬ ===")
    
    await jarvis.health_monitoring()
    await asyncio.sleep(2)
    
    # ì¢…ë£Œ
    await jarvis.shutdown()

# ì‹¤í–‰
if __name__ == "__main__":
    asyncio.run(jarvis_demo())
```

---

## Day 3 ë§ˆë¬´ë¦¬ - ìš°ë¦¬ê°€ ë§Œë“  ê²ƒë“¤

### ì˜¤ëŠ˜ ë°°ìš´ ë‚´ìš© ì´ì •ë¦¬

1. **ëª¨ë¸ ìµœì í™”**
   - ì–‘ìí™”: 32ë¹„íŠ¸ â†’ 8ë¹„íŠ¸ë¡œ ëª¨ë¸ í¬ê¸° 75% ê°ì†Œ
   - ê°€ì§€ì¹˜ê¸°: ë¶ˆí•„ìš”í•œ ì—°ê²° ì œê±°ë¡œ ì†ë„ í–¥ìƒ
   - ì§€ì‹ ì¦ë¥˜: ì‘ì€ ëª¨ë¸ë¡œ í° ëª¨ë¸ì˜ ì„±ëŠ¥ ì¬í˜„
   - í•˜ë“œì›¨ì–´ë³„ ìµœì í™” ì „ëµ

2. **ìŒì„± ì¸í„°í˜ì´ìŠ¤**
   - Whisperë¥¼ í™œìš©í•œ ê³ ê¸‰ STT
   - ê°ì • í‘œí˜„ì´ ê°€ëŠ¥í•œ TTS
   - ì‹¤ì‹œê°„ ìŒì„± ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

3. **JARVIS ì‹œìŠ¤í…œ**
   - ìŒì„± ëŒ€í™” ì¸í„°í˜ì´ìŠ¤
   - ì»´í“¨í„° ë¹„ì „ í†µí•©
   - ìŠ¤ë§ˆíŠ¸í™ˆ ì œì–´ (ì‹œë®¬ë ˆì´ì…˜)
   - ì •ë³´ ê²€ìƒ‰ ë° ì‘ì—… ìë™í™”

### 3ì¼ê°„ì˜ ì—¬ì • ìš”ì•½

**Day 1**: ë©€í‹°ëª¨ë‹¬ AIì˜ ì„¸ê³„
- ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ì´í•´í•˜ëŠ” AI
- ìŠ¤ë§ˆíŠ¸ ì‚¬ì§„ ì¼ê¸° ì• í”Œë¦¬ì¼€ì´ì…˜

**Day 2**: ì‹¤ì‹œê°„ ë¹„ì „ AI
- YOLOë¡œ ê°ì²´ íƒì§€ ë° ì¶”ì 
- ìŠ¤ë§ˆíŠ¸ ë³´ì•ˆ ì‹œìŠ¤í…œ êµ¬ì¶•

**Day 3**: í†µí•© AI ì‹œìŠ¤í…œ
- ëª¨ë¸ ìµœì í™”ë¡œ ì‹¤ìš©ì ì¸ AI
- JARVIS ìŠ¤íƒ€ì¼ AI ë¹„ì„œ ì™„ì„±

### ğŸ“ ìˆ˜ë£Œë¥¼ ì¶•í•˜í•©ë‹ˆë‹¤!

3ì¼ê°„ì˜ ì˜¨ë””ë°”ì´ìŠ¤ AI ì—¬ì •ì„ ì™„ì£¼í•˜ì‹  ê²ƒì„ ì¶•í•˜í•©ë‹ˆë‹¤! 

ì´ì œ ì—¬ëŸ¬ë¶„ì€:
- âœ… ìµœì‹  AI ëª¨ë¸ì„ ì§ì ‘ êµ¬ë™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- âœ… ì‹¤ì‹œê°„ ë¹„ì „ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- âœ… ìŒì„± ëŒ€í™”í˜• AIë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤
- âœ… ëª¨ë¸ì„ ìµœì í™”í•˜ì—¬ ì‹¤ì œ ì œí’ˆì— ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤

### ë‹¤ìŒ ë‹¨ê³„ëŠ”?

1. **í”„ë¡œì íŠ¸ í™•ì¥**
   - ë§Œë“  í”„ë¡œì íŠ¸ì— ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶”ê°€
   - ì—¬ëŸ¬ í”„ë¡œì íŠ¸ë¥¼ ê²°í•©í•œ í†µí•© ì‹œìŠ¤í…œ
   - ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ì œí’ˆìœ¼ë¡œ ë°œì „

2. **ìƒˆë¡œìš´ ë„ì „**
   - ë¡œë´‡ ì œì–´ ì‹œìŠ¤í…œ
   - AR/VRê³¼ AI ê²°í•©
   - ì—£ì§€ ë””ë°”ì´ìŠ¤ ë°°í¬

3. **ì»¤ë®¤ë‹ˆí‹° ì°¸ì—¬**
   - GitHubì— í”„ë¡œì íŠ¸ ê³µìœ 
   - ë¸”ë¡œê·¸ ì‘ì„±
   - ì˜¤í”ˆì†ŒìŠ¤ ê¸°ì—¬

### ë§ˆì§€ë§‰ ë©”ì‹œì§€

AIëŠ” ë” ì´ìƒ ë¨¼ ë¯¸ë˜ì˜ ê¸°ìˆ ì´ ì•„ë‹™ë‹ˆë‹¤. ì—¬ëŸ¬ë¶„ì˜ ë…¸íŠ¸ë¶ì—ì„œ, ì—¬ëŸ¬ë¶„ì˜ ì†ìœ¼ë¡œ ì§ì ‘ ë§Œë“¤ ìˆ˜ ìˆëŠ” í˜„ì‹¤ì˜ ê¸°ìˆ ì…ë‹ˆë‹¤.

ì´ êµì¬ì—ì„œ ë°°ìš´ ë‚´ìš©ì€ ì‹œì‘ì— ë¶ˆê³¼í•©ë‹ˆë‹¤. AI ê¸°ìˆ ì€ ë§¤ì¼ ë°œì „í•˜ê³  ìˆìœ¼ë©°, ì—¬ëŸ¬ë¶„ì´ ë§Œë“¤ì–´ê°ˆ ë¯¸ë˜ëŠ” ë¬´í•œí•œ ê°€ëŠ¥ì„±ìœ¼ë¡œ ê°€ë“í•©ë‹ˆë‹¤.

**"The best way to predict the future is to invent it."** - Alan Kay

ì—¬ëŸ¬ë¶„ì´ ë§Œë“¤ì–´ê°ˆ AIì˜ ë¯¸ë˜ë¥¼ ê¸°ëŒ€í•©ë‹ˆë‹¤! ğŸš€

---

## ë¶€ë¡: ì¶”ê°€ í•™ìŠµ ìë£Œ

### ğŸ“š ì¶”ì²œ ë„ì„œ
1. "Deep Learning" - Ian Goodfellow
2. "Hands-On Machine Learning" - AurÃ©lien GÃ©ron
3. "The Hundred-Page Machine Learning Book" - Andriy Burkov

### ğŸŒ ì˜¨ë¼ì¸ ë¦¬ì†ŒìŠ¤
1. **Hugging Face**: https://huggingface.co
2. **Papers with Code**: https://paperswithcode.com
3. **Fast.ai**: https://www.fast.ai
4. **PyTorch Tutorials**: https://pytorch.org/tutorials

### ğŸ“ ì˜¨ë¼ì¸ ê°•ì¢Œ
1. **Coursera - Deep Learning Specialization**
2. **Fast.ai - Practical Deep Learning**
3. **Udacity - AI Programming with Python**

### ğŸ› ï¸ ìœ ìš©í•œ ë„êµ¬
1. **Weights & Biases**: ì‹¤í—˜ ì¶”ì 
2. **TensorBoard**: ì‹œê°í™”
3. **Gradio**: ë°ëª¨ UI ìƒì„±
4. **Streamlit**: ì›¹ ì•± ê°œë°œ

### ğŸ’¬ ì»¤ë®¤ë‹ˆí‹°
1. **Reddit - r/MachineLearning**
2. **Discord - Hugging Face**
3. **Stack Overflow - AI/ML íƒœê·¸**
4. **Korean AI Community**: AI ê°œë°œì ëª¨ì„

í–‰ìš´ì„ ë¹•ë‹ˆë‹¤! ğŸ€