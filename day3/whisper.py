
import whisper
import numpy as np
import queue
import threading
import webrtcvad
import collections
import time
from typing import Optional, Callable
import matplotlib.pyplot as plt
import torch

class AdvancedWhisperSTT:
    """ê³ ê¸‰ ìŒì„± ì¸ì‹ ì‹œìŠ¤í…œ"""
    
    def __init__(self, model_size="base", language="ko"):
        print(f"ğŸ¤ Whisper {model_size} ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        
        self.model = whisper.load_model(model_size)
        self.language = language
        
        # VAD (Voice Activity Detection)
        self.vad = webrtcvad.Vad(2)  # ë¯¼ê°ë„ 0-3
        
        # ì˜¤ë””ì˜¤ ì„¤ì •
        self.sample_rate = 16000
        self.frame_duration = 30  # ms
        self.frame_size = int(self.sample_rate * self.frame_duration / 1000)
        
        # ë²„í¼
        self.audio_queue = queue.Queue()
        self.is_recording = False
        
        print("âœ… ìŒì„± ì¸ì‹ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
    
    def visualize_audio_processing(self):
        """ì˜¤ë””ì˜¤ ì²˜ë¦¬ ê³¼ì • ì‹œê°í™”"""
        
        fig, axes = plt.subplots(3, 2, figsize=(12, 10))
        
        # 1. ì›ë³¸ ì˜¤ë””ì˜¤ ì‹ í˜¸
        ax1 = axes[0, 0]
        t = np.linspace(0, 1, self.sample_rate)
        # ìŒì„± ì‹ í˜¸ ì‹œë®¬ë ˆì´ì…˜ (ì—¬ëŸ¬ ì£¼íŒŒìˆ˜ í˜¼í•©)
        signal = (0.5 * np.sin(2 * np.pi * 200 * t) + 
                 0.3 * np.sin(2 * np.pi * 400 * t) + 
                 0.2 * np.sin(2 * np.pi * 800 * t))
        noise = np.random.normal(0, 0.1, len(t))
        audio = signal + noise
        
        ax1.plot(t[:1000], audio[:1000], alpha=0.7)
        ax1.set_title('ì›ë³¸ ì˜¤ë””ì˜¤ ì‹ í˜¸', fontweight='bold')
        ax1.set_xlabel('ì‹œê°„ (ì´ˆ)')
        ax1.set_ylabel('ì§„í­')
        ax1.grid(True, alpha=0.3)
        
        # 2. ì£¼íŒŒìˆ˜ ìŠ¤í™íŠ¸ëŸ¼
        ax2 = axes[0, 1]
        freqs = np.fft.fftfreq(len(audio), 1/self.sample_rate)
        fft = np.abs(np.fft.fft(audio))
        
        ax2.plot(freqs[:len(freqs)//2], fft[:len(fft)//2])
        ax2.set_title('ì£¼íŒŒìˆ˜ ìŠ¤í™íŠ¸ëŸ¼', fontweight='bold')
        ax2.set_xlabel('ì£¼íŒŒìˆ˜ (Hz)')
        ax2.set_ylabel('ê°•ë„')
        ax2.set_xlim(0, 2000)
        ax2.grid(True, alpha=0.3)
        
        # 3. VAD (ìŒì„± í™œë™ ê°ì§€)
        ax3 = axes[1, 0]
        # VAD ì‹œë®¬ë ˆì´ì…˜
        vad_result = np.abs(audio) > 0.3
        vad_smoothed = np.convolve(vad_result, np.ones(100)/100, mode='same')
        
        ax3.plot(t[:5000], audio[:5000], alpha=0.5, label='ì˜¤ë””ì˜¤')
        ax3.fill_between(t[:5000], -1, 1, where=vad_smoothed[:5000] > 0.5,
                         alpha=0.3, color='red', label='ìŒì„± ê°ì§€')
        ax3.set_title('ìŒì„± í™œë™ ê°ì§€ (VAD)', fontweight='bold')
        ax3.set_xlabel('ì‹œê°„ (ì´ˆ)')
        ax3.set_ylabel('ì§„í­')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨
        ax4 = axes[1, 1]
        # ê°„ë‹¨í•œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ì‹œë®¬ë ˆì´ì…˜
        spec_data = np.random.rand(128, 100) * np.linspace(1, 0.1, 128).reshape(-1, 1)
        im = ax4.imshow(spec_data, aspect='auto', origin='lower', cmap='viridis')
        ax4.set_title('ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨', fontweight='bold')
        ax4.set_xlabel('ì‹œê°„ í”„ë ˆì„')
        ax4.set_ylabel('ë©œ ë¹ˆ')
        plt.colorbar(im, ax=ax4)
        
        # 5. ìŒì„± ì¸ì‹ ê³¼ì •
        ax5 = axes[2, 0]
        ax5.text(0.5, 0.8, 'ìŒì„± ì…ë ¥', ha='center', fontsize=12, 
                bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue"))
        ax5.text(0.5, 0.6, 'â†“', ha='center', fontsize=16)
        ax5.text(0.5, 0.4, 'íŠ¹ì§• ì¶”ì¶œ', ha='center', fontsize=12,
                bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgreen"))
        ax5.text(0.5, 0.2, 'â†“', ha='center', fontsize=16)
        ax5.text(0.5, 0.0, 'í…ìŠ¤íŠ¸ ë³€í™˜', ha='center', fontsize=12,
                bbox=dict(boxstyle="round,pad=0.3", facecolor="lightyellow"))
        ax5.set_xlim(0, 1)
        ax5.set_ylim(-0.2, 1)
        ax5.axis('off')
        ax5.set_title('Whisper ì²˜ë¦¬ ê³¼ì •', fontweight='bold')
        
        # 6. ì–¸ì–´ ëª¨ë¸ í™•ë¥ 
        ax6 = axes[2, 1]
        words = ['ì•ˆë…•í•˜ì„¸ìš”', 'ì•ˆë…•í•˜ì‹­ë‹ˆê¹Œ', 'ì•ˆë…•íˆ', 'ì•ˆë…•', 'ì•ˆë…„í•˜ì„¸ìš”']
        probs = [0.4, 0.3, 0.15, 0.1, 0.05]
        
        bars = ax6.bar(words, probs, color='orange')
        ax6.set_title('ì–¸ì–´ ëª¨ë¸ ì˜ˆì¸¡ í™•ë¥ ', fontweight='bold')
        ax6.set_ylabel('í™•ë¥ ')
        ax6.set_xticklabels(words, rotation=45, ha='right')
        
        for bar, prob in zip(bars, probs):
            ax6.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{prob:.2f}', ha='center')
        
        plt.tight_layout()
        plt.show()
    
    def voice_activity_detection(self, audio_frame):
        """ìŒì„± í™œë™ ê°ì§€"""
        # 16ë¹„íŠ¸ PCMìœ¼ë¡œ ë³€í™˜
        audio_int16 = (audio_frame * 32767).astype(np.int16)
        
        # VAD ì ìš©
        return self.vad.is_speech(audio_int16.tobytes(), self.sample_rate)
    
    def continuous_recognition(self, callback: Callable[[str], None]):
        """ì—°ì† ìŒì„± ì¸ì‹"""
        
        def audio_callback(indata, frames, time_info, status):
            """ì˜¤ë””ì˜¤ ì½œë°±"""
            if status:
                print(f"ì˜¤ë””ì˜¤ ìƒíƒœ: {status}")
            
            self.audio_queue.put(indata.copy())
        
        # ìŠ¤íŠ¸ë¦¼ ì‹œì‘
        stream = sd.InputStream(
            samplerate=self.sample_rate,
            channels=1,
            callback=audio_callback,
            blocksize=self.frame_size
        )
        
        with stream:
            print("\nğŸ¤ ìŒì„± ì¸ì‹ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            print("ë§ì”€í•˜ì„¸ìš”! (ì¢…ë£Œ: Ctrl+C)")
            
            # ë§ ë²„í¼ (3ì´ˆ ë¶„ëŸ‰)
            ring_buffer = collections.deque(maxlen=100)
            
            # ìŒì„± ê°ì§€ ìƒíƒœ
            triggered = False
            voiced_frames = []
            
            while self.is_recording:
                try:
                    frame = self.audio_queue.get(timeout=0.1)
                    
                    # VAD í™•ì¸
                    is_speech = self.voice_activity_detection(frame)
                    
                    if not triggered:
                        ring_buffer.append((frame, is_speech))
                        num_voiced = len([f for f, speech in ring_buffer if speech])
                        
                        # ìŒì„± ì‹œì‘ ê°ì§€ (0.3ì´ˆ ì´ìƒ)
                        if num_voiced > 0.3 * 100:
                            triggered = True
                            print("ğŸ”Š ìŒì„± ê°ì§€ë¨...")
                            
                            # ë§ ë²„í¼ì˜ ë‚´ìš©ì„ voiced_framesì— ì¶”ê°€
                            for f, s in ring_buffer:
                                voiced_frames.append(f)
                            ring_buffer.clear()
                    else:
                        # ìŒì„± ìˆ˜ì§‘
                        voiced_frames.append(frame)
                        ring_buffer.append((frame, is_speech))
                        
                        # ìŒì„± ì¢…ë£Œ ê°ì§€ (1ì´ˆ ì´ìƒ ì¡°ìš©)
                        num_unvoiced = len([f for f, speech in ring_buffer if not speech])
                        if num_unvoiced > 1.0 * 100:
                            print("ğŸ”‡ ìŒì„± ì¢…ë£Œ, ì¸ì‹ ì¤‘...")
                            
                            # ìŒì„± ì¸ì‹ ìˆ˜í–‰
                            audio_data = np.concatenate(voiced_frames)
                            text = self.recognize(audio_data)
                            
                            if text and callback:
                                callback(text)
                            
                            # ì´ˆê¸°í™”
                            triggered = False
                            voiced_frames = []
                            ring_buffer.clear()
                            
                except queue.Empty:
                    continue
                except KeyboardInterrupt:
                    break
    
    def recognize(self, audio_data: np.ndarray) -> str:
        """ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        # WhisperëŠ” float32 í•„ìš”
        audio_float32 = audio_data.astype(np.float32)
        
        # íŒ¨ë”© (ìµœì†Œ ê¸¸ì´ í™•ë³´)
        if len(audio_float32) < self.sample_rate:
            audio_float32 = np.pad(audio_float32, (0, self.sample_rate - len(audio_float32)))
        
        # ìŒì„± ì¸ì‹
        result = self.model.transcribe(
            audio_float32,
            language=self.language,
            fp16=torch.cuda.is_available()
        )
        
        return result["text"].strip()
    
    def start_recording(self):
        """ë…¹ìŒ ì‹œì‘"""
        self.is_recording = True
        
        def on_recognition(text):
            print(f"\nğŸ“ ì¸ì‹ëœ í…ìŠ¤íŠ¸: {text}")
        
        # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
        self.recognition_thread = threading.Thread(
            target=self.continuous_recognition,
            args=(on_recognition,)
        )
        self.recognition_thread.start()
    
    def stop_recording(self):
        """ë…¹ìŒ ì¤‘ì§€"""
        self.is_recording = False
        if hasattr(self, 'recognition_thread'):
            self.recognition_thread.join()

# ìŒì„± ì¸ì‹ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
def test_advanced_stt():
    stt = AdvancedWhisperSTT(model_size="base", language="ko")
    
    # ì˜¤ë””ì˜¤ ì²˜ë¦¬ ê³¼ì • ì‹œê°í™”
    stt.visualize_audio_processing()
    
    # ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ í…ŒìŠ¤íŠ¸
    print("\n=== ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ í…ŒìŠ¤íŠ¸ ===")
    
    try:
        stt.start_recording()
        time.sleep(30)  # 30ì´ˆ ë™ì•ˆ ë…¹ìŒ
    except KeyboardInterrupt:
        print("\nì¸ì‹ ì¤‘ë‹¨...")
    finally:
        stt.stop_recording()
        
if __name__ == "__main__":
    test_advanced_stt()
    