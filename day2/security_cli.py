#!/usr/bin/env python3
"""
Command-line interface for AI Security System - Headless Video Analysis

Usage:
    python security_cli.py --video sample.mp4 --output-dir results/
    python security_cli.py -v video.mp4 -o results/ --confidence 0.7 --save-video
"""

import argparse
import sys
import os
import cv2
import torch
import numpy as np
from ultralytics import YOLO
import supervision as sv
from collections import defaultdict, deque
import time
from datetime import datetime
import json
from typing import Dict, List, Tuple, Optional

class HeadlessSecuritySystem:
    """AI ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ë³´ì•ˆ ì‹œìŠ¤í…œ - CLI ë²„ì „ (UI ì—†ìŒ)"""
    
    def __init__(self, config: Dict = None, output_dir: str = "security_output"):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        print("ğŸ”’ AI ë³´ì•ˆ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        
        # ì„¤ì •
        self.config = config or self.get_default_config()
        self.output_dir = output_dir
        os.makedirs(self.output_dir, exist_ok=True)
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.init_models()
        
        # ì¶”ì ê¸° ì´ˆê¸°í™”
        self.tracker = sv.ByteTrack()
        
        # ë°ì´í„° ì €ì¥ì†Œ
        self.init_storage()
        
        print("âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def get_default_config(self) -> Dict:
        """ê¸°ë³¸ ì„¤ì •ê°’"""
        return {
            'yolo_model': 'yolo11m.pt',
            'confidence_threshold': 0.5,
            'max_tracked_objects': 50,
            'suspicious_behaviors': {
                'loitering': {'duration': 60, 'area': 100},
                'running': {'speed_threshold': 5.0},
                'abandoned_object': {'duration': 300}
            }
        }
    
    def init_models(self):
        """AI ëª¨ë¸ ì´ˆê¸°í™”"""
        print("  ğŸ§  YOLO ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.yolo = YOLO(self.config['yolo_model'])
        self.behavior_analyzer = BehaviorAnalyzer(self.config['suspicious_behaviors'])
        
    def init_storage(self):
        """ë°ì´í„° ì €ì¥ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        # ì¶”ì  ë°ì´í„°
        self.track_history = defaultdict(lambda: {
            'positions': deque(maxlen=300),
            'timestamps': deque(maxlen=300),
            'class': None,
            'first_seen': None,
            'last_seen': None,
            'alerts': []
        })
        
        # ì´ë²¤íŠ¸ ë¡œê·¸
        self.event_log = []
        self.frame_count = 0
        self.alerts_count = defaultdict(int)
        
    def process_frame(self, frame: np.ndarray, frame_id: int, save_annotated: bool = False) -> Tuple[Optional[np.ndarray], List[Dict]]:
        """ë‹¨ì¼ í”„ë ˆì„ ì²˜ë¦¬"""
        
        # 1. ê°ì²´ íƒì§€
        results = self.yolo(frame, conf=self.config['confidence_threshold'], verbose=False)[0]
        detections = sv.Detections.from_ultralytics(results)
        
        # 2. ê°ì²´ ì¶”ì 
        tracks = self.tracker.update_with_detections(detections)
        
        # 3. ì¶”ì  ë°ì´í„° ì—…ë°ì´íŠ¸
        current_time = time.time()
        events = []
        
        for i in range(len(tracks)):
            track_id = tracks.tracker_id[i]
            class_id = tracks.class_id[i]
            class_name = self.yolo.names[class_id]
            bbox = tracks.xyxy[i]
            
            # ì¤‘ì‹¬ì  ê³„ì‚°
            center = ((bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2)
            
            # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            history = self.track_history[track_id]
            history['positions'].append(center)
            history['timestamps'].append(current_time)
            history['class'] = class_name
            
            if history['first_seen'] is None:
                history['first_seen'] = current_time
                events.append({
                    'type': 'new_object',
                    'track_id': track_id,
                    'class': class_name,
                    'time': current_time,
                    'frame_id': frame_id
                })
            
            history['last_seen'] = current_time
        
        # 4. í–‰ë™ ë¶„ì„
        suspicious_activities = self.behavior_analyzer.analyze(
            self.track_history, current_time
        )
        
        # 5. ì•Œë¦¼ ìƒì„± ë° ì´ë²¤íŠ¸ ê¸°ë¡
        for activity in suspicious_activities:
            self.alerts_count[activity['type']] += 1
            activity['frame_id'] = frame_id
            events.append(activity)
            
            # ì½˜ì†” ì•Œë¦¼
            print(f"ğŸš¨ ALERT: {activity['type']} (Frame {frame_id})")
            print(f"   Details: {activity.get('details', {}).get('message', 'N/A')}")
        
        # 6. ì‹œê°í™” (í•„ìš”í•œ ê²½ìš°)
        annotated_frame = None
        if save_annotated:
            annotated_frame = self.visualize_results(frame, tracks, suspicious_activities, frame_id)
        
        return annotated_frame, events
    
    def visualize_results(self, frame: np.ndarray, tracks: sv.Detections, 
                         activities: List[Dict], frame_id: int) -> np.ndarray:
        """ê²°ê³¼ ì‹œê°í™” (ì–´ë…¸í…Œì´ì…˜)"""
        annotated = frame.copy()
        
        # ë°•ìŠ¤ ê·¸ë¦¬ê¸°
        box_annotator = sv.BoxAnnotator()
        annotated = box_annotator.annotate(scene=annotated, detections=tracks)
        
        # ì¶”ì  IDì™€ í´ë˜ìŠ¤ í‘œì‹œ
        for i in range(len(tracks)):
            track_id = tracks.tracker_id[i]
            class_id = tracks.class_id[i]
            class_name = self.yolo.names[class_id]
            bbox = tracks.xyxy[i]
            
            # ë¼ë²¨
            label = f"ID:{track_id} {class_name}"
            
            # ì˜ì‹¬ í–‰ë™ì´ ìˆëŠ” ê²½ìš° ê°•ì¡°
            is_suspicious = any(
                activity.get('track_id') == track_id 
                for activity in activities
            )
            
            color = (0, 0, 255) if is_suspicious else (0, 255, 0)
            
            cv2.putText(
                annotated, label,
                (int(bbox[0]), int(bbox[1] - 10)),
                cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2
            )
        
        # ìƒíƒœ í‘œì‹œ
        self.draw_status(annotated, len(tracks), activities, frame_id)
        
        return annotated
    
    def draw_status(self, frame: np.ndarray, num_objects: int, activities: List[Dict], frame_id: int):
        """ìƒíƒœ ì •ë³´ í‘œì‹œ"""
        h, w = frame.shape[:2]
        
        # ìƒë‹¨ ì •ë³´ ë°”
        overlay = frame.copy()
        cv2.rectangle(overlay, (0, 0), (w, 50), (0, 0, 0), -1)
        frame[:50] = cv2.addWeighted(overlay[:50], 0.7, frame[:50], 0.3, 0)
        
        # í”„ë ˆì„ ë²ˆí˜¸
        cv2.putText(frame, f"Frame: {frame_id}", (10, 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        # ê°ì²´ ìˆ˜
        cv2.putText(frame, f"Objects: {num_objects}", (200, 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        # ê²½ê³ 
        if activities:
            alert_text = f"ALERT: {activities[0]['type']}"
            cv2.putText(frame, alert_text, (400, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
    
    def analyze_video_file(self, video_path: str, save_frames: bool = False, 
                          save_video: bool = False, progress_interval: int = 30) -> Dict:
        """ë¹„ë””ì˜¤ íŒŒì¼ ë¶„ì„ (í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ)"""
        
        print(f"\nğŸ“¹ ë¹„ë””ì˜¤ ë¶„ì„ ì‹œì‘: {video_path}")
        
        if not os.path.exists(video_path):
            raise FileNotFoundError(f"ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise RuntimeError(f"ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        
        # ë¹„ë””ì˜¤ ì†ì„±
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = int(cap.get(cv2.CAP_PROP_FPS)) or 30
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        print(f"   ì´ í”„ë ˆì„: {total_frames}")
        print(f"   FPS: {fps}")
        print(f"   í•´ìƒë„: {width}x{height}")
        
        # ë¹„ë””ì˜¤ ì €ì¥ ì„¤ì •
        video_writer = None
        if save_video:
            output_video_path = os.path.join(self.output_dir, "analyzed_video.mp4")
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))
            print(f"   ê²°ê³¼ ë¹„ë””ì˜¤ ì €ì¥: {output_video_path}")
        
        # í”„ë ˆì„ë³„ ë¶„ì„
        frame_id = 0
        start_time = time.time()
        
        print("\nğŸ” ë¶„ì„ ì§„í–‰ ì¤‘...")
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # í”„ë ˆì„ ì²˜ë¦¬
            annotated_frame, events = self.process_frame(
                frame, frame_id, save_annotated=(save_frames or save_video)
            )
            
            # ì´ë²¤íŠ¸ ë¡œê¹…
            for event in events:
                self.log_event(event)
            
            # ì–´ë…¸í…Œì´ì…˜ëœ í”„ë ˆì„ ì €ì¥
            if save_frames and annotated_frame is not None:
                frame_filename = f"frame_{frame_id:06d}.jpg"
                frame_path = os.path.join(self.output_dir, frame_filename)
                cv2.imwrite(frame_path, annotated_frame)
            
            # ë¹„ë””ì˜¤ ì €ì¥
            if save_video and video_writer and annotated_frame is not None:
                video_writer.write(annotated_frame)
            
            # ì§„í–‰ë¥  í‘œì‹œ
            if frame_id % progress_interval == 0:
                progress = (frame_id / total_frames) * 100
                elapsed = time.time() - start_time
                fps_current = frame_id / elapsed if elapsed > 0 else 0
                print(f"   ì§„í–‰ë¥ : {progress:.1f}% ({frame_id}/{total_frames}) - {fps_current:.1f} FPS")
            
            frame_id += 1
            self.frame_count = frame_id
        
        # ì •ë¦¬
        cap.release()
        if video_writer:
            video_writer.release()
        
        analysis_time = time.time() - start_time
        
        print(f"\nâœ… ë¶„ì„ ì™„ë£Œ!")
        print(f"   ì´ ì²˜ë¦¬ ì‹œê°„: {analysis_time:.2f}ì´ˆ")
        print(f"   í‰ê·  FPS: {frame_id / analysis_time:.2f}")
        
        # ê²°ê³¼ ìƒì„±
        return self.generate_analysis_report()
    
    def log_event(self, event: Dict):
        """ì´ë²¤íŠ¸ ë¡œê¹…"""
        event['timestamp'] = datetime.now().isoformat()
        self.event_log.append(event)
    
    def convert_to_json_serializable(self, obj):
        """Convert objects to JSON serializable format"""
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {key: self.convert_to_json_serializable(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self.convert_to_json_serializable(item) for item in obj]
        elif isinstance(obj, tuple):
            return tuple(self.convert_to_json_serializable(item) for item in obj)
        elif hasattr(obj, '__dict__'):
            return self.convert_to_json_serializable(obj.__dict__)
        else:
            return obj

    def generate_analysis_report(self) -> Dict:
        """ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("\nğŸ“Š ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        # ì „ì²´ í†µê³„
        total_objects = len(self.track_history)
        
        # í´ë˜ìŠ¤ë³„ í†µê³„
        class_counts = defaultdict(int)
        for track_data in self.track_history.values():
            if track_data['class']:
                class_counts[track_data['class']] += 1
        
        # ì´ë²¤íŠ¸ í†µê³„
        event_counts = defaultdict(int)
        for event in self.event_log:
            event_counts[event['type']] += 1
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        report = {
            'analysis_summary': {
                'total_frames_processed': int(self.frame_count),
                'total_objects_tracked': int(total_objects),
                'total_events': len(self.event_log),
                'analysis_timestamp': datetime.now().isoformat()
            },
            'object_statistics': {
                'by_class': dict(class_counts),
                'total_unique_objects': int(total_objects)
            },
            'event_statistics': {
                'by_type': dict(event_counts),
                'alerts_summary': dict(self.alerts_count)
            },
            'detailed_events': self.convert_to_json_serializable(self.event_log),
            'configuration': self.convert_to_json_serializable(self.config)
        }
        
        # Convert entire report to ensure JSON compatibility
        report = self.convert_to_json_serializable(report)
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        report_path = os.path.join(self.output_dir, 'analysis_report.json')
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"   ğŸ“„ ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")
        
        return report
    
    def print_summary(self, report: Dict):
        """ì½˜ì†”ì— ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“‹ AI ë³´ì•ˆ ì‹œìŠ¤í…œ - ë¶„ì„ ê²°ê³¼ ìš”ì•½")
        print("="*60)
        
        summary = report['analysis_summary']
        print(f"ì²˜ë¦¬ëœ í”„ë ˆì„ ìˆ˜: {summary['total_frames_processed']:,}")
        print(f"ì¶”ì ëœ ê°ì²´ ìˆ˜: {summary['total_objects_tracked']}")
        print(f"ê°ì§€ëœ ì´ë²¤íŠ¸ ìˆ˜: {summary['total_events']}")
        
        print(f"\nğŸ“Š ê°ì²´ í´ë˜ìŠ¤ë³„ í†µê³„:")
        for class_name, count in sorted(report['object_statistics']['by_class'].items()):
            print(f"  - {class_name}: {count}ê°œ")
        
        print(f"\nğŸš¨ ì´ë²¤íŠ¸ ìœ í˜•ë³„ í†µê³„:")
        for event_type, count in sorted(report['event_statistics']['by_type'].items()):
            print(f"  - {event_type}: {count}íšŒ")
        
        print(f"\nâš ï¸  ë³´ì•ˆ ì•Œë¦¼ í†µê³„:")
        alerts = report['event_statistics']['alerts_summary']
        if alerts:
            for alert_type, count in sorted(alerts.items()):
                print(f"  - {alert_type}: {count}íšŒ")
        else:
            print("  - ì˜ì‹¬ìŠ¤ëŸ¬ìš´ í™œë™ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ âœ…")
        
        print("="*60)


class BehaviorAnalyzer:
    """í–‰ë™ ë¶„ì„ê¸° (ì›ë³¸ê³¼ ë™ì¼)"""
    
    def __init__(self, config: Dict):
        self.config = config
    
    def analyze(self, track_history: Dict, current_time: float) -> List[Dict]:
        """ì˜ì‹¬ìŠ¤ëŸ¬ìš´ í–‰ë™ ë¶„ì„"""
        suspicious_activities = []
        
        for track_id, history in track_history.items():
            if len(history['positions']) < 10:
                continue
            
            # 1. ë°°íšŒ ê°ì§€ (Loitering)
            loitering = self.detect_loitering(history, current_time)
            if loitering:
                suspicious_activities.append({
                    'type': 'loitering',
                    'track_id': track_id,
                    'details': loitering,
                    'time': current_time
                })
            
            # 2. ë¹ ë¥¸ ì›€ì§ì„ ê°ì§€ (Running)
            running = self.detect_running(history)
            if running:
                suspicious_activities.append({
                    'type': 'running',
                    'track_id': track_id,
                    'details': running,
                    'time': current_time
                })
            
            # 3. ë°©ì¹˜ëœ ë¬¼ì²´ ê°ì§€
            if history['class'] in ['backpack', 'suitcase', 'handbag']:
                abandoned = self.detect_abandoned_object(history, current_time)
                if abandoned:
                    suspicious_activities.append({
                        'type': 'abandoned_object',
                        'track_id': track_id,
                        'details': abandoned,
                        'time': current_time
                    })
        
        return suspicious_activities
    
    def detect_loitering(self, history: Dict, current_time: float) -> Optional[Dict]:
        """ë°°íšŒ ê°ì§€"""
        duration = current_time - history['first_seen']
        
        if duration < self.config['loitering']['duration']:
            return None
        
        # ì´ë™ ë²”ìœ„ ê³„ì‚°
        positions = list(history['positions'])
        if len(positions) < 2:
            return None
        
        xs = [p[0] for p in positions]
        ys = [p[1] for p in positions]
        
        x_range = max(xs) - min(xs)
        y_range = max(ys) - min(ys)
        area = x_range * y_range
        
        if area < self.config['loitering']['area']:
            return {
                'duration': duration,
                'area': area,
                'message': f"{duration:.1f}ì´ˆ ë™ì•ˆ ì‘ì€ ì˜ì—­ì— ë¨¸ë¬¼ëŸ¬ ìˆìŒ"
            }
        
        return None
    
    def detect_running(self, history: Dict) -> Optional[Dict]:
        """ë¹ ë¥¸ ì›€ì§ì„ ê°ì§€"""
        positions = list(history['positions'])
        timestamps = list(history['timestamps'])
        
        if len(positions) < 5:
            return None
        
        # ìµœê·¼ 5í”„ë ˆì„ì˜ ì†ë„ ê³„ì‚°
        speeds = []
        for i in range(len(positions) - 5, len(positions) - 1):
            dx = positions[i+1][0] - positions[i][0]
            dy = positions[i+1][1] - positions[i][1]
            dt = timestamps[i+1] - timestamps[i]
            
            if dt > 0:
                speed = np.sqrt(dx**2 + dy**2) / dt
                speeds.append(speed)
        
        avg_speed = np.mean(speeds) if speeds else 0
        
        if avg_speed > self.config['running']['speed_threshold']:
            return {
                'speed': avg_speed,
                'message': f"ë¹ ë¥¸ ì†ë„ë¡œ ì´ë™ ì¤‘ ({avg_speed:.1f} pixels/s)"
            }
        
        return None
    
    def detect_abandoned_object(self, history: Dict, current_time: float) -> Optional[Dict]:
        """ë°©ì¹˜ëœ ë¬¼ì²´ ê°ì§€"""
        # ë§ˆì§€ë§‰ìœ¼ë¡œ ë³¸ ì‹œê°„ í™•ì¸
        time_since_last_movement = current_time - history['last_seen']
        
        if time_since_last_movement > 1.0:  # 1ì´ˆ ì´ìƒ ì—…ë°ì´íŠ¸ ì—†ìŒ
            # ì •ì§€ ìƒíƒœ í™•ì¸
            positions = list(history['positions'])[-10:]
            if len(positions) < 10:
                return None
            
            # ìœ„ì¹˜ ë³€í™” ê³„ì‚°
            position_std = np.std([p[0] for p in positions]) + np.std([p[1] for p in positions])
            
            if position_std < 5.0:  # ê±°ì˜ ì›€ì§ì„ ì—†ìŒ
                stationary_duration = current_time - history['first_seen']
                
                if stationary_duration > self.config['abandoned_object']['duration']:
                    return {
                        'duration': stationary_duration,
                        'message': f"ë¬¼ì²´ê°€ {stationary_duration:.1f}ì´ˆ ë™ì•ˆ ë°©ì¹˜ë¨"
                    }
        
        return None


def main():
    parser = argparse.ArgumentParser(
        description="AI Security System - Command Line Interface",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python security_cli.py --video sample.mp4
  python security_cli.py -v video.mp4 -o results/ --confidence 0.7 --save-video
  python security_cli.py -v surveillance.mp4 -o analysis/ --save-frames --progress 60
        """
    )
    
    parser.add_argument(
        '-v', '--video',
        required=True,
        help='Path to the video file to analyze'
    )
    
    parser.add_argument(
        '-o', '--output-dir',
        default='security_output',
        help='Output directory for results (default: security_output)'
    )
    
    parser.add_argument(
        '-c', '--confidence',
        type=float,
        default=0.5,
        help='Detection confidence threshold (default: 0.5)'
    )
    
    parser.add_argument(
        '--save-frames',
        action='store_true',
        help='Save annotated frames as images'
    )
    
    parser.add_argument(
        '--save-video',
        action='store_true',
        help='Save annotated video'
    )
    
    parser.add_argument(
        '--progress',
        type=int,
        default=30,
        help='Progress update interval in frames (default: 30)'
    )
    
    parser.add_argument(
        '--model',
        default='yolo11m.pt',
        help='YOLO model to use (default: yolo11m.pt)'
    )
    
    args = parser.parse_args()
    
    print("ğŸ›¡ï¸  AI Security System - CLI Mode")
    print("="*50)
    
    try:
        # ì„¤ì • ìƒì„±
        config = {
            'yolo_model': args.model,
            'confidence_threshold': args.confidence,
            'max_tracked_objects': 50,
            'suspicious_behaviors': {
                'loitering': {'duration': 60, 'area': 100},
                'running': {'speed_threshold': 5.0},
                'abandoned_object': {'duration': 300}
            }
        }
        
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        security_system = HeadlessSecuritySystem(config, args.output_dir)
        
        # ë¹„ë””ì˜¤ ë¶„ì„
        report = security_system.analyze_video_file(
            args.video,
            save_frames=args.save_frames,
            save_video=args.save_video,
            progress_interval=args.progress
        )
        
        # ê²°ê³¼ ì¶œë ¥
        security_system.print_summary(report)
        
        print(f"\nğŸ“ ëª¨ë“  ê²°ê³¼ê°€ '{args.output_dir}' ë””ë ‰í† ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ë¶„ì„ì´ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()