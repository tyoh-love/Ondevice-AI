# practical_optimization.py - ì‹¤ì œ ëª¨ë¸ ìµœì í™”

import torch
import torch.nn as nn
import torch.quantization as quantization
from torch.utils.data import DataLoader, TensorDataset
import time
import os
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import argparse
import platform

# YOLO ëª¨ë¸ ê´€ë ¨ import
try:
    from yolo_models import YOLOModelLoader, YOLOOptimizer
    YOLO_AVAILABLE = True
except ImportError:
    YOLO_AVAILABLE = False

class OptimizableModel(nn.Module):
    """ìµœì í™”í•  ì˜ˆì‹œ ëª¨ë¸"""
    
    def __init__(self, input_size=784, hidden_size=256, num_classes=10):
        super().__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu1 = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.relu2 = nn.ReLU()
        self.fc3 = nn.Linear(hidden_size, num_classes)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.relu1(x)
        x = self.fc2(x)
        x = self.relu2(x)
        x = self.fc3(x)
        return x

class ModelOptimizer:
    """ëª¨ë¸ ìµœì í™” ë„êµ¬"""
    
    def __init__(self, explain_mode=False, model_type='simple'):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        # í”Œë«í¼ ì²´í¬
        self.platform = platform.system()
        self.is_arm = platform.machine().startswith('arm') or platform.machine() == 'aarch64'
        
        if self.platform == 'Darwin' or self.is_arm:
            print(f"âš ï¸  í”Œë«í¼: {self.platform} ({platform.machine()}) - ì–‘ìí™”ê°€ ì œí•œì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        # í•œê¸€ í°íŠ¸ ì„¤ì •
        self.setup_korean_font()
        
        # ì„¤ëª… ëª¨ë“œ
        self.explain_mode = explain_mode
        self.model_type = model_type
        
        if self.explain_mode:
            print("\nğŸ“š êµìœ¡ ëª¨ë“œê°€ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ê° ë‹¨ê³„ë¥¼ ìì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤.\n")
        
        # YOLO ëª¨ë¸ ê´€ë ¨ ì´ˆê¸°í™”
        if model_type == 'yolo' and YOLO_AVAILABLE:
            self.yolo_loader = YOLOModelLoader(explain_mode=explain_mode)
            self.yolo_optimizer = YOLOOptimizer(explain_mode=explain_mode)
            if explain_mode:
                print("ğŸ¯ YOLO ëª¨ë¸ ëª¨ë“œê°€ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        elif model_type == 'yolo' and not YOLO_AVAILABLE:
            print("âš ï¸  YOLO ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´ ultralytics íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            print("   ì„¤ì¹˜ ë°©ë²•: pip install ultralytics")
            self.model_type = 'simple'
    
    def setup_korean_font(self):
        """í•œê¸€ í°íŠ¸ ì„¤ì •"""
        try:
            if self.platform == 'Darwin':  # macOS
                font_paths = [
                    '/Library/Fonts/AppleGothic.ttf',
                    '/System/Library/Fonts/AppleSDGothicNeo.ttc',
                    '/Library/Fonts/NanumGothic.ttf',
                    '/Library/Fonts/NanumBarunGothic.ttf'
                ]
                
                font_found = False
                for font_path in font_paths:
                    if os.path.exists(font_path):
                        font_prop = fm.FontProperties(fname=font_path)
                        plt.rcParams['font.family'] = font_prop.get_name()
                        plt.rcParams['axes.unicode_minus'] = False
                        font_found = True
                        print(f"âœ… í•œê¸€ í°íŠ¸ ì„¤ì •: {font_prop.get_name()}")
                        break
                
                if not font_found:
                    print("âš ï¸  í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì°¨íŠ¸ì˜ í•œê¸€ì´ ê¹¨ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    plt.rcParams['font.family'] = 'DejaVu Sans'
                    
            elif self.platform == 'Windows':
                plt.rcParams['font.family'] = 'Malgun Gothic'
                plt.rcParams['axes.unicode_minus'] = False
                
            else:  # Linux
                plt.rcParams['font.family'] = 'NanumGothic'
                plt.rcParams['axes.unicode_minus'] = False
                
        except Exception as e:
            print(f"âš ï¸  í°íŠ¸ ì„¤ì • ì¤‘ ì˜¤ë¥˜: {e}")
            # Fallback to English labels
            self.use_english_labels = True
        else:
            self.use_english_labels = False
    
    def print_ascii_art(self, art_type):
        """ASCII ì•„íŠ¸ë¡œ ìµœì í™” ë°©ë²• ì‹œê°í™”"""
        if art_type == "quantization":
            print("ğŸ¨ ì–‘ìí™” ê³¼ì • ì‹œê°í™”:")
            print("    32-bit ê°€ì¤‘ì¹˜     â†’     8-bit ê°€ì¤‘ì¹˜")
            print("    [0.123456789]     â†’     [123]")
            print("    [0.987654321]     â†’     [246]")
            print("    [0.555555555]     â†’     [138]")
            print("    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â†’   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
            print("    â”‚  4 bytes    â”‚   â†’   â”‚ 1 byte  â”‚")
            print("    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â†’   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
            print("    ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 75% ê°ì†Œ!")
        
        elif art_type == "pruning":
            print("ğŸ¨ ê°€ì§€ì¹˜ê¸° ê³¼ì • ì‹œê°í™”:")
            print("    ì›ë³¸ ì‹ ê²½ë§           â†’     ê°€ì§€ì¹˜ê¸°ëœ ì‹ ê²½ë§")
            print("    â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—         â†’     â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—")
            print("    â”‚â•² â•±â”‚â•² â•±â”‚â•² â•±â”‚         â†’     â”‚   â”‚   â”‚   â”‚")
            print("    â”‚ â•³ â”‚ â•³ â”‚ â•³ â”‚         â†’     â”‚   â”‚   â”‚   â”‚")
            print("    â”‚â•± â•²â”‚â•± â•²â”‚â•± â•²â”‚         â†’     â”‚   â”‚   â”‚   â”‚")
            print("    â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—         â†’     â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—")
            print("    ğŸ”— ì—°ê²°: 16ê°œ          â†’     ğŸ”— ì—°ê²°: 8ê°œ")
            print("    âœ‚ï¸ ì¤‘ìš”í•˜ì§€ ì•Šì€ ì—°ê²° ì œê±°!")
        
        elif art_type == "distillation":
            print("ğŸ¨ ì§€ì‹ ì¦ë¥˜ ê³¼ì • ì‹œê°í™”:")
            print("    êµì‚¬ ëª¨ë¸ (í° ëª¨ë¸)     â†’     í•™ìƒ ëª¨ë¸ (ì‘ì€ ëª¨ë¸)")
            print("    â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—       â†’     â—â”€â”€â”€â—â”€â”€â”€â—")
            print("    â”‚â•² â•±â”‚â•² â•±â”‚â•² â•±â”‚â•² â•±â”‚       â†’     â”‚â•² â•±â”‚â•² â•±â”‚")
            print("    â”‚ â•³ â”‚ â•³ â”‚ â•³ â”‚ â•³ â”‚       â†’     â”‚ â•³ â”‚ â•³ â”‚")
            print("    â”‚â•± â•²â”‚â•± â•²â”‚â•± â•²â”‚â•± â•²â”‚       â†’     â”‚â•± â•²â”‚â•± â•²â”‚")
            print("    â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—       â†’     â—â”€â”€â”€â—â”€â”€â”€â—")
            print("    ğŸ§  ì§€ì‹ ì „ë‹¬: í° ëª¨ë¸ â†’ ì‘ì€ ëª¨ë¸")
            print("    ğŸ“š ì„±ëŠ¥ ìœ ì§€í•˜ë©´ì„œ í¬ê¸° ê°ì†Œ!")
        
        print()
    
    def explain_concept(self, concept):
        """ê°œë… ì„¤ëª…"""
        if not self.explain_mode:
            return
            
        explanations = {
            "quantization": """
ğŸ“– ì–‘ìí™”ë€?
â€¢ ì •ì˜: 32ë¹„íŠ¸ ë¶€ë™ì†Œìˆ˜ì  â†’ 8ë¹„íŠ¸ ì •ìˆ˜ë¡œ ë³€í™˜
â€¢ ë¹„ìœ : ê³ í™”ì§ˆ ì‚¬ì§„ì„ ì••ì¶•í•˜ì—¬ ìš©ëŸ‰ ì¤„ì´ê¸°
â€¢ ì¥ì : ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 4ë°° ê°ì†Œ, ì¶”ë¡  ì†ë„ í–¥ìƒ
â€¢ ë‹¨ì : ì •í™•ë„ ì•½ê°„ ê°ì†Œ ê°€ëŠ¥
""",
            "pruning": """
ğŸ“– ê°€ì§€ì¹˜ê¸°ë€?
â€¢ ì •ì˜: ì¤‘ìš”í•˜ì§€ ì•Šì€ ì‹ ê²½ë§ ì—°ê²° ì œê±°
â€¢ ë¹„ìœ : ë‚˜ë¬´ ê°€ì§€ì¹˜ê¸° - ë¶ˆí•„ìš”í•œ ê°€ì§€ ì œê±°
â€¢ ì¥ì : ëª¨ë¸ í¬ê¸° ê°ì†Œ, ì¶”ë¡  ì†ë„ í–¥ìƒ
â€¢ ë‹¨ì : ê³¼ë„í•œ ê°€ì§€ì¹˜ê¸° ì‹œ ì„±ëŠ¥ ì €í•˜
""",
            "distillation": """
ğŸ“– ì§€ì‹ ì¦ë¥˜ë€?
â€¢ ì •ì˜: í° ëª¨ë¸(êµì‚¬)ì˜ ì§€ì‹ì„ ì‘ì€ ëª¨ë¸(í•™ìƒ)ì—ê²Œ ì „ë‹¬
â€¢ ë¹„ìœ : ì„ ìƒë‹˜ì´ í•™ìƒì—ê²Œ í•µì‹¬ ì§€ì‹ ì „ìˆ˜
â€¢ ì¥ì : ì‘ì€ ëª¨ë¸ë¡œ í° ëª¨ë¸ ì„±ëŠ¥ ê·¼ì‚¬
â€¢ ë‹¨ì : ì¶”ê°€ í›ˆë ¨ ì‹œê°„ í•„ìš”
"""
        }
        
        if concept in explanations:
            print(explanations[concept])
    
    def visualize_model_structure(self, model, name="ëª¨ë¸"):
        """ëª¨ë¸ êµ¬ì¡° ì‹œê°í™”"""
        print(f"\nğŸ—ï¸ {name} êµ¬ì¡°:")
        print("=" * 50)
        
        total_params = 0
        for name, param in model.named_parameters():
            param_count = param.numel()
            total_params += param_count
            print(f"ğŸ“Š {name:15} | í˜•íƒœ: {str(param.shape):20} | íŒŒë¼ë¯¸í„°: {param_count:,}")
        
        print("=" * 50)
        print(f"ğŸ”¢ ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚°
        memory_mb = total_params * 4 / (1024 * 1024)  # 32-bit float
        print(f"ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_mb:.2f} MB")
        
        if self.explain_mode:
            print("\nğŸ’¡ êµ¬ì¡° ì„¤ëª…:")
            print("â€¢ fc1: ì…ë ¥ì¸µ â†’ ì²« ë²ˆì§¸ ì€ë‹‰ì¸µ (784 â†’ 256/64)")
            print("â€¢ fc2: ì²« ë²ˆì§¸ ì€ë‹‰ì¸µ â†’ ë‘ ë²ˆì§¸ ì€ë‹‰ì¸µ (256/64 â†’ 256/64)")
            print("â€¢ fc3: ë‘ ë²ˆì§¸ ì€ë‹‰ì¸µ â†’ ì¶œë ¥ì¸µ (256/64 â†’ 10)")
            print("â€¢ ReLU: ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜")
        
        print()
    
    def plot_weight_histogram(self, model, name="ëª¨ë¸"):
        """ê°€ì¤‘ì¹˜ íˆìŠ¤í† ê·¸ë¨ ì‹œê°í™”"""
        if not self.explain_mode:
            return
            
        print(f"\nğŸ“Š {name} ê°€ì¤‘ì¹˜ ë¶„í¬ ë¶„ì„:")
        
        # ëª¨ë“  ê°€ì¤‘ì¹˜ë¥¼ ìˆ˜ì§‘
        all_weights = []
        layer_weights = {}
        
        for name, param in model.named_parameters():
            if 'weight' in name:
                weights = param.data.cpu().numpy().flatten()
                all_weights.extend(weights)
                layer_weights[name] = weights
        
        # ê¸°ë³¸ í†µê³„ ì¶œë ¥
        import numpy as np
        all_weights = np.array(all_weights)
        
        print(f"ğŸ“ˆ ì „ì²´ ê°€ì¤‘ì¹˜ í†µê³„:")
        print(f"  â€¢ í‰ê· : {np.mean(all_weights):.4f}")
        print(f"  â€¢ í‘œì¤€í¸ì°¨: {np.std(all_weights):.4f}")
        print(f"  â€¢ ìµœì†Ÿê°’: {np.min(all_weights):.4f}")
        print(f"  â€¢ ìµœëŒ“ê°’: {np.max(all_weights):.4f}")
        print(f"  â€¢ 0ì— ê°€ê¹Œìš´ ê°’ (ì ˆëŒ“ê°’ < 0.01): {np.sum(np.abs(all_weights) < 0.01):,}ê°œ ({np.sum(np.abs(all_weights) < 0.01)/len(all_weights)*100:.1f}%)")
        
        # ë ˆì´ì–´ë³„ í†µê³„
        print(f"\nğŸ“Š ë ˆì´ì–´ë³„ ê°€ì¤‘ì¹˜ ë¶„í¬:")
        for layer_name, weights in layer_weights.items():
            print(f"  â€¢ {layer_name}: í‰ê· ={np.mean(weights):.4f}, í‘œì¤€í¸ì°¨={np.std(weights):.4f}")
        
        # ì‹œê°í™” ìƒì„± (matplotlibê°€ ìˆëŠ” ê²½ìš°)
        try:
            fig, axes = plt.subplots(2, 2, figsize=(12, 8))
            fig.suptitle(f'{name} ê°€ì¤‘ì¹˜ ë¶„í¬', fontsize=16)
            
            # ì „ì²´ ê°€ì¤‘ì¹˜ íˆìŠ¤í† ê·¸ë¨
            axes[0, 0].hist(all_weights, bins=50, alpha=0.7, color='blue')
            axes[0, 0].set_title('ì „ì²´ ê°€ì¤‘ì¹˜ ë¶„í¬')
            axes[0, 0].set_xlabel('ê°€ì¤‘ì¹˜ ê°’')
            axes[0, 0].set_ylabel('ë¹ˆë„')
            axes[0, 0].axvline(0, color='red', linestyle='--', alpha=0.7)
            
            # 0 ê·¼ì²˜ í™•ëŒ€
            small_weights = all_weights[np.abs(all_weights) < 0.1]
            axes[0, 1].hist(small_weights, bins=50, alpha=0.7, color='green')
            axes[0, 1].set_title('0 ê·¼ì²˜ ê°€ì¤‘ì¹˜ ë¶„í¬ (ì ˆëŒ“ê°’ < 0.1)')
            axes[0, 1].set_xlabel('ê°€ì¤‘ì¹˜ ê°’')
            axes[0, 1].set_ylabel('ë¹ˆë„')
            axes[0, 1].axvline(0, color='red', linestyle='--', alpha=0.7)
            
            # ë ˆì´ì–´ë³„ ë¹„êµ (ì²˜ìŒ ë‘ ë ˆì´ì–´)
            layer_names = list(layer_weights.keys())[:2]
            for i, layer_name in enumerate(layer_names):
                weights = layer_weights[layer_name]
                axes[1, i].hist(weights, bins=30, alpha=0.7, color=['orange', 'purple'][i])
                axes[1, i].set_title(f'{layer_name} ê°€ì¤‘ì¹˜ ë¶„í¬')
                axes[1, i].set_xlabel('ê°€ì¤‘ì¹˜ ê°’')
                axes[1, i].set_ylabel('ë¹ˆë„')
                axes[1, i].axvline(0, color='red', linestyle='--', alpha=0.7)
            
            plt.tight_layout()
            plt.show()
            
        except Exception as e:
            print(f"  âš ï¸ íˆìŠ¤í† ê·¸ë¨ ì‹œê°í™” ì˜¤ë¥˜: {e}")
        
        print()
    
    def create_dummy_data(self, num_samples=1000, device=None):
        """í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ë°ì´í„° ìƒì„±"""
        if device is None:
            device = 'cpu'  # Default to CPU to avoid issues with quantization
        X = torch.randn(num_samples, 784, device=device)
        y = torch.randint(0, 10, (num_samples,), device=device)
        return DataLoader(TensorDataset(X, y), batch_size=32, shuffle=True)
    
    def measure_performance(self, model, dataloader, name="Model", model_type=None):
        """ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •"""
        print(f"\nâ±ï¸  {name} ì„±ëŠ¥ ì¸¡ì • ì¤‘...")
        model.eval()
        
        # í¬ê¸° ì¸¡ì • - YOLO ëª¨ë¸ê³¼ ì¼ë°˜ ëª¨ë¸ êµ¬ë¶„
        print("  - ëª¨ë¸ í¬ê¸° ê³„ì‚° ì¤‘...")
        
        # ëª¨ë¸ íƒ€ì… ìë™ ê°ì§€
        if model_type is None:
            model_type = self.model_type
        
        if model_type == 'yolo' and hasattr(model, 'model'):
            # YOLO ëª¨ë¸ì˜ ê²½ìš° íŒŒë¼ë¯¸í„° ìˆ˜ ê¸°ë°˜ ê³„ì‚°
            pytorch_model = model.model
            total_params = sum(p.numel() for p in pytorch_model.parameters())
            model_size = total_params * 4 / (1024 * 1024)  # 32-bit float assumption
            print(f"  - YOLO ëª¨ë¸ ({total_params:,} íŒŒë¼ë¯¸í„°)")
        else:
            # ì¼ë°˜ ëª¨ë¸ì˜ ê²½ìš° state_dict ì €ì¥ ë°©ì‹
            torch.save(model.state_dict(), 'temp_model.pth')
            model_size = os.path.getsize('temp_model.pth') / (1024 * 1024)  # MB
            os.remove('temp_model.pth')
            print(f"  - ì¼ë°˜ ëª¨ë¸ (state_dict ê¸°ë°˜)")
        
        # ì†ë„ ì¸¡ì •
        print("  - ì¶”ë¡  ì†ë„ ì¸¡ì • ì¤‘...")
        total_time = 0
        num_batches = 0
        
        with torch.no_grad():
            for X, _ in dataloader:
                # Move input to the same device as the model
                # More robust quantized model detection
                is_quantized = (
                    'quantized' in str(type(model)).lower() or 
                    hasattr(model, '_is_quantized') or
                    any('quantized' in str(type(m)).lower() for m in model.modules()) or
                    any(hasattr(m, '_packed_params') for m in model.modules())  # Common in quantized layers
                )
                
                if is_quantized:
                    # Quantized models MUST run on CPU
                    model_device = torch.device('cpu')
                    # Ensure model is on CPU
                    if hasattr(model, 'to'):
                        model = model.cpu()
                else:
                    try:
                        model_device = next(model.parameters()).device
                    except StopIteration:
                        # If model has no parameters, default to CPU for safety
                        model_device = torch.device('cpu')
                
                # Move input to model device
                X = X.to(model_device)
                
                start_time = time.time()
                _ = model(X)
                
                # GPU ë™ê¸°í™” (ì •í™•í•œ ì‹œê°„ ì¸¡ì •)
                if model_device.type == 'cuda':
                    torch.cuda.synchronize()
                
                total_time += time.time() - start_time
                num_batches += 1
                
                if num_batches >= 10:  # 10ë°°ì¹˜ë§Œ í…ŒìŠ¤íŠ¸
                    break
        
        avg_inference_time = total_time / num_batches * 1000  # ms
        
        print(f"\n{name} ì„±ëŠ¥:")
        print(f"  - ëª¨ë¸ í¬ê¸°: {model_size:.2f} MB")
        print(f"  - í‰ê·  ì¶”ë¡  ì‹œê°„: {avg_inference_time:.2f} ms/batch")
        
        return model_size, avg_inference_time
    
    def quantize_dynamic(self, model):
        """ë™ì  ì–‘ìí™” ì ìš©"""
        print("\nğŸ”„ ë™ì  ì–‘ìí™” ì ìš© ì¤‘...")
        
        # êµìœ¡ ëª¨ë“œì—ì„œ ì„¤ëª… ë° ì‹œê°í™”
        if self.explain_mode:
            self.explain_concept("quantization")
            self.print_ascii_art("quantization")
        
        try:
            print("  ğŸ“‹ 1ë‹¨ê³„: ëª¨ë¸ ë¶„ì„ ì¤‘...")
            if self.explain_mode:
                print("    â€¢ Linear ë ˆì´ì–´ë¥¼ ì°¾ì•„ ì–‘ìí™” ëŒ€ìƒ ì„ ì •")
                print("    â€¢ í˜„ì¬ ê°€ì¤‘ì¹˜ í˜•íƒœ: 32-bit ë¶€ë™ì†Œìˆ˜ì ")
            
            print("  ğŸ”§ 2ë‹¨ê³„: ë™ì  ì–‘ìí™” ì ìš©...")
            if self.explain_mode:
                print("    â€¢ ëŸ°íƒ€ì„ì— ë™ì ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ 8-bitë¡œ ë³€í™˜")
                print("    â€¢ ì…ë ¥ ë°ì´í„°ì— ë”°ë¼ ìŠ¤ì¼€ì¼ ì¡°ì •")
            
            quantized_model = torch.quantization.quantize_dynamic(
                model,
                {nn.Linear},  # Linear ë ˆì´ì–´ë§Œ ì–‘ìí™”
                dtype=torch.qint8
            )
            
            # Ensure quantized model stays on CPU
            quantized_model = quantized_model.cpu()
            
            print("  âœ… 3ë‹¨ê³„: ì–‘ìí™” ì™„ë£Œ!")
            if self.explain_mode:
                print("    â€¢ ê°€ì¤‘ì¹˜ê°€ 8-bit ì •ìˆ˜ë¡œ ë³€í™˜ë¨")
                print("    â€¢ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì•½ 4ë°° ê°ì†Œ")
                print("    â€¢ ì¶”ë¡  ì†ë„ í–¥ìƒ")
                print("    â€¢ ëª¨ë¸ì´ CPUì—ì„œ ì‹¤í–‰ë¨ (ì–‘ìí™” ìš”êµ¬ì‚¬í•­)")
            
            return quantized_model
        except RuntimeError as e:
            if "NoQEngine" in str(e) or "quantized" in str(e):
                print("  âš ï¸  ì–‘ìí™”ê°€ ì´ í”Œë«í¼ì—ì„œ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (macOS/ARM).")
                print("  â„¹ï¸  ëŒ€ì‹  ê°€ì§€ì¹˜ê¸°ì™€ ì§€ì‹ ì¦ë¥˜ë¥¼ ì‚¬ìš©í•´ ë³´ì„¸ìš”.")
                return None
            else:
                raise
    
    def quantize_static(self, model, dataloader):
        """ì •ì  ì–‘ìí™” ì ìš©"""
        print("\nì •ì  ì–‘ìí™” ì ìš© ì¤‘...")
        
        try:
            # ëª¨ë¸ ì¤€ë¹„
            model.eval()
            model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
            
            # í“¨ì „ ê°€ëŠ¥í•œ ëª¨ë“ˆ ê²°í•©
            model_fused = torch.quantization.fuse_modules(
                model, 
                [['fc1', 'relu1'], ['fc2', 'relu2']]
            )
            
            # ì–‘ìí™” ì¤€ë¹„
            model_prepared = torch.quantization.prepare(model_fused)
            
            # ìº˜ë¦¬ë¸Œë ˆì´ì…˜
            print("  ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì‹¤í–‰ ì¤‘...")
            with torch.no_grad():
                for i, (X, _) in enumerate(dataloader):
                    model_prepared(X)
                    print(f"    - ë°°ì¹˜ {i+1}/10 ì²˜ë¦¬ ì¤‘...", end='\r')
                    if i >= 10:  # 10ë°°ì¹˜ë¡œ ìº˜ë¦¬ë¸Œë ˆì´ì…˜
                        break
            print("    - ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì™„ë£Œ!    ")
            
            # ì–‘ìí™” ë³€í™˜
            model_quantized = torch.quantization.convert(model_prepared)
            
            return model_quantized
        except RuntimeError as e:
            if "NoQEngine" in str(e) or "quantized" in str(e):
                print("  âš ï¸  ì •ì  ì–‘ìí™”ê°€ ì´ í”Œë«í¼ì—ì„œ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return None
            else:
                raise
    
    def prune_model(self, model, sparsity=0.5):
        """ê°€ì§€ì¹˜ê¸° ì ìš©"""
        print(f"\nâœ‚ï¸ ê°€ì§€ì¹˜ê¸° ì ìš© ì¤‘... (í¬ì†Œì„±: {sparsity*100}%)")
        
        # êµìœ¡ ëª¨ë“œì—ì„œ ì„¤ëª… ë° ì‹œê°í™”
        if self.explain_mode:
            self.explain_concept("pruning")
            self.print_ascii_art("pruning")
        
        import torch.nn.utils.prune as prune
        
        print("  ğŸ“‹ 1ë‹¨ê³„: ê°€ì¤‘ì¹˜ ì¤‘ìš”ë„ ë¶„ì„...")
        if self.explain_mode:
            print("    â€¢ L1 normì„ ì‚¬ìš©í•´ ê°€ì¤‘ì¹˜ ì¤‘ìš”ë„ ê³„ì‚°")
            print("    â€¢ ì‘ì€ ê°€ì¤‘ì¹˜ë“¤ì€ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì´ ì ìŒ")
        
        # ê° Linear ë ˆì´ì–´ì— ê°€ì§€ì¹˜ê¸° ì ìš©
        layer_count = 0
        for name, module in model.named_modules():
            if isinstance(module, nn.Linear):
                layer_count += 1
                
        print(f"  âœ‚ï¸ 2ë‹¨ê³„: {layer_count}ê°œ ë ˆì´ì–´ì— ê°€ì§€ì¹˜ê¸° ì ìš©...")
        current_layer = 0
        for name, module in model.named_modules():
            if isinstance(module, nn.Linear):
                current_layer += 1
                if self.explain_mode:
                    print(f"    â€¢ {name} ë ˆì´ì–´: {sparsity*100}%ì˜ ê°€ì¤‘ì¹˜ ì œê±° ì¤‘...")
                prune.l1_unstructured(module, name='weight', amount=sparsity)
        
        print("  ğŸ”§ 3ë‹¨ê³„: ê°€ì§€ì¹˜ê¸° ì˜êµ¬ ì ìš©...")
        if self.explain_mode:
            print("    â€¢ ë§ˆìŠ¤í¬ë¥¼ ì ìš©í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ ì‹¤ì œë¡œ ì œê±°")
            print("    â€¢ ëª¨ë¸ êµ¬ì¡°ëŠ” ìœ ì§€í•˜ë˜ ì—°ê²°ì´ ëŠì–´ì§")
        
        # ê°€ì§€ì¹˜ê¸° ì˜êµ¬ ì ìš©
        for name, module in model.named_modules():
            if isinstance(module, nn.Linear):
                prune.remove(module, 'weight')
        
        print("  âœ… ê°€ì§€ì¹˜ê¸° ì™„ë£Œ!")
        if self.explain_mode:
            print(f"    â€¢ ì „ì²´ ê°€ì¤‘ì¹˜ì˜ {sparsity*100}%ê°€ ì œê±°ë¨")
            print("    â€¢ ëª¨ë¸ í¬ê¸° ê°ì†Œ, ì¶”ë¡  ì†ë„ í–¥ìƒ")
            self.plot_weight_histogram(model, f"ê°€ì§€ì¹˜ê¸° í›„ ëª¨ë¸ ({sparsity*100}% ì œê±°)")
        
        return model
    
    def knowledge_distillation(self, teacher_model, student_model, dataloader, epochs=5):
        """ì§€ì‹ ì¦ë¥˜"""
        print("\nğŸ“ ì§€ì‹ ì¦ë¥˜ ìˆ˜í–‰ ì¤‘...")
        
        # êµìœ¡ ëª¨ë“œì—ì„œ ì„¤ëª… ë° ì‹œê°í™”
        if self.explain_mode:
            self.explain_concept("distillation")
            self.print_ascii_art("distillation")
        
        print("  ğŸ“‹ 1ë‹¨ê³„: í•™ìƒ ëª¨ë¸ ì¤€ë¹„...")
        # í•™ìƒ ëª¨ë¸ (ë” ì‘ì€ ëª¨ë¸)
        small_model = OptimizableModel(hidden_size=64)  # ë” ì‘ì€ íˆë“  í¬ê¸°
        small_model = small_model.to(self.device)
        
        if self.explain_mode:
            print("    â€¢ êµì‚¬ ëª¨ë¸: 256ê°œ íˆë“  ë‰´ëŸ°")
            print("    â€¢ í•™ìƒ ëª¨ë¸: 64ê°œ íˆë“  ë‰´ëŸ° (4ë°° ì‘ìŒ)")
            self.visualize_model_structure(teacher_model, "êµì‚¬ ëª¨ë¸")
            self.visualize_model_structure(small_model, "í•™ìƒ ëª¨ë¸")
        
        optimizer = torch.optim.Adam(small_model.parameters(), lr=0.001)
        
        print("  ğŸ”§ 2ë‹¨ê³„: ì¦ë¥˜ ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •...")
        # ì¦ë¥˜ ì†ì‹¤ í•¨ìˆ˜
        def distillation_loss(student_logits, teacher_logits, temperature=3.0):
            soft_targets = nn.functional.softmax(teacher_logits / temperature, dim=1)
            soft_predictions = nn.functional.log_softmax(student_logits / temperature, dim=1)
            return nn.functional.kl_div(soft_predictions, soft_targets, reduction='batchmean') * temperature * temperature
        
        if self.explain_mode:
            print("    â€¢ KL Divergence ì†ì‹¤ ì‚¬ìš©")
            print("    â€¢ Temperature scalingìœ¼ë¡œ soft targets ìƒì„±")
        
        teacher_model.eval()
        
        print(f"  ğŸ¯ 3ë‹¨ê³„: {epochs}ë²ˆì˜ ì—í¬í¬ë¡œ í•™ìŠµ...")
        for epoch in range(epochs):
            total_loss = 0
            batch_count = 0
            print(f"\n  ğŸ“š Epoch {epoch+1}/{epochs}:")
            if self.explain_mode:
                print("    â€¢ êµì‚¬ ëª¨ë¸ì˜ ì§€ì‹ì„ í•™ìƒ ëª¨ë¸ì—ê²Œ ì „ìˆ˜")
            
            for X, y in dataloader:
                X = X.to(self.device)
                
                # êµì‚¬ ëª¨ë¸ ì˜ˆì¸¡
                with torch.no_grad():
                    teacher_logits = teacher_model(X)
                
                # í•™ìƒ ëª¨ë¸ ì˜ˆì¸¡
                student_logits = small_model(X)
                
                # ì†ì‹¤ ê³„ì‚°
                loss = distillation_loss(student_logits, teacher_logits)
                
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
                batch_count += 1
                if batch_count % 5 == 0:
                    print(f"    - ë°°ì¹˜ {batch_count}/{len(dataloader)} ì²˜ë¦¬ ì¤‘...", end='\r')
            
            avg_loss = total_loss/len(dataloader)
            print(f"    - í‰ê·  ì†ì‹¤: {avg_loss:.4f}                    ")
            
            if self.explain_mode and epoch == 0:
                print("    â€¢ ì†ì‹¤ì´ ê°ì†Œí• ìˆ˜ë¡ í•™ìƒì´ êµì‚¬ë¥¼ ë” ì˜ ëª¨ë°©")
        
        print("  âœ… ì§€ì‹ ì¦ë¥˜ ì™„ë£Œ!")
        if self.explain_mode:
            print("    â€¢ ì‘ì€ ëª¨ë¸ì´ í° ëª¨ë¸ì˜ ì§€ì‹ì„ ì„±ê³µì ìœ¼ë¡œ í•™ìŠµ")
            print("    â€¢ í¬ê¸°ëŠ” ì‘ì§€ë§Œ ë¹„ìŠ·í•œ ì„±ëŠ¥ ë‹¬ì„±")
        
        return small_model
    
    def save_model(self, model, filepath, model_type="optimized"):
        """ìµœì í™”ëœ ëª¨ë¸ ì €ì¥"""
        print(f"\nğŸ’¾ {model_type} ëª¨ë¸ì„ {filepath}ì— ì €ì¥ ì¤‘...")
        torch.save({
            'model_state_dict': model.state_dict(),
            'model_type': model_type,
            'saved_time': time.strftime('%Y-%m-%d %H:%M:%S')
        }, filepath)
        file_size = os.path.getsize(filepath) / (1024 * 1024)
        print(f"   âœ… ì €ì¥ ì™„ë£Œ! (í¬ê¸°: {file_size:.2f} MB)")
        return file_size
    
    def load_model(self, filepath, model_class=None):
        """ì €ì¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°"""
        print(f"\nğŸ“‚ {filepath}ì—ì„œ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
        checkpoint = torch.load(filepath)
        
        if model_class is None:
            model_class = OptimizableModel
        
        model = model_class()
        model.load_state_dict(checkpoint['model_state_dict'])
        
        print(f"   âœ… ëª¨ë¸ íƒ€ì…: {checkpoint.get('model_type', 'unknown')}")
        print(f"   âœ… ì €ì¥ ì‹œê°„: {checkpoint.get('saved_time', 'unknown')}")
        
        return model
    
    def compile_model(self, model):
        """ëª¨ë¸ ì»´íŒŒì¼ (PyTorch 2.0+)"""
        if hasattr(torch, 'compile'):
            print("\nëª¨ë¸ ì»´íŒŒì¼ ì¤‘...")
            compiled_model = torch.compile(model, mode="reduce-overhead")
            return compiled_model
        else:
            print("\nPyTorch 2.0 ì´ìƒì´ í•„ìš”í•©ë‹ˆë‹¤.")
            return model
    
    def compare_all_optimizations(self):
        """ëª¨ë“  ìµœì í™” ê¸°ë²• ë¹„êµ"""
        print("\n=== ëª¨ë¸ ìµœì í™” ì¢…í•© ë¹„êµ ===\n")
        
        # ë°ì´í„° ì¤€ë¹„
        dataloader = self.create_dummy_data()
        
        # ì›ë³¸ ëª¨ë¸
        original_model = OptimizableModel().to(self.device)
        original_size, original_time = self.measure_performance(
            original_model, dataloader, "ì›ë³¸ ëª¨ë¸"
        )
        
        results = {
            'Original': {'size': original_size, 'time': original_time}
        }
        
        # 1. ë™ì  ì–‘ìí™”
        if self.device.type == 'cpu':
            dynamic_quantized = self.quantize_dynamic(original_model.cpu())
            if dynamic_quantized is not None:
                size, time = self.measure_performance(
                    dynamic_quantized, dataloader, "ë™ì  ì–‘ìí™”"
                )
                results['Dynamic Quantization'] = {'size': size, 'time': time}
            else:
                print("  â­ï¸  ë™ì  ì–‘ìí™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        
        # 2. ê°€ì§€ì¹˜ê¸°
        import copy
        pruned_model = self.prune_model(copy.deepcopy(original_model), sparsity=0.5)
        size, time = self.measure_performance(
            pruned_model, dataloader, "ê°€ì§€ì¹˜ê¸° (50%)"
        )
        results['Pruning'] = {'size': size, 'time': time}
        
        # 3. ì§€ì‹ ì¦ë¥˜
        student_model = self.knowledge_distillation(
            original_model, None, dataloader
        )
        size, time = self.measure_performance(
            student_model, dataloader, "ì§€ì‹ ì¦ë¥˜ (ì‘ì€ ëª¨ë¸)"
        )
        results['Knowledge Distillation'] = {'size': size, 'time': time}
        
        # ê²°ê³¼ ì‹œê°í™”
        self.visualize_comparison(results)
        
        # ê²°ê³¼ í‘œ ì¶œë ¥
        self.print_summary_table(results)
        
        return results
    
    def visualize_comparison(self, results):
        """ìµœì í™” ê²°ê³¼ ë¹„êµ ì‹œê°í™”"""
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        methods = list(results.keys())
        sizes = [results[m]['size'] for m in methods]
        times = [results[m]['time'] for m in methods]
        
        # English labels if Korean font not available
        if hasattr(self, 'use_english_labels') and self.use_english_labels:
            size_label = 'Model Size (MB)'
            size_title = 'Model Size Comparison'
            time_label = 'Inference Time (ms/batch)'
            time_title = 'Inference Speed Comparison'
        else:
            size_label = 'ëª¨ë¸ í¬ê¸° (MB)'
            size_title = 'ëª¨ë¸ í¬ê¸° ë¹„êµ'
            time_label = 'ì¶”ë¡  ì‹œê°„ (ms/batch)'
            time_title = 'ì¶”ë¡  ì†ë„ ë¹„êµ'
        
        # ëª¨ë¸ í¬ê¸° ë¹„êµ
        bars1 = ax1.bar(methods, sizes, color=['red', 'green', 'blue', 'orange'][:len(methods)])
        ax1.set_ylabel(size_label)
        ax1.set_title(size_title, fontweight='bold')
        ax1.set_xticklabels(methods, rotation=45, ha='right')
        
        # í¬ê¸° ê°ì†Œìœ¨ í‘œì‹œ
        for i, bar in enumerate(bars1):
            if i > 0:
                reduction = (sizes[0] - sizes[i]) / sizes[0] * 100
                ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                        f'-{reduction:.0f}%', ha='center', fontweight='bold')
        
        # ì¶”ë¡  ì‹œê°„ ë¹„êµ
        bars2 = ax2.bar(methods, times, color=['red', 'green', 'blue', 'orange'][:len(methods)])
        ax2.set_ylabel(time_label)
        ax2.set_title(time_title, fontweight='bold')
        ax2.set_xticklabels(methods, rotation=45, ha='right')
        
        # ì†ë„ í–¥ìƒ í‘œì‹œ
        for i, bar in enumerate(bars2):
            if i > 0:
                speedup = times[0] / times[i]
                ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.2,
                        f'{speedup:.1f}x', ha='center', fontweight='bold')
        
        plt.tight_layout()
        plt.show()
    
    def print_summary_table(self, results):
        """ê²°ê³¼ ìš”ì•½ í‘œ ì¶œë ¥"""
        print("\n\nğŸ“Š === ìµœì í™” ê²°ê³¼ ìš”ì•½ í‘œ === ğŸ“Š")
        print("-" * 85)
        print(f"{'ë°©ë²•':^25} | {'ëª¨ë¸ í¬ê¸° (MB)':^20} | {'ì¶”ë¡  ì‹œê°„ (ms)':^20} | {'í¬ê¸° ê°ì†Œ':^15}")
        print("-" * 85)
        
        original_size = results['Original']['size']
        original_time = results['Original']['time']
        
        for method, data in results.items():
            size = data['size']
            time = data['time']
            
            if method == 'Original':
                size_reduction = "-"
                time_speedup = "-"
            else:
                size_reduction = f"{(original_size - size) / original_size * 100:.1f}%"
                time_speedup = f"{original_time / time:.1f}x"
            
            print(f"{method:^25} | {size:^20.2f} | {time:^20.2f} | {size_reduction:^15}")
        
        print("-" * 85)
        
        # ìµœì  ë°©ë²• ì°¾ê¸°
        best_size_method = min(results.items(), key=lambda x: x[1]['size'])[0]
        best_time_method = min(results.items(), key=lambda x: x[1]['time'])[0]
        
        print(f"\nâœ¨ ìµœì  ê²°ê³¼:")
        print(f"  - ê°€ì¥ ì‘ì€ ëª¨ë¸: {best_size_method} ({results[best_size_method]['size']:.2f} MB)")
        print(f"  - ê°€ì¥ ë¹ ë¥¸ ëª¨ë¸: {best_time_method} ({results[best_time_method]['time']:.2f} ms/batch)")
    
    def demo_dynamic_quantization(self):
        """ë™ì  ì–‘ìí™” ë°ëª¨"""
        print("\n=== ë™ì  ì–‘ìí™” ë°ëª¨ ===\n")
        
        # ë°ì´í„° ì¤€ë¹„ - CPUìš© ë°ì´í„°ë¡œë” ìƒì„± (ì–‘ìí™”ëŠ” CPUì—ì„œë§Œ ì‘ë™)
        dataloader = self.create_dummy_data(device='cpu')
        
        # ì›ë³¸ ëª¨ë¸
        original_model = OptimizableModel().cpu()  # ë™ì  ì–‘ìí™”ëŠ” CPUì—ì„œë§Œ ì‘ë™
        original_size, original_time = self.measure_performance(
            original_model, dataloader, "ì›ë³¸ ëª¨ë¸"
        )
        
        # ë™ì  ì–‘ìí™” ì ìš©
        quantized_model = self.quantize_dynamic(original_model)
        if quantized_model is not None:
            quantized_size, quantized_time = self.measure_performance(
                quantized_model, dataloader, "ë™ì  ì–‘ìí™” ëª¨ë¸"
            )
            
            # ê²°ê³¼ ìš”ì•½
            print(f"\nğŸ“Š ë™ì  ì–‘ìí™” ê²°ê³¼:")
            print(f"  - í¬ê¸° ê°ì†Œ: {original_size:.2f}MB â†’ {quantized_size:.2f}MB ({(original_size-quantized_size)/original_size*100:.1f}% ê°ì†Œ)")
            print(f"  - ì†ë„ í–¥ìƒ: {original_time:.2f}ms â†’ {quantized_time:.2f}ms ({original_time/quantized_time:.1f}x ë¹ ë¦„)")
            
            return quantized_model
        else:
            print("\nğŸ’¡ ì–‘ìí™” ëŒ€ì‹  ë‹¤ë¥¸ ìµœì í™” ê¸°ë²•ì„ ì‹œë„í•´ ë³´ì„¸ìš”.")
            return None
    
    def demo_pruning(self, sparsity=0.5):
        """ê°€ì§€ì¹˜ê¸° ë°ëª¨"""
        print(f"\n=== ê°€ì§€ì¹˜ê¸° ë°ëª¨ (í¬ì†Œì„± {sparsity*100}%) ===\n")
        
        # ë°ì´í„° ì¤€ë¹„
        dataloader = self.create_dummy_data()
        
        # ì›ë³¸ ëª¨ë¸
        original_model = OptimizableModel().to(self.device)
        original_size, original_time = self.measure_performance(
            original_model, dataloader, "ì›ë³¸ ëª¨ë¸"
        )
        
        # ê°€ì§€ì¹˜ê¸° ì ìš©
        import copy
        pruned_model = self.prune_model(copy.deepcopy(original_model), sparsity=sparsity)
        pruned_size, pruned_time = self.measure_performance(
            pruned_model, dataloader, f"ê°€ì§€ì¹˜ê¸° ëª¨ë¸ ({sparsity*100}%)"
        )
        
        # ê²°ê³¼ ìš”ì•½
        print(f"\nğŸ“Š ê°€ì§€ì¹˜ê¸° ê²°ê³¼:")
        print(f"  - í¬ê¸° ê°ì†Œ: {original_size:.2f}MB â†’ {pruned_size:.2f}MB ({(original_size-pruned_size)/original_size*100:.1f}% ê°ì†Œ)")
        print(f"  - ì†ë„ í–¥ìƒ: {original_time:.2f}ms â†’ {pruned_time:.2f}ms ({original_time/pruned_time:.1f}x ë¹ ë¦„)")
        
        return pruned_model
    
    def demo_knowledge_distillation(self):
        """ì§€ì‹ ì¦ë¥˜ ë°ëª¨"""
        print("\n=== ì§€ì‹ ì¦ë¥˜ ë°ëª¨ ===\n")
        
        # ë°ì´í„° ì¤€ë¹„
        dataloader = self.create_dummy_data()
        
        # êµì‚¬ ëª¨ë¸ (í° ëª¨ë¸)
        teacher_model = OptimizableModel(hidden_size=256).to(self.device)
        teacher_size, teacher_time = self.measure_performance(
            teacher_model, dataloader, "êµì‚¬ ëª¨ë¸ (í° ëª¨ë¸)"
        )
        
        # ì§€ì‹ ì¦ë¥˜ë¡œ í•™ìƒ ëª¨ë¸ í›ˆë ¨
        student_model = self.knowledge_distillation(teacher_model, None, dataloader)
        student_size, student_time = self.measure_performance(
            student_model, dataloader, "í•™ìƒ ëª¨ë¸ (ì‘ì€ ëª¨ë¸)"
        )
        
        # ê²°ê³¼ ìš”ì•½
        print(f"\nğŸ“Š ì§€ì‹ ì¦ë¥˜ ê²°ê³¼:")
        print(f"  - í¬ê¸° ê°ì†Œ: {teacher_size:.2f}MB â†’ {student_size:.2f}MB ({(teacher_size-student_size)/teacher_size*100:.1f}% ê°ì†Œ)")
        print(f"  - ì†ë„ í–¥ìƒ: {teacher_time:.2f}ms â†’ {student_time:.2f}ms ({teacher_time/student_time:.1f}x ë¹ ë¦„)")
        
        return student_model
    
    def demo_yolo_pruning(self, model_name='yolov8s', sparsity=0.5):
        """YOLO ëª¨ë¸ ê°€ì§€ì¹˜ê¸° ë°ëª¨"""
        if self.model_type != 'yolo':
            print("âš ï¸  YOLO ëª¨ë¸ ëª¨ë“œê°€ ì•„ë‹™ë‹ˆë‹¤.")
            return None
        
        print(f"\n=== YOLO ê°€ì§€ì¹˜ê¸° ë°ëª¨ ({model_name}) ===\n")
        
        # ì›ë³¸ ëª¨ë¸ ë¡œë“œ
        original_model = self.yolo_loader.load_model(model_name)
        original_info = self.yolo_loader.get_model_info(original_model)
        
        # ìƒ˜í”Œ ì´ë¯¸ì§€ ì¤€ë¹„
        images = self.yolo_loader.create_sample_images(5)
        
        # ì›ë³¸ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •
        print("ğŸ“Š ì›ë³¸ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •...")
        original_benchmark = self.yolo_loader.benchmark_inference(original_model, images, runs=5)
        
        # ê°€ì§€ì¹˜ê¸° ì ìš©
        pruned_model = self.yolo_optimizer.prune_yolo_model(original_model, sparsity=sparsity)
        pruned_info = self.yolo_loader.get_model_info(pruned_model)
        
        # ê°€ì§€ì¹˜ê¸°ëœ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •
        print("ğŸ“Š ê°€ì§€ì¹˜ê¸°ëœ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •...")
        pruned_benchmark = self.yolo_loader.benchmark_inference(pruned_model, images, runs=5)
        
        # ê²°ê³¼ ë¹„êµ
        print(f"\nğŸ“Š YOLO ê°€ì§€ì¹˜ê¸° ê²°ê³¼:")
        print(f"  - ëª¨ë¸ í¬ê¸°: {original_info['model_size_mb']:.2f}MB â†’ {pruned_info['model_size_mb']:.2f}MB")
        size_reduction = (original_info['model_size_mb'] - pruned_info['model_size_mb']) / original_info['model_size_mb'] * 100
        print(f"  - í¬ê¸° ê°ì†Œ: {size_reduction:.1f}%")
        print(f"  - FPS: {original_benchmark['fps']:.1f} â†’ {pruned_benchmark['fps']:.1f}")
        fps_change = (pruned_benchmark['fps'] - original_benchmark['fps']) / original_benchmark['fps'] * 100
        print(f"  - FPS ë³€í™”: {fps_change:+.1f}%")
        
        return pruned_model
    
    def demo_yolo_distillation(self, teacher_model='yolov8m', student_model='yolov8n'):
        """YOLO ì§€ì‹ ì¦ë¥˜ ë°ëª¨"""
        if self.model_type != 'yolo':
            print("âš ï¸  YOLO ëª¨ë¸ ëª¨ë“œê°€ ì•„ë‹™ë‹ˆë‹¤.")
            return None
        
        print(f"\n=== YOLO ì§€ì‹ ì¦ë¥˜ ë°ëª¨ ({teacher_model} â†’ {student_model}) ===\n")
        
        # êµì‚¬ ëª¨ë¸ ë¡œë“œ
        teacher = self.yolo_loader.load_model(teacher_model)
        teacher_info = self.yolo_loader.get_model_info(teacher)
        
        # í•™ìƒ ëª¨ë¸ ë¡œë“œ
        student = self.yolo_loader.load_model(student_model)
        student_info = self.yolo_loader.get_model_info(student)
        
        # ìƒ˜í”Œ ì´ë¯¸ì§€ ì¤€ë¹„
        images = self.yolo_loader.create_sample_images(5)
        
        # êµì‚¬ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •
        print("ğŸ“Š êµì‚¬ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •...")
        teacher_benchmark = self.yolo_loader.benchmark_inference(teacher, images, runs=5)
        
        # í•™ìƒ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •
        print("ğŸ“Š í•™ìƒ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •...")
        student_benchmark = self.yolo_loader.benchmark_inference(student, images, runs=5)
        
        # ê²°ê³¼ ë¹„êµ
        print(f"\nğŸ“Š YOLO ì§€ì‹ ì¦ë¥˜ ê²°ê³¼:")
        print(f"  - êµì‚¬ ëª¨ë¸ í¬ê¸°: {teacher_info['model_size_mb']:.2f}MB")
        print(f"  - í•™ìƒ ëª¨ë¸ í¬ê¸°: {student_info['model_size_mb']:.2f}MB")
        size_reduction = (teacher_info['model_size_mb'] - student_info['model_size_mb']) / teacher_info['model_size_mb'] * 100
        print(f"  - í¬ê¸° ê°ì†Œ: {size_reduction:.1f}%")
        print(f"  - êµì‚¬ FPS: {teacher_benchmark['fps']:.1f}")
        print(f"  - í•™ìƒ FPS: {student_benchmark['fps']:.1f}")
        speedup = student_benchmark['fps'] / teacher_benchmark['fps']
        print(f"  - ì†ë„ í–¥ìƒ: {speedup:.1f}x")
        
        return student
    
    def demo_yolo_comprehensive(self):
        """YOLO ì¢…í•© ìµœì í™” ë°ëª¨"""
        if self.model_type != 'yolo':
            print("âš ï¸  YOLO ëª¨ë¸ ëª¨ë“œê°€ ì•„ë‹™ë‹ˆë‹¤.")
            return None
        
        print("\n=== YOLO ì¢…í•© ìµœì í™” ë°ëª¨ ===\n")
        
        # ë‹¤ì–‘í•œ YOLO ëª¨ë¸ ë¹„êµ
        models_to_test = ['yolov8n', 'yolov8s', 'yolov8m']
        results = {}
        
        # ìƒ˜í”Œ ì´ë¯¸ì§€ ì¤€ë¹„
        images = self.yolo_loader.create_sample_images(5)
        
        for model_name in models_to_test:
            print(f"\nğŸ”„ {model_name} ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘...")
            
            # ëª¨ë¸ ë¡œë“œ
            model = self.yolo_loader.load_model(model_name)
            model_info = self.yolo_loader.get_model_info(model)
            
            # ì„±ëŠ¥ ì¸¡ì •
            benchmark = self.yolo_loader.benchmark_inference(model, images, runs=3)
            
            # ê°€ì§€ì¹˜ê¸° í…ŒìŠ¤íŠ¸
            pruned_model = self.yolo_optimizer.prune_yolo_model(model, sparsity=0.3)
            pruned_benchmark = self.yolo_loader.benchmark_inference(pruned_model, images, runs=3)
            
            results[model_name] = {
                'original': {
                    'size_mb': model_info['model_size_mb'],
                    'fps': benchmark['fps'],
                    'params': model_info['total_params']
                },
                'pruned': {
                    'fps': pruned_benchmark['fps']
                }
            }
        
        # ê²°ê³¼ í‘œ ì¶œë ¥
        print(f"\nğŸ“Š === YOLO ëª¨ë¸ ì¢…í•© ë¹„êµ === ğŸ“Š")
        print("-" * 80)
        print(f"{'ëª¨ë¸':^12} | {'í¬ê¸°(MB)':^10} | {'íŒŒë¼ë¯¸í„°':^12} | {'ì›ë³¸ FPS':^10} | {'ê°€ì§€ì¹˜ê¸° FPS':^12}")
        print("-" * 80)
        
        for model_name, data in results.items():
            print(f"{model_name:^12} | {data['original']['size_mb']:^10.1f} | {data['original']['params']:^12,} | {data['original']['fps']:^10.1f} | {data['pruned']['fps']:^12.1f}")
        
        print("-" * 80)
        
        return results
    
    def demo_yolo_m_before_after(self):
        """YOLOv8m ëª¨ë¸ ìµœì í™” ì „í›„ ë¹„êµ ë°ëª¨"""
        if self.model_type != 'yolo':
            print("âš ï¸  YOLO ëª¨ë¸ ëª¨ë“œê°€ ì•„ë‹™ë‹ˆë‹¤.")
            return None
        
        print("\n=== YOLOv8m ìµœì í™” ì „í›„ ë¹„êµ ë°ëª¨ ===\n")
        
        # ìƒ˜í”Œ ì´ë¯¸ì§€ ì¤€ë¹„
        images = self.yolo_loader.create_sample_images(5)
        
        # 1. ì›ë³¸ YOLOv8m ëª¨ë¸ ë¡œë“œ ë° ì¸¡ì •
        print("ğŸ”„ 1ë‹¨ê³„: ì›ë³¸ YOLOv8m ëª¨ë¸ ë¡œë“œ ë° ì„±ëŠ¥ ì¸¡ì •\n")
        original_model = self.yolo_loader.load_model('yolov8m')
        original_info = self.yolo_loader.get_model_info(original_model)
        
        print("ğŸ“Š ì›ë³¸ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •...")
        original_benchmark = self.yolo_loader.benchmark_inference(original_model, images, runs=5)
        
        # 2. ê°€ì§€ì¹˜ê¸° ì ìš©
        print(f"\n{'='*60}")
        print("ğŸ”„ 2ë‹¨ê³„: ê°€ì§€ì¹˜ê¸° ì ìš© (30% í¬ì†Œì„±)\n")
        
        # ê°€ì§€ì¹˜ê¸°ë¥¼ ìœ„í•´ ìƒˆ ëª¨ë¸ ë¡œë“œ (deepcopy ëŒ€ì‹ )
        pruned_model = self.yolo_loader.load_model('yolov8m')
        pruned_model = self.yolo_optimizer.prune_yolo_model(pruned_model, sparsity=0.3)
        pruned_info = self.yolo_loader.get_model_info(pruned_model)
        
        print("ğŸ“Š ê°€ì§€ì¹˜ê¸°ëœ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •...")
        pruned_benchmark = self.yolo_loader.benchmark_inference(pruned_model, images, runs=5)
        
        # 3. ì§€ì‹ ì¦ë¥˜ ì ìš© (YOLOv8m â†’ YOLOv8n)
        print(f"\n{'='*60}")
        print("ğŸ”„ 3ë‹¨ê³„: ì§€ì‹ ì¦ë¥˜ ì ìš© (YOLOv8m â†’ YOLOv8n)\n")
        
        student_model = self.yolo_loader.load_model('yolov8n')
        student_info = self.yolo_loader.get_model_info(student_model)
        
        print("ğŸ“Š í•™ìƒ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •...")
        student_benchmark = self.yolo_loader.benchmark_inference(student_model, images, runs=5)
        
        # 4. ê²°ê³¼ ë¹„êµ ë° ì‹œê°í™”
        print(f"\n{'='*60}")
        print("ğŸ“Š === YOLOv8m ìµœì í™” ì „í›„ ë¹„êµ ê²°ê³¼ === ğŸ“Š\n")
        
        # ìƒì„¸ ê²°ê³¼ í‘œ
        print("-" * 100)
        print(f"{'ìµœì í™” ë°©ë²•':^20} | {'ëª¨ë¸ í¬ê¸°(MB)':^15} | {'íŒŒë¼ë¯¸í„° ìˆ˜':^15} | {'FPS':^10} | {'í¬ê¸° ê°ì†Œ':^12} | {'ì†ë„ ë³€í™”':^12}")
        print("-" * 100)
        
        # ì›ë³¸ ëª¨ë¸
        print(f"{'ì›ë³¸ (YOLOv8m)':^20} | {original_info['model_size_mb']:^15.1f} | {original_info['total_params']:^15,} | {original_benchmark['fps']:^10.1f} | {'-':^12} | {'-':^12}")
        
        # ê°€ì§€ì¹˜ê¸° ëª¨ë¸
        size_reduction_prune = (original_info['model_size_mb'] - pruned_info['model_size_mb']) / original_info['model_size_mb'] * 100
        fps_change_prune = (pruned_benchmark['fps'] - original_benchmark['fps']) / original_benchmark['fps'] * 100
        print(f"{'ê°€ì§€ì¹˜ê¸° (30%)':^20} | {pruned_info['model_size_mb']:^15.1f} | {pruned_info['total_params']:^15,} | {pruned_benchmark['fps']:^10.1f} | {size_reduction_prune:^12.1f}% | {fps_change_prune:^+12.1f}%")
        
        # ì§€ì‹ ì¦ë¥˜ ëª¨ë¸
        size_reduction_distill = (original_info['model_size_mb'] - student_info['model_size_mb']) / original_info['model_size_mb'] * 100
        fps_change_distill = (student_benchmark['fps'] - original_benchmark['fps']) / original_benchmark['fps'] * 100
        print(f"{'ì§€ì‹ ì¦ë¥˜ (â†’n)':^20} | {student_info['model_size_mb']:^15.1f} | {student_info['total_params']:^15,} | {student_benchmark['fps']:^10.1f} | {size_reduction_distill:^12.1f}% | {fps_change_distill:^+12.1f}%")
        
        print("-" * 100)
        
        # í•µì‹¬ ì¸ì‚¬ì´íŠ¸
        print(f"\nğŸ¯ === í•µì‹¬ ì¸ì‚¬ì´íŠ¸ === ğŸ¯")
        print(f"ğŸ“¦ ëª¨ë¸ í¬ê¸°:")
        print(f"   â€¢ ì›ë³¸: {original_info['model_size_mb']:.1f}MB")
        print(f"   â€¢ ê°€ì§€ì¹˜ê¸°: {pruned_info['model_size_mb']:.1f}MB ({size_reduction_prune:.1f}% ê°ì†Œ)")
        print(f"   â€¢ ì§€ì‹ ì¦ë¥˜: {student_info['model_size_mb']:.1f}MB ({size_reduction_distill:.1f}% ê°ì†Œ)")
        
        print(f"\nâš¡ ì¶”ë¡  ì†ë„:")
        print(f"   â€¢ ì›ë³¸: {original_benchmark['fps']:.1f} FPS")
        print(f"   â€¢ ê°€ì§€ì¹˜ê¸°: {pruned_benchmark['fps']:.1f} FPS ({fps_change_prune:+.1f}% ë³€í™”)")
        print(f"   â€¢ ì§€ì‹ ì¦ë¥˜: {student_benchmark['fps']:.1f} FPS ({fps_change_distill:+.1f}% ë³€í™”)")
        
        # ì¶”ì²œ ë°©ë²•
        best_size_method = "ì§€ì‹ ì¦ë¥˜" if size_reduction_distill > size_reduction_prune else "ê°€ì§€ì¹˜ê¸°"
        best_speed_method = "ì§€ì‹ ì¦ë¥˜" if fps_change_distill > fps_change_prune else "ê°€ì§€ì¹˜ê¸°"
        
        print(f"\nğŸ’¡ === ì¶”ì²œ ë°©ë²• === ğŸ’¡")
        print(f"ğŸ† í¬ê¸° ê°ì†Œ ìµœê³ : {best_size_method}")
        print(f"ğŸ† ì†ë„ í–¥ìƒ ìµœê³ : {best_speed_method}")
        
        if self.explain_mode:
            print(f"\nğŸ“š === êµìœ¡ì  ì„¤ëª… === ğŸ“š")
            print(f"ğŸ” ê°€ì§€ì¹˜ê¸° (Pruning):")
            print(f"   â€¢ ì¤‘ìš”í•˜ì§€ ì•Šì€ ë‰´ëŸ° ì—°ê²°ì„ ì œê±°")
            print(f"   â€¢ ëª¨ë¸ êµ¬ì¡°ëŠ” ìœ ì§€í•˜ë˜ ê°€ì¤‘ì¹˜ë¥¼ 0ìœ¼ë¡œ ì„¤ì •")
            print(f"   â€¢ ì‹¤ì œ í¬ê¸° ê°ì†ŒëŠ” ì œí•œì ì´ì§€ë§Œ ì—°ì‚°ëŸ‰ ê°ì†Œ")
            
            print(f"\nğŸ“ ì§€ì‹ ì¦ë¥˜ (Knowledge Distillation):")
            print(f"   â€¢ í° ëª¨ë¸ì˜ ì§€ì‹ì„ ì‘ì€ ëª¨ë¸ë¡œ ì „ìˆ˜")
            print(f"   â€¢ ë“œë¼ë§ˆí‹±í•œ í¬ê¸° ê°ì†Œ íš¨ê³¼")
            print(f"   â€¢ ì‘ì€ ëª¨ë¸ì´ì§€ë§Œ í° ëª¨ë¸ì˜ ì„±ëŠ¥ ê·¼ì‚¬")
        
        # ë°˜í™˜í•  ê²°ê³¼ ë°ì´í„°
        results = {
            'original': {
                'size_mb': original_info['model_size_mb'],
                'fps': original_benchmark['fps'],
                'params': original_info['total_params']
            },
            'pruned': {
                'size_mb': pruned_info['model_size_mb'],
                'fps': pruned_benchmark['fps'],
                'params': pruned_info['total_params'],
                'size_reduction': size_reduction_prune,
                'fps_change': fps_change_prune
            },
            'distilled': {
                'size_mb': student_info['model_size_mb'],
                'fps': student_benchmark['fps'],
                'params': student_info['total_params'],
                'size_reduction': size_reduction_distill,
                'fps_change': fps_change_distill
            }
        }
        
        return results

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='ëª¨ë¸ ìµœì í™” ë°ëª¨')
    parser.add_argument('--mode', type=str, default='all', 
                        choices=['all', 'quantize', 'prune', 'distill', 'individual', 'yolo-prune', 'yolo-distill', 'yolo-comprehensive', 'yolo-m-demo'],
                        help='ì‹¤í–‰í•  ìµœì í™” ëª¨ë“œ ì„ íƒ')
    parser.add_argument('--model-type', type=str, default='simple',
                        choices=['simple', 'yolo'],
                        help='ëª¨ë¸ íƒ€ì… ì„ íƒ')
    parser.add_argument('--yolo-model', type=str, default='yolov8s',
                        choices=['yolov8n', 'yolov8s', 'yolov8m', 'yolov8l', 'yolov8x'],
                        help='YOLO ëª¨ë¸ ì„ íƒ')
    parser.add_argument('--sparsity', type=float, default=0.5,
                        help='ê°€ì§€ì¹˜ê¸° í¬ì†Œì„± (0-1)')
    parser.add_argument('--save-models', action='store_true',
                        help='ìµœì í™”ëœ ëª¨ë¸ ì €ì¥')
    parser.add_argument('--no-plot', action='store_true',
                        help='ê·¸ë˜í”„ í‘œì‹œ ì•ˆ í•¨')
    parser.add_argument('--explain', action='store_true',
                        help='êµìœ¡ ëª¨ë“œ: ê° ë‹¨ê³„ë¥¼ ìì„¸íˆ ì„¤ëª…')
    
    args = parser.parse_args()
    
    # ìµœì í™” ì‹¤í–‰
    optimizer = ModelOptimizer(explain_mode=args.explain, model_type=args.model_type)
    
    if args.mode == 'all':
        results = optimizer.compare_all_optimizations()
    elif args.mode == 'quantize':
        model = optimizer.demo_dynamic_quantization()
        if model is not None and args.save_models:
            optimizer.save_model(model, 'quantized_model.pth', 'quantized')
    elif args.mode == 'prune':
        model = optimizer.demo_pruning(sparsity=args.sparsity)
        if args.save_models:
            optimizer.save_model(model, f'pruned_model_{args.sparsity}.pth', f'pruned_{args.sparsity}')
    elif args.mode == 'distill':
        model = optimizer.demo_knowledge_distillation()
        if args.save_models:
            optimizer.save_model(model, 'distilled_model.pth', 'distilled')
    elif args.mode == 'individual':
        print("\nğŸš€ ê°œë³„ ìµœì í™” ë°ëª¨ ì‹¤í–‰\n")
        print("1. ë™ì  ì–‘ìí™” ë°ëª¨")
        optimizer.demo_dynamic_quantization()
        print("\n" + "="*80 + "\n")
        
        print("2. ê°€ì§€ì¹˜ê¸° ë°ëª¨")
        optimizer.demo_pruning()
        print("\n" + "="*80 + "\n")
        
        print("3. ì§€ì‹ ì¦ë¥˜ ë°ëª¨")
        optimizer.demo_knowledge_distillation()
    elif args.mode == 'yolo-prune':
        optimizer.demo_yolo_pruning(model_name=args.yolo_model, sparsity=args.sparsity)
    elif args.mode == 'yolo-distill':
        optimizer.demo_yolo_distillation()
    elif args.mode == 'yolo-comprehensive':
        optimizer.demo_yolo_comprehensive()
    elif args.mode == 'yolo-m-demo':
        optimizer.demo_yolo_m_before_after()
