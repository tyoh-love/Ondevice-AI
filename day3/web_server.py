from flask import Flask, request, jsonify, send_from_directory, Response
import whisper
import numpy as np
import wave
import tempfile
import os
import torch
import ollama
import traceback
import edge_tts
import asyncio
import io
import uuid
import base64
from PIL import Image

app = Flask(__name__)

# Helper function to run async functions in Flask
def run_async(func):
    """Flaskì—ì„œ async í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ê¸° ìœ„í•œ í—¬í¼"""
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        return loop.run_until_complete(func)
    finally:
        loop.close()

class Qwen2VLService:
    """Qwen2.5-VL Multimodal Service for both text and vision"""
    
    def __init__(self, model_name="qwen2.5-vl:latest"):
        self.model_name = model_name
        print("ğŸ¤– Qwen2.5-VL ë©€í‹°ëª¨ë‹¬ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
    
    def answer_question(self, question, language="ko", image_data=None):
        """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± (í…ìŠ¤íŠ¸ ë˜ëŠ” ë¹„ì „+í…ìŠ¤íŠ¸)"""
        try:
            if image_data:
                print(f"ğŸ‘ï¸ Qwen2.5-VLë¡œ ë¹„ì „+í…ìŠ¤íŠ¸ ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘: {question}")
            else:
                print(f"ğŸ§  Qwen2.5-VLë¡œ í…ìŠ¤íŠ¸ ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘: {question}")
            
            if language == "ko":
                if image_data:
                    prompt = f"""ì´ë¯¸ì§€ë¥¼ ë³´ê³  ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ë”°ëœ»í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ì œê³µí•´ì£¼ì„¸ìš”. ì´ë¯¸ì§€ì—ì„œ ë³´ì´ëŠ” ê²ƒì„ ì •í™•íˆ ì„¤ëª…í•˜ê³  ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

ë‹µë³€:"""
                else:
                    prompt = f"""ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ì„œ ë”°ëœ»í•œ ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ì œê³µí•´ì£¼ì„¸ìš”. ê°„ê²°í•˜ë©´ì„œë„ ì™„ì „í•œ ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

ë‹µë³€:"""
            else:
                if image_data:
                    prompt = f"""Please analyze the image and provide an accurate, detailed answer to the user's question. Describe what you see and answer the question thoroughly.

Question: {question}

Answer:"""
                else:
                    prompt = f"""Please provide an accurate and helpful answer to the following question. Keep it concise but complete.

Question: {question}

Answer:"""
            
            # Prepare message content
            if image_data:
                # For vision tasks, include both text and image
                message_content = [
                    {
                        "type": "text",
                        "text": prompt
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{image_data}"
                        }
                    }
                ]
            else:
                # For text-only tasks
                message_content = prompt
            
            response = ollama.chat(
                model=self.model_name,
                messages=[{
                    'role': 'user',
                    'content': message_content
                }],
                options={
                    'temperature': 0.3,
                    'top_p': 0.9,
                    'num_predict': 1024,
                    'repeat_penalty': 1.1,
                    'num_ctx': 4096
                }
            )
            
            answer = response['message']['content'].strip()
            if image_data:
                print(f"âœ… Qwen2.5-VL ë¹„ì „ ë‹µë³€ ì™„ë£Œ")
            else:
                print(f"âœ… Qwen2.5-VL í…ìŠ¤íŠ¸ ë‹µë³€ ì™„ë£Œ")
            return answer
            
        except Exception as e:
            print(f"âŒ Qwen2.5-VL ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            traceback.print_exc()
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

class TTSService:
    """Edge-TTS Text-to-Speech Service"""
    
    def __init__(self):
        self.korean_voices = {
            'female': 'ko-KR-SunHiNeural',
            'male': 'ko-KR-InJoonNeural'
        }
        self.english_voices = {
            'female': 'en-US-JennyNeural', 
            'male': 'en-US-GuyNeural'
        }
        self.audio_cache = {}  # Simple in-memory cache
        print("ğŸ”Š TTS ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
    
    async def text_to_speech(self, text, language="ko", gender="female", rate="+0%", pitch="+0Hz"):
        """í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜"""
        try:
            print(f"ğŸµ TTS ë³€í™˜ ì¤‘: {text[:50]}...")
            
            # Voice selection based on language and gender
            if language == "ko":
                voice = self.korean_voices.get(gender, self.korean_voices['female'])
            else:
                voice = self.english_voices.get(gender, self.english_voices['female'])
            
            # Create cache key (simplified without rate/pitch)
            cache_key = f"{hash(text)}_{voice}"
            
            # Check cache first
            if cache_key in self.audio_cache:
                print("ğŸ“¦ ìºì‹œì—ì„œ ì˜¤ë””ì˜¤ ë°˜í™˜")
                return self.audio_cache[cache_key]
            
            # Use plain text directly (no SSML to avoid XML tags being spoken)
            # Edge-TTS works best with simple text input
            communicate = edge_tts.Communicate(text, voice)
            audio_data = b""
            
            async for chunk in communicate.stream():
                if chunk["type"] == "audio":
                    audio_data += chunk["data"]
            
            # Cache the result (limit cache size)
            if len(self.audio_cache) > 50:  # Limit cache size
                # Remove oldest entry
                oldest_key = next(iter(self.audio_cache))
                del self.audio_cache[oldest_key]
            
            self.audio_cache[cache_key] = audio_data
            
            print(f"âœ… TTS ë³€í™˜ ì™„ë£Œ ({len(audio_data)} bytes)")
            return audio_data
            
        except Exception as e:
            print(f"âŒ TTS ë³€í™˜ ì˜¤ë¥˜: {e}")
            traceback.print_exc()
            return None
    
    def get_available_voices(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ìŒì„± ëª©ë¡ ë°˜í™˜"""
        return {
            'korean': self.korean_voices,
            'english': self.english_voices
        }
    
    async def text_to_speech_sync(self, text, language="ko", gender="female", rate="+0%", pitch="+0Hz"):
        """ë™ê¸° ë˜í¼ í•¨ìˆ˜"""
        return await self.text_to_speech(text, language, gender, rate, pitch)

class WebWhisperSTT:
    """Web-based Whisper STT Service"""
    
    def __init__(self):
        self.models = {}
        print("ğŸŒ Web Whisper STT ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
    
    def load_model(self, model_size="base"):
        """ëª¨ë¸ ë¡œë“œ (ìºì‹±)"""
        if model_size not in self.models:
            print(f"ğŸ“¥ Whisper {model_size} ëª¨ë¸ ë¡œë“œ ì¤‘...")
            self.models[model_size] = whisper.load_model(model_size)
            print(f"âœ… {model_size} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        return self.models[model_size]
    
    def process_webm_audio(self, audio_file):
        """WebM ì˜¤ë””ì˜¤ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜"""
        try:
            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            with tempfile.NamedTemporaryFile(suffix='.webm', delete=False) as temp_file:
                audio_file.save(temp_file.name)
                temp_path = temp_file.name
            
            # ffmpegë¥¼ ì‚¬ìš©í•˜ì—¬ WAVë¡œ ë³€í™˜
            import subprocess
            wav_path = temp_path.replace('.webm', '.wav')
            
            subprocess.run([
                'ffmpeg', '-y', '-i', temp_path, 
                '-ar', '16000', '-ac', '1', '-c:a', 'pcm_s16le',
                wav_path
            ], capture_output=True, check=True)
            
            # WAV íŒŒì¼ ì½ê¸°
            with wave.open(wav_path, 'rb') as wav_file:
                frames = wav_file.readframes(-1)
                sound_info = np.frombuffer(frames, dtype=np.int16)
                audio_float32 = sound_info.astype(np.float32) / 32768.0
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            os.unlink(temp_path)
            os.unlink(wav_path)
            
            return audio_float32
            
        except subprocess.CalledProcessError as e:
            print(f"FFmpeg ì˜¤ë¥˜: {e}")
            return None
        except Exception as e:
            print(f"ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return None
    
    def transcribe_audio(self, audio_data, language="ko", model_size="base"):
        """ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        try:
            model = self.load_model(model_size)
            
            # ìµœì†Œ ê¸¸ì´ í™•ë³´
            if len(audio_data) < 16000:  # 1ì´ˆ ë¯¸ë§Œ
                audio_data = np.pad(audio_data, (0, 16000 - len(audio_data)))
            
            # Whisper ì‹¤í–‰
            result = model.transcribe(
                audio_data,
                language=None if language == "auto" else language,
                fp16=torch.cuda.is_available()
            )
            
            return result["text"].strip()
            
        except Exception as e:
            print(f"ìŒì„± ì¸ì‹ ì˜¤ë¥˜: {e}")
            return None

# ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
stt_service = WebWhisperSTT()
qa_service = Qwen2VLService()
tts_service = TTSService()

@app.route('/')
def index():
    """ë©”ì¸ í˜ì´ì§€"""
    return send_from_directory('.', 'index.html')

@app.route('/transcribe', methods=['POST'])
def transcribe():
    """ìŒì„± ì¸ì‹ API"""
    try:
        # íŒŒì¼ í™•ì¸
        if 'audio' not in request.files:
            return jsonify({'success': False, 'error': 'ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤'})
        
        audio_file = request.files['audio']
        if audio_file.filename == '':
            return jsonify({'success': False, 'error': 'íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤'})
        
        # ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        language = request.form.get('language', 'ko')
        model_size = request.form.get('model', 'base')
        
        print(f"ğŸ¤ ìŒì„± ì¸ì‹ ìš”ì²­: ì–¸ì–´={language}, ëª¨ë¸={model_size}")
        
        # ì˜¤ë””ì˜¤ ì²˜ë¦¬
        audio_data = stt_service.process_webm_audio(audio_file)
        if audio_data is None:
            return jsonify({'success': False, 'error': 'ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì‹¤íŒ¨'})
        
        # ìŒì„± ì¸ì‹
        text = stt_service.transcribe_audio(audio_data, language, model_size)
        if text is None:
            return jsonify({'success': False, 'error': 'ìŒì„± ì¸ì‹ ì‹¤íŒ¨'})
        
        print(f"ğŸ“ ì¸ì‹ ê²°ê³¼: {text}")
        
        return jsonify({
            'success': True,
            'text': text,
            'language': language,
            'model': model_size
        })
        
    except Exception as e:
        print(f"ì„œë²„ ì˜¤ë¥˜: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/answer', methods=['POST'])
def answer_question():
    """ExaOneìœ¼ë¡œ ì§ˆë¬¸ ë‹µë³€ API"""
    try:
        data = request.get_json()
        if not data or 'question' not in data:
            return jsonify({'success': False, 'error': 'ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤'})
        
        question = data['question'].strip()
        if not question:
            return jsonify({'success': False, 'error': 'ë¹ˆ ì§ˆë¬¸ì…ë‹ˆë‹¤'})
        
        language = data.get('language', 'ko')
        
        print(f"ğŸ’¬ ì§ˆë¬¸ ì²˜ë¦¬ ìš”ì²­: {question}")
        
        # ExaOneìœ¼ë¡œ ë‹µë³€ ìƒì„±
        answer = qa_service.answer_question(question, language)
        
        return jsonify({
            'success': True,
            'question': question,
            'answer': answer,
            'language': language
        })
        
    except Exception as e:
        print(f"ì§ˆë¬¸ ë‹µë³€ ì˜¤ë¥˜: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/chat', methods=['POST'])
def voice_chat():
    """ìŒì„± ëŒ€í™” API (STT + ExaOne í†µí•©)"""
    try:
        # íŒŒì¼ í™•ì¸
        if 'audio' not in request.files:
            return jsonify({'success': False, 'error': 'ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤'})
        
        audio_file = request.files['audio']
        if audio_file.filename == '':
            return jsonify({'success': False, 'error': 'íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤'})
        
        # ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        language = request.form.get('language', 'ko')
        model_size = request.form.get('model', 'base')
        tts_enabled = request.form.get('tts_enabled', 'true').lower() == 'true'
        tts_gender = request.form.get('tts_gender', 'female')
        
        print(f"ğŸ™ï¸ ìŒì„± ëŒ€í™” ìš”ì²­: ì–¸ì–´={language}, ëª¨ë¸={model_size}, TTS={tts_enabled}")
        
        # 1ë‹¨ê³„: ìŒì„± ì¸ì‹
        audio_data = stt_service.process_webm_audio(audio_file)
        if audio_data is None:
            return jsonify({'success': False, 'error': 'ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì‹¤íŒ¨'})
        
        text = stt_service.transcribe_audio(audio_data, language, model_size)
        if text is None:
            return jsonify({'success': False, 'error': 'ìŒì„± ì¸ì‹ ì‹¤íŒ¨'})
        
        print(f"ğŸ“ ì¸ì‹ëœ ì§ˆë¬¸: {text}")
        
        # 2ë‹¨ê³„: ExaOneìœ¼ë¡œ ë‹µë³€ ìƒì„±
        answer = qa_service.answer_question(text, language)
        
        print(f"ğŸ¤– ìƒì„±ëœ ë‹µë³€: {answer}")
        
        # 3ë‹¨ê³„: TTS ë³€í™˜ (ì„ íƒì )
        audio_url = None
        if tts_enabled:
            try:
                audio_data_tts = run_async(tts_service.text_to_speech(answer, language, tts_gender))
                if audio_data_tts:
                    # ì˜¤ë””ì˜¤ ID ìƒì„± ë° ì„ì‹œ ì €ì¥
                    audio_id = str(uuid.uuid4())
                    temp_dir = tempfile.gettempdir()
                    audio_path = os.path.join(temp_dir, f"tts_{audio_id}.mp3")
                    
                    with open(audio_path, 'wb') as f:
                        f.write(audio_data_tts)
                    
                    audio_url = f'/audio/{audio_id}'
                    print(f"ğŸ”Š TTS ì˜¤ë””ì˜¤ ìƒì„± ì™„ë£Œ: {audio_url}")
            except Exception as e:
                print(f"âš ï¸ TTS ë³€í™˜ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {e}")
        
        return jsonify({
            'success': True,
            'question': text,
            'answer': answer,
            'language': language,
            'model': model_size,
            'audio_url': audio_url,
            'tts_enabled': tts_enabled
        })
        
    except Exception as e:
        print(f"ìŒì„± ëŒ€í™” ì˜¤ë¥˜: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/speak', methods=['POST'])
def speak():
    """í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” API"""
    try:
        data = request.get_json()
        if not data or 'text' not in data:
            return jsonify({'success': False, 'error': 'í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤'})
        
        text = data['text'].strip()
        if not text:
            return jsonify({'success': False, 'error': 'ë¹ˆ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤'})
        
        language = data.get('language', 'ko')
        gender = data.get('gender', 'female')
        rate = data.get('rate', '+0%')
        pitch = data.get('pitch', '+0Hz')
        
        print(f"ğŸ”Š TTS ìš”ì²­: {text[:50]}... (ì–¸ì–´: {language}, ì„±ë³„: {gender})")
        
        # TTS ë³€í™˜
        audio_data = run_async(tts_service.text_to_speech(text, language, gender, rate, pitch))
        
        if audio_data is None:
            return jsonify({'success': False, 'error': 'TTS ë³€í™˜ ì‹¤íŒ¨'})
        
        # ì˜¤ë””ì˜¤ ID ìƒì„± (ì„ì‹œ ì €ì¥ìš©)
        audio_id = str(uuid.uuid4())
        
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        temp_dir = tempfile.gettempdir()
        audio_path = os.path.join(temp_dir, f"tts_{audio_id}.mp3")
        
        with open(audio_path, 'wb') as f:
            f.write(audio_data)
        
        return jsonify({
            'success': True,
            'audio_id': audio_id,
            'audio_url': f'/audio/{audio_id}',
            'text': text,
            'language': language,
            'gender': gender
        })
        
    except Exception as e:
        print(f"TTS API ì˜¤ë¥˜: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/audio/<audio_id>')
def serve_audio(audio_id):
    """ìƒì„±ëœ ì˜¤ë””ì˜¤ íŒŒì¼ ì œê³µ"""
    try:
        temp_dir = tempfile.gettempdir()
        audio_path = os.path.join(temp_dir, f"tts_{audio_id}.mp3")
        
        if not os.path.exists(audio_path):
            return "Audio not found", 404
        
        def generate():
            with open(audio_path, 'rb') as f:
                data = f.read()
                yield data
            # íŒŒì¼ ì „ì†¡ í›„ ì‚­ì œ
            try:
                os.unlink(audio_path)
            except:
                pass
        
        return Response(generate(), mimetype='audio/mpeg')
        
    except Exception as e:
        print(f"ì˜¤ë””ì˜¤ ì„œë¹™ ì˜¤ë¥˜: {e}")
        return "Error serving audio", 500

@app.route('/voices')
def get_voices():
    """ì‚¬ìš© ê°€ëŠ¥í•œ TTS ìŒì„± ëª©ë¡ ë°˜í™˜"""
    try:
        voices = tts_service.get_available_voices()
        return jsonify({
            'success': True,
            'voices': voices
        })
    except Exception as e:
        print(f"ìŒì„± ëª©ë¡ ì˜¤ë¥˜: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/analyze_image', methods=['POST'])
def analyze_image():
    """ì´ë¯¸ì§€ ë¶„ì„ API"""
    try:
        data = request.get_json()
        if not data or 'image' not in data or 'question' not in data:
            return jsonify({'success': False, 'error': 'ì´ë¯¸ì§€ì™€ ì§ˆë¬¸ì´ í•„ìš”í•©ë‹ˆë‹¤'})
        
        image_data = data['image']
        question = data['question'].strip()
        language = data.get('language', 'ko')
        
        if not question:
            question = "ì´ ì´ë¯¸ì§€ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”." if language == 'ko' else "Please describe this image."
        
        print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ë¶„ì„ ìš”ì²­: {question}")
        
        # Remove data URL prefix if present
        if image_data.startswith('data:image'):
            image_data = image_data.split(',')[1]
        
        # Generate answer using Qwen2.5-VL with vision
        answer = qa_service.answer_question(question, language, image_data)
        
        return jsonify({
            'success': True,
            'question': question,
            'answer': answer,
            'language': language
        })
        
    except Exception as e:
        print(f"ì´ë¯¸ì§€ ë¶„ì„ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)})

@app.route('/vision_chat', methods=['POST'])
def vision_chat():
    """ë¹„ì „ + ìŒì„± ëŒ€í™” API (STT + ì´ë¯¸ì§€ ë¶„ì„ + TTS í†µí•©)"""
    try:
        # íŒŒì¼ ë° ë°ì´í„° í™•ì¸
        if 'audio' not in request.files:
            return jsonify({'success': False, 'error': 'ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤'})
        
        audio_file = request.files['audio']
        if audio_file.filename == '':
            return jsonify({'success': False, 'error': 'íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤'})
        
        # ì´ë¯¸ì§€ ë°ì´í„° í™•ì¸
        image_data = request.form.get('image')
        if not image_data:
            return jsonify({'success': False, 'error': 'ì´ë¯¸ì§€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤'})
        
        # ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        language = request.form.get('language', 'ko')
        model_size = request.form.get('model', 'base')
        tts_enabled = request.form.get('tts_enabled', 'true').lower() == 'true'
        tts_gender = request.form.get('tts_gender', 'female')
        
        print(f"ğŸ‘ï¸ğŸ™ï¸ ë¹„ì „ + ìŒì„± ëŒ€í™” ìš”ì²­: ì–¸ì–´={language}, TTS={tts_enabled}")
        
        # 1ë‹¨ê³„: ìŒì„± ì¸ì‹
        audio_data = stt_service.process_webm_audio(audio_file)
        if audio_data is None:
            return jsonify({'success': False, 'error': 'ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì‹¤íŒ¨'})
        
        text = stt_service.transcribe_audio(audio_data, language, model_size)
        if text is None:
            return jsonify({'success': False, 'error': 'ìŒì„± ì¸ì‹ ì‹¤íŒ¨'})
        
        print(f"ğŸ“ ì¸ì‹ëœ ì§ˆë¬¸: {text}")
        
        # Remove data URL prefix if present
        if image_data.startswith('data:image'):
            image_data = image_data.split(',')[1]
        
        # 2ë‹¨ê³„: Qwen2.5-VLë¡œ ë¹„ì „ ë¶„ì„
        answer = qa_service.answer_question(text, language, image_data)
        
        print(f"ğŸ¤– ìƒì„±ëœ ë‹µë³€: {answer}")
        
        # 3ë‹¨ê³„: TTS ë³€í™˜ (ì„ íƒì )
        audio_url = None
        if tts_enabled:
            try:
                audio_data_tts = run_async(tts_service.text_to_speech(answer, language, tts_gender))
                if audio_data_tts:
                    # ì˜¤ë””ì˜¤ ID ìƒì„± ë° ì„ì‹œ ì €ì¥
                    audio_id = str(uuid.uuid4())
                    temp_dir = tempfile.gettempdir()
                    audio_path = os.path.join(temp_dir, f"tts_{audio_id}.mp3")
                    
                    with open(audio_path, 'wb') as f:
                        f.write(audio_data_tts)
                    
                    audio_url = f'/audio/{audio_id}'
                    print(f"ğŸ”Š TTS ì˜¤ë””ì˜¤ ìƒì„± ì™„ë£Œ: {audio_url}")
            except Exception as e:
                print(f"âš ï¸ TTS ë³€í™˜ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {e}")
        
        return jsonify({
            'success': True,
            'question': text,
            'answer': answer,
            'language': language,
            'model': model_size,
            'audio_url': audio_url,
            'tts_enabled': tts_enabled,
            'has_vision': True
        })
        
    except Exception as e:
        print(f"ë¹„ì „ + ìŒì„± ëŒ€í™” ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)})

@app.route('/status')
def status():
    """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    # Qwen2.5-VL ëª¨ë¸ ìƒíƒœ í™•ì¸
    qwen2vl_available = False
    try:
        models_response = ollama.list()
        model_names = [model.model for model in models_response.models]
        qwen2vl_available = 'qwen2.5-vl:latest' in model_names
    except:
        pass
    
    return jsonify({
        'status': 'running',
        'loaded_models': list(stt_service.models.keys()),
        'available_models': ['base', 'small', 'medium', 'large'],
        'cuda_available': torch.cuda.is_available(),
        'qwen2vl_available': qwen2vl_available,
        'qa_service': 'Qwen2.5-VL Multimodal',
        'vision_enabled': True,
        'vad_enabled': True,
        'tts_service': 'Edge-TTS',
        'tts_voices': tts_service.get_available_voices()
    })

@app.route('/health')
def health():
    """í—¬ìŠ¤ ì²´í¬"""
    return jsonify({'status': 'healthy'})

if __name__ == '__main__':
    print("ğŸš€ Multimodal AI Assistant with Qwen2.5-VL ì„œë²„ ì‹œì‘ ì¤‘...")
    print("ğŸ“± ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:5000 ì ‘ì†")
    print("ğŸ¤ ë§ˆì´í¬ ê¶Œí•œì„ í—ˆìš©í•´ì£¼ì„¸ìš” (VAD ìë™ ê°ì§€)")
    print("ğŸ‘ï¸ ì¹´ë©”ë¼ ê¶Œí•œì„ í—ˆìš©í•´ì£¼ì„¸ìš” (ë¹„ì „ ë¶„ì„)")
    print("ğŸ¤– ìŒì„±+ë¹„ì „ìœ¼ë¡œ ì§ˆë¬¸í•˜ë©´ Qwen2.5-VLì´ ë‹µë³€í•©ë‹ˆë‹¤")
    print("ğŸ”Š TTSë¡œ ë‹µë³€ì„ ìŒì„±ìœ¼ë¡œ ë“¤ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
    print("âš¡ CUDA ì‚¬ìš© ê°€ëŠ¥:", torch.cuda.is_available())
    
    # FFmpeg ì„¤ì¹˜ í™•ì¸
    try:
        import subprocess
        subprocess.run(['ffmpeg', '-version'], capture_output=True, check=True)
        print("âœ… FFmpeg ì„¤ì¹˜ í™•ì¸ë¨")
    except (subprocess.CalledProcessError, FileNotFoundError):
        print("âš ï¸  FFmpegê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤:")
        print("   Ubuntu/WSL: sudo apt install ffmpeg")
        print("   Windows: chocolateyë¡œ ì„¤ì¹˜í•˜ê±°ë‚˜ ê³µì‹ ì‚¬ì´íŠ¸ì—ì„œ ë‹¤ìš´ë¡œë“œ")
    
    app.run(host='0.0.0.0', port=5000, debug=True)