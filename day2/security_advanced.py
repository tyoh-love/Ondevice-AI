
import numpy as np
import time
import face_recognition
import pickle
import supervision as sv
from sklearn.cluster import DBSCAN
import networkx as nx
from collections import Counter, defaultdict
from typing import List, Dict, Tuple, Optional

# Import SmartSecuritySystem from the main security module
# Note: Python module names can't have hyphens, so we need to import it differently
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# Set environment variables for headless operation BEFORE importing cv2
# This prevents Qt platform errors in WSL/SSH environments
if 'WSL_DISTRO_NAME' in os.environ or 'SSH_CLIENT' in os.environ:
    os.environ['QT_QPA_PLATFORM'] = 'offscreen'
    os.environ['OPENCV_VIDEOIO_PRIORITY_MSMF'] = '0'
    print("WSL/SSH environment detected - running in compatibility mode")

import cv2

try:
    # Import the module with hyphen in filename
    import importlib.util
    spec = importlib.util.spec_from_file_location("realtime_behavior", "realtime-behavior.py")
    realtime_behavior = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(realtime_behavior)
    SmartSecuritySystem = realtime_behavior.SmartSecuritySystem
except Exception as e:
    print(f"Warning: Could not import SmartSecuritySystem: {e}")
    # If running as standalone, define a mock class
    class SmartSecuritySystem:
        pass

class AdvancedSecurityFeatures:
    """ê³ ê¸‰ ë³´ì•ˆ ê¸°ëŠ¥ ëª¨ìŒ"""
    
    def __init__(self):
        self.known_faces = {}
        self.load_known_faces()
    
    def load_known_faces(self):
        """ì•Œë ¤ì§„ ì–¼êµ´ ë°ì´í„° ë¡œë“œ"""
        try:
            with open('known_faces.pkl', 'rb') as f:
                self.known_faces = pickle.load(f)
        except FileNotFoundError:
            print("ì•Œë ¤ì§„ ì–¼êµ´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    def face_recognition_analysis(self, frame: np.ndarray) -> List[Dict]:
        """ì–¼êµ´ ì¸ì‹ ë¶„ì„"""
        # ì–¼êµ´ ì°¾ê¸°
        face_locations = face_recognition.face_locations(frame)
        face_encodings = face_recognition.face_encodings(frame, face_locations)
        
        results = []
        
        for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):
            # ì•Œë ¤ì§„ ì–¼êµ´ê³¼ ë¹„êµ
            matches = face_recognition.compare_faces(
                list(self.known_faces.values()),
                face_encoding
            )
            
            name = "Unknown"
            
            if True in matches:
                match_index = matches.index(True)
                name = list(self.known_faces.keys())[match_index]
            
            results.append({
                'name': name,
                'location': (left, top, right, bottom),
                'authorized': name != "Unknown"
            })
        
        return results
    
    def crowd_analysis(self, detections: sv.Detections) -> Dict:
        """êµ°ì¤‘ ë¶„ì„"""
        # ì‚¬ëŒë§Œ í•„í„°ë§
        person_indices = [i for i, class_id in enumerate(detections.class_id) 
                         if class_id == 0]  # 0ì€ 'person' í´ë˜ìŠ¤
        
        if len(person_indices) < 2:
            return {'density': 'low', 'clusters': 0}
        
        # ìœ„ì¹˜ ì¶”ì¶œ
        positions = []
        for i in person_indices:
            bbox = detections.xyxy[i]
            center = ((bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2)
            positions.append(center)
        
        positions = np.array(positions)
        
        # DBSCAN í´ëŸ¬ìŠ¤í„°ë§
        clustering = DBSCAN(eps=100, min_samples=3).fit(positions)
        n_clusters = len(set(clustering.labels_)) - (1 if -1 in clustering.labels_ else 0)
        
        # ë°€ë„ ê³„ì‚°
        total_people = len(person_indices)
        if total_people < 5:
            density = 'low'
        elif total_people < 15:
            density = 'medium'
        else:
            density = 'high'
        
        return {
            'density': density,
            'clusters': n_clusters,
            'total_people': total_people,
            'positions': positions,
            'labels': clustering.labels_
        }
    
    def path_prediction(self, track_history: Dict, track_id: int) -> Optional[np.ndarray]:
        """ê²½ë¡œ ì˜ˆì¸¡"""
        if track_id not in track_history:
            return None
        
        positions = list(track_history[track_id]['positions'])
        
        if len(positions) < 10:
            return None
        
        # ìµœê·¼ 10ê°œ ìœ„ì¹˜
        recent_positions = np.array(positions[-10:])
        
        # ê°„ë‹¨í•œ ì„ í˜• ì˜ˆì¸¡
        x_positions = recent_positions[:, 0]
        y_positions = recent_positions[:, 1]
        
        # ì„ í˜• íšŒê·€
        t = np.arange(len(x_positions))
        x_coef = np.polyfit(t, x_positions, 1)
        y_coef = np.polyfit(t, y_positions, 1)
        
        # ë‹¤ìŒ 5í”„ë ˆì„ ì˜ˆì¸¡
        future_t = np.arange(len(x_positions), len(x_positions) + 5)
        future_x = np.polyval(x_coef, future_t)
        future_y = np.polyval(y_coef, future_t)
        
        future_positions = np.column_stack((future_x, future_y))
        
        return future_positions
    
    def zone_monitoring(self, frame_shape: Tuple[int, int]) -> Dict:
        """êµ¬ì—­ë³„ ëª¨ë‹ˆí„°ë§ ì„¤ì •"""
        height, width = frame_shape
        
        zones = {
            'entrance': {
                'polygon': np.array([
                    [0, height * 0.7],
                    [width * 0.3, height * 0.7],
                    [width * 0.3, height],
                    [0, height]
                ], dtype=np.int32),
                'type': 'entrance',
                'max_loitering_time': 30
            },
            'restricted': {
                'polygon': np.array([
                    [width * 0.7, 0],
                    [width, 0],
                    [width, height * 0.3],
                    [width * 0.7, height * 0.3]
                ], dtype=np.int32),
                'type': 'restricted',
                'authorized_only': True
            },
            'exit': {
                'polygon': np.array([
                    [width * 0.7, height * 0.7],
                    [width, height * 0.7],
                    [width, height],
                    [width * 0.7, height]
                ], dtype=np.int32),
                'type': 'exit',
                'direction': 'out'
            }
        }
        
        return zones
    
    def check_zone_violations(self, position: Tuple[float, float], 
                            zones: Dict, is_authorized: bool = False) -> List[str]:
        """êµ¬ì—­ ìœ„ë°˜ í™•ì¸"""
        violations = []
        
        point = np.array(position, dtype=np.float32)
        
        for zone_name, zone_info in zones.items():
            # ì ì´ ë‹¤ê°í˜• ë‚´ë¶€ì— ìˆëŠ”ì§€ í™•ì¸
            result = cv2.pointPolygonTest(zone_info['polygon'], point, False)
            
            if result >= 0:  # ë‚´ë¶€ ë˜ëŠ” ê²½ê³„
                if zone_info['type'] == 'restricted' and not is_authorized:
                    violations.append(f"Unauthorized access to {zone_name}")
                
        return violations
    
    def network_analysis(self, track_history: Dict, time_window: float = 60) -> nx.Graph:
        """ê°ì²´ ê°„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„"""
        G = nx.Graph()
        
        current_time = time.time()
        
        # í™œì„± íŠ¸ë™ë§Œ ì„ íƒ
        active_tracks = {
            track_id: data for track_id, data in track_history.items()
            if current_time - data['last_seen'] < time_window
        }
        
        # ë…¸ë“œ ì¶”ê°€
        for track_id, data in active_tracks.items():
            G.add_node(track_id, 
                      object_class=data['class'],
                      duration=current_time - data['first_seen'])
        
        # ì—£ì§€ ì¶”ê°€ (ê·¼ì ‘ì„± ê¸°ë°˜)
        track_ids = list(active_tracks.keys())
        
        for i in range(len(track_ids)):
            for j in range(i + 1, len(track_ids)):
                id1, id2 = track_ids[i], track_ids[j]
                
                # ìµœê·¼ ìœ„ì¹˜
                if active_tracks[id1]['positions'] and active_tracks[id2]['positions']:
                    pos1 = active_tracks[id1]['positions'][-1]
                    pos2 = active_tracks[id2]['positions'][-1]
                    
                    distance = np.sqrt((pos1[0] - pos2[0])**2 + (pos1[1] - pos2[1])**2)
                    
                    if distance < 100:  # ê·¼ì ‘ ì„ê³„ê°’
                        G.add_edge(id1, id2, weight=1/distance)
        
        return G
    
    def generate_heatmap(self, track_history: Dict, frame_shape: Tuple[int, int]) -> np.ndarray:
        """ì›€ì§ì„ íˆíŠ¸ë§µ ìƒì„±"""
        height, width = frame_shape
        heatmap = np.zeros((height, width), dtype=np.float32)
        
        for track_data in track_history.values():
            for position in track_data['positions']:
                x, y = int(position[0]), int(position[1])
                
                # ê°€ìš°ì‹œì•ˆ ì»¤ë„ ì ìš©
                if 0 <= x < width and 0 <= y < height:
                    cv2.circle(heatmap, (x, y), 20, 1, -1)
        
        # ì •ê·œí™”
        heatmap = cv2.GaussianBlur(heatmap, (21, 21), 0)
        heatmap = (heatmap / heatmap.max() * 255).astype(np.uint8)
        
        # ì»¬ëŸ¬ë§µ ì ìš©
        heatmap_colored = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)
        
        return heatmap_colored

# ì‹¤ì‹œê°„ ë¶„ì„ ëŒ€ì‹œë³´ë“œ
class RealtimeAnalyticsDashboard:
    """ì‹¤ì‹œê°„ ë¶„ì„ ëŒ€ì‹œë³´ë“œ"""
    
    def __init__(self, security_system: SmartSecuritySystem):
        self.security_system = security_system
        self.advanced_features = AdvancedSecurityFeatures()
        
    def create_dashboard(self):
        """ëŒ€ì‹œë³´ë“œ ìƒì„±"""
        import plotly.graph_objects as go
        from plotly.subplots import make_subplots
        import dash
        from dash import dcc, html
        from dash.dependencies import Input, Output
        import dash_bootstrap_components as dbc
        
        # Dash ì•± ì´ˆê¸°í™”
        app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])
        
        app.layout = dbc.Container([
            dbc.Row([
                dbc.Col([
                    html.H1("ğŸ”’ ì‹¤ì‹œê°„ ë³´ì•ˆ ë¶„ì„ ëŒ€ì‹œë³´ë“œ", className="text-center mb-4"),
                    html.Hr()
                ])
            ]),
            
            dbc.Row([
                dbc.Col([
                    dcc.Graph(id='live-video-feed'),
                    dcc.Interval(id='video-update', interval=100)  # 100msë§ˆë‹¤ ì—…ë°ì´íŠ¸
                ], width=8),
                
                dbc.Col([
                    dbc.Card([
                        dbc.CardHeader("ì‹¤ì‹œê°„ í†µê³„"),
                        dbc.CardBody([
                            html.H4(id='total-objects', children="0"),
                            html.P("ì´ ê°ì²´ ìˆ˜"),
                            html.Hr(),
                            html.H4(id='alert-count', children="0"),
                            html.P("ê²½ê³  íšŸìˆ˜"),
                            html.Hr(),
                            html.H4(id='crowd-density', children="Low"),
                            html.P("êµ°ì¤‘ ë°€ë„")
                        ])
                    ])
                ], width=4)
            ], className="mb-4"),
            
            dbc.Row([
                dbc.Col([
                    dcc.Graph(id='heatmap')
                ], width=6),
                
                dbc.Col([
                    dcc.Graph(id='network-graph')
                ], width=6)
            ]),
            
            dbc.Row([
                dbc.Col([
                    dcc.Graph(id='timeline-chart'),
                    dcc.Interval(id='chart-update', interval=1000)  # 1ì´ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸
                ])
            ], className="mt-4")
        ], fluid=True)
        
        # ì½œë°± í•¨ìˆ˜ë“¤
        @app.callback(
            [Output('total-objects', 'children'),
             Output('alert-count', 'children'),
             Output('crowd-density', 'children')],
            [Input('video-update', 'n_intervals')]
        )
        def update_stats(n):
            """í†µê³„ ì—…ë°ì´íŠ¸"""
            total_objects = len(self.security_system.track_history)
            alert_count = len([e for e in self.security_system.event_log 
                             if e.get('type') in ['loitering', 'running']])
            
            # êµ°ì¤‘ ë°€ë„ ê³„ì‚° (ê°„ë‹¨í•œ ì˜ˆì‹œ)
            active_objects = sum(1 for track in self.security_system.track_history.values()
                               if time.time() - track['last_seen'] < 5)
            
            if active_objects < 5:
                density = "Low"
            elif active_objects < 15:
                density = "Medium"
            else:
                density = "High"
            
            return str(total_objects), str(alert_count), density
        
        @app.callback(
            Output('timeline-chart', 'figure'),
            [Input('chart-update', 'n_intervals')]
        )
        def update_timeline(n):
            """íƒ€ì„ë¼ì¸ ì°¨íŠ¸ ì—…ë°ì´íŠ¸"""
            # ìµœê·¼ 60ì´ˆ ë°ì´í„°
            current_time = time.time()
            time_bins = defaultdict(int)
            
            for event in self.security_system.event_log:
                if 'time' in event:
                    time_diff = current_time - event['time']
                    if time_diff < 60:
                        bin_index = int(time_diff / 5)  # 5ì´ˆ ë‹¨ìœ„
                        time_bins[bin_index] += 1
            
            x = list(range(12))
            y = [time_bins.get(11-i, 0) for i in x]
            
            fig = go.Figure(data=go.Bar(x=x, y=y))
            fig.update_layout(
                title="ìµœê·¼ 60ì´ˆ ì´ë²¤íŠ¸ íƒ€ì„ë¼ì¸",
                xaxis_title="ì‹œê°„ (5ì´ˆ ë‹¨ìœ„)",
                yaxis_title="ì´ë²¤íŠ¸ ìˆ˜",
                xaxis=dict(ticktext=[f"{i*5}s" for i in range(12)],
                          tickvals=list(range(12)))
            )
            
            return fig
        
        return app

# ì „ì²´ ì‹œìŠ¤í…œ í†µí•©
def run_advanced_security_system(video_source=None, headless=False):
    """ê³ ê¸‰ ë³´ì•ˆ ì‹œìŠ¤í…œ ì‹¤í–‰
    
    Args:
        video_source: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” ì¹´ë©”ë¼ ì¸ë±ìŠ¤ (ê¸°ë³¸ê°’: Noneìœ¼ë¡œ ì‚¬ìš©ì ì…ë ¥ ë°›ìŒ)
        headless: Trueë©´ í™”ë©´ í‘œì‹œ ì—†ì´ ì‹¤í–‰ (WSL í™˜ê²½ìš©)
    """
    
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘    ğŸ”’ ê³ ê¸‰ AI ë³´ì•ˆ ì‹œìŠ¤í…œ v2.0 ğŸ”’         â•‘
    â•‘                                          â•‘
    â•‘  â€¢ ì–¼êµ´ ì¸ì‹        â€¢ êµ°ì¤‘ ë¶„ì„            â•‘
    â•‘  â€¢ ê²½ë¡œ ì˜ˆì¸¡        â€¢ êµ¬ì—­ ëª¨ë‹ˆí„°ë§         â•‘
    â•‘  â€¢ ë„¤íŠ¸ì›Œí¬ ë¶„ì„    â€¢ íˆíŠ¸ë§µ ìƒì„±           â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # ë¹„ë””ì˜¤ ì†ŒìŠ¤ ì„ íƒ
    if video_source is None:
        print("\nì‹¤í–‰ ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”:")
        print("1. ë¹„ë””ì˜¤ íŒŒì¼ ë¶„ì„")
        print("2. ì›¹ìº  ì‹¤ì‹œê°„ ë¶„ì„ (WSLì—ì„œëŠ” ì‘ë™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ)")
        
        choice = input("\nì„ íƒ (1-2): ")
        
        if choice == "1":
            video_source = input("ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")
            if not os.path.exists(video_source):
                print(f"ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - {video_source}")
                return
        elif choice == "2":
            video_source = 0
            print("ì›¹ìº ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. WSL í™˜ê²½ì—ì„œëŠ” ì‘ë™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        else:
            print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
            return
    
    security_system = SmartSecuritySystem()
    advanced_features = AdvancedSecurityFeatures()
    
    # ë¹„ë””ì˜¤ ìº¡ì²˜
    cap = cv2.VideoCapture(video_source)
    if not cap.isOpened():
        print(f"ì˜¤ë¥˜: ë¹„ë””ì˜¤ ì†ŒìŠ¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - {video_source}")
        return
    
    # ë¹„ë””ì˜¤ ì •ë³´
    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 30
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    if isinstance(video_source, str):
        print(f"\në¹„ë””ì˜¤ íŒŒì¼ ì •ë³´:")
        print(f"  ê²½ë¡œ: {video_source}")
        print(f"  ì´ í”„ë ˆì„: {total_frames}")
        print(f"  FPS: {fps}")
    
    # ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ VideoWriter (headless ëª¨ë“œ)
    out_writer = None
    if headless and isinstance(video_source, str):
        output_path = video_source.rsplit('.', 1)[0] + '_analyzed.mp4'
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        out_writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        print(f"  ë¶„ì„ ê²°ê³¼ ì €ì¥: {output_path}")
    
    frame_count = 0
    print("\në¶„ì„ ì¤‘... (ESC ë˜ëŠ” 'q' í‚¤ë¡œ ì¢…ë£Œ)")
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        # ê¸°ë³¸ ì²˜ë¦¬
        annotated_frame, events = security_system.process_frame(frame, frame_count)
        
        # ì–¼êµ´ ì¸ì‹
        try:
            faces = advanced_features.face_recognition_analysis(frame)
            for face in faces:
                x1, y1, x2, y2 = face['location']
                color = (0, 255, 0) if face['authorized'] else (0, 0, 255)
                cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)
                cv2.putText(annotated_frame, face['name'], (x1, y1-10),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
        except Exception as e:
            # ì–¼êµ´ ì¸ì‹ ì‹¤íŒ¨ ì‹œ ê³„ì† ì§„í–‰
            pass
        
        # íˆíŠ¸ë§µ ì˜¤ë²„ë ˆì´
        if len(security_system.track_history) > 0:
            heatmap = advanced_features.generate_heatmap(
                security_system.track_history, 
                frame.shape[:2]
            )
            annotated_frame = cv2.addWeighted(annotated_frame, 0.7, heatmap, 0.3, 0)
        
        # ì§„í–‰ë¥  í‘œì‹œ
        if isinstance(video_source, str) and frame_count % 30 == 0:
            progress = (frame_count / total_frames * 100) if total_frames > 0 else 0
            print(f"\rì§„í–‰ë¥ : {progress:.1f}% ({frame_count}/{total_frames})", end='', flush=True)
        
        # ê²°ê³¼ ì €ì¥ (headless ëª¨ë“œ)
        if out_writer:
            out_writer.write(annotated_frame)
        
        # í™”ë©´ í‘œì‹œ (headlessê°€ ì•„ë‹Œ ê²½ìš°)
        if not headless:
            cv2.imshow('Advanced Security System', annotated_frame)
            
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q') or key == 27:  # 'q' ë˜ëŠ” ESC
                break
        
        frame_count += 1
    
    print(f"\n\në¶„ì„ ì™„ë£Œ! ì´ {frame_count} í”„ë ˆì„ ì²˜ë¦¬ë¨.")
    
    # ì •ë¦¬
    cap.release()
    if out_writer:
        out_writer.release()
    if not headless:
        cv2.destroyAllWindows()
    
    # ìµœì¢… í†µê³„ ì¶œë ¥
    print("\nğŸ“Š ë¶„ì„ í†µê³„:")
    print(f"  ì¶”ì ëœ ê°ì²´ ìˆ˜: {len(security_system.track_history)}")
    print(f"  ê°ì§€ëœ ì´ë²¤íŠ¸ ìˆ˜: {len(security_system.event_log)}")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(
        description="ê³ ê¸‰ AI ë³´ì•ˆ ì‹œìŠ¤í…œ",
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  ì¼ë°˜ ì‹¤í–‰: python security_advanced.py
  ë¹„ë””ì˜¤ íŒŒì¼: python security_advanced.py -v video.mp4
  WSL/í—¤ë“œë¦¬ìŠ¤: python security_advanced.py -v video.mp4 --headless
        """,
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument('-v', '--video', help='ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--headless', action='store_true', 
                       help='í™”ë©´ í‘œì‹œ ì—†ì´ ì‹¤í–‰ (WSL/ì„œë²„ í™˜ê²½ìš©)')
    
    args = parser.parse_args()
    
    # WSL í™˜ê²½ì—ì„œ headless ëª¨ë“œ ìë™ ê¶Œì¥
    if 'WSL_DISTRO_NAME' in os.environ and not args.headless and args.video:
        print("\nâš ï¸  WSL í™˜ê²½ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("GUI í‘œì‹œ ë¬¸ì œë¥¼ í”¼í•˜ë ¤ë©´ --headless ì˜µì…˜ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤:")
        print(f"python {sys.argv[0]} -v {args.video} --headless\n")
        
        response = input("ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
        if response.lower() != 'y':
            print("--headless ì˜µì…˜ê³¼ í•¨ê»˜ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            sys.exit(0)
    
    try:
        run_advanced_security_system(video_source=args.video, headless=args.headless)
    except Exception as e:
        print(f"\nì˜¤ë¥˜ ë°œìƒ: {e}")
        if "qt.qpa.plugin" in str(e).lower() or "xcb" in str(e).lower():
            print("\nğŸ’¡ í•´ê²° ë°©ë²•: --headless ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”")
            print(f"   python {sys.argv[0]} -v {args.video} --headless")