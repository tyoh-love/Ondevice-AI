# yolo_models.py - YOLO ëª¨ë¸ ê´€ë¦¬ ë° ìµœì í™”

import torch
import torch.nn as nn
from ultralytics import YOLO
import numpy as np
from PIL import Image
import requests
import os
from pathlib import Path

class YOLOModelLoader:
    """YOLO ëª¨ë¸ ë¡œë”© ë° ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, explain_mode=False):
        self.explain_mode = explain_mode
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.available_models = {
            'yolov8n': 'yolov8n.pt',  # nano - ê°€ì¥ ì‘ìŒ
            'yolov8s': 'yolov8s.pt',  # small
            'yolov8m': 'yolov8m.pt',  # medium
            'yolov8l': 'yolov8l.pt',  # large
            'yolov8x': 'yolov8x.pt'   # extra large - ê°€ì¥ í¼
        }
        
        if self.explain_mode:
            print("ğŸ“‹ YOLO ëª¨ë¸ ì •ë³´:")
            print("  â€¢ YOLOv8n: 3.2M íŒŒë¼ë¯¸í„°, 6.2MB")
            print("  â€¢ YOLOv8s: 11.2M íŒŒë¼ë¯¸í„°, 21.4MB")
            print("  â€¢ YOLOv8m: 25.9M íŒŒë¼ë¯¸í„°, 49.7MB")
            print("  â€¢ YOLOv8l: 43.7M íŒŒë¼ë¯¸í„°, 83.7MB")
            print("  â€¢ YOLOv8x: 68.2M íŒŒë¼ë¯¸í„°, 130.5MB")
    
    def load_model(self, model_name='yolov8n', load_state_dict=True):
        """YOLO ëª¨ë¸ ë¡œë”©"""
        if model_name not in self.available_models:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model_name}")
        
        model_path = self.available_models[model_name]
        
        if self.explain_mode:
            print(f"\nğŸ”„ {model_name} ëª¨ë¸ ë¡œë”© ì¤‘...")
            print(f"  â€¢ ëª¨ë¸ íŒŒì¼: {model_path}")
            print(f"  â€¢ ë””ë°”ì´ìŠ¤: {self.device}")
        
        # YOLO ëª¨ë¸ ë¡œë“œ
        model = YOLO(model_path)
        
        if self.explain_mode:
            print(f"  âœ… {model_name} ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
        
        return model
    
    def get_model_info(self, model):
        """ëª¨ë¸ ì •ë³´ ì¶”ì¶œ"""
        # PyTorch ëª¨ë¸ ì¶”ì¶œ
        pytorch_model = model.model
        
        # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
        total_params = sum(p.numel() for p in pytorch_model.parameters())
        trainable_params = sum(p.numel() for p in pytorch_model.parameters() if p.requires_grad)
        
        # ëª¨ë¸ í¬ê¸° ê³„ì‚° (MB)
        model_size_mb = total_params * 4 / (1024 * 1024)  # 32-bit float
        
        info = {
            'total_params': total_params,
            'trainable_params': trainable_params,
            'model_size_mb': model_size_mb,
            'device': next(pytorch_model.parameters()).device
        }
        
        if self.explain_mode:
            print(f"\nğŸ“Š ëª¨ë¸ ì •ë³´:")
            print(f"  â€¢ ì´ íŒŒë¼ë¯¸í„°: {total_params:,}")
            print(f"  â€¢ í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {trainable_params:,}")
            print(f"  â€¢ ëª¨ë¸ í¬ê¸°: {model_size_mb:.2f} MB")
            print(f"  â€¢ ë””ë°”ì´ìŠ¤: {info['device']}")
        
        return info
    
    def create_sample_images(self, num_images=10):
        """ìƒ˜í”Œ ì´ë¯¸ì§€ ìƒì„± (COCO ë°ì´í„°ì…‹ì—ì„œ ë‹¤ìš´ë¡œë“œ)"""
        sample_urls = [
            "https://ultralytics.com/images/bus.jpg",
            "https://ultralytics.com/images/zidane.jpg",
            "https://images.unsplash.com/photo-1544947950-fa07a98d237f?w=640",
            "https://images.unsplash.com/photo-1517849845537-4d257902454a?w=640",
            "https://images.unsplash.com/photo-1552053831-71594a27632d?w=640"
        ]
        
        # ìƒ˜í”Œ ì´ë¯¸ì§€ ë””ë ‰í„°ë¦¬ ìƒì„±
        sample_dir = Path("sample_images")
        sample_dir.mkdir(exist_ok=True)
        
        images = []
        for i, url in enumerate(sample_urls[:num_images]):
            try:
                img_path = sample_dir / f"sample_{i+1}.jpg"
                
                if not img_path.exists():
                    if self.explain_mode:
                        print(f"  ğŸ“¥ ìƒ˜í”Œ ì´ë¯¸ì§€ {i+1} ë‹¤ìš´ë¡œë“œ ì¤‘...")
                    
                    response = requests.get(url, stream=True)
                    with open(img_path, 'wb') as f:
                        for chunk in response.iter_content(chunk_size=8192):
                            f.write(chunk)
                
                # ì´ë¯¸ì§€ ë¡œë“œ
                img = Image.open(img_path)
                images.append(str(img_path))
                
            except Exception as e:
                if self.explain_mode:
                    print(f"  âš ï¸ ì´ë¯¸ì§€ {i+1} ë¡œë“œ ì‹¤íŒ¨: {e}")
                continue
        
        if self.explain_mode:
            print(f"  âœ… {len(images)}ê°œ ìƒ˜í”Œ ì´ë¯¸ì§€ ì¤€ë¹„ ì™„ë£Œ!")
        
        return images
    
    def benchmark_inference(self, model, images, runs=10):
        """ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬"""
        if self.explain_mode:
            print(f"\nâ±ï¸ ì¶”ë¡  ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ({runs}íšŒ ì‹¤í–‰)...")
        
        # ì›Œë°ì—…
        for _ in range(3):
            model(images[0], verbose=False)
        
        # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        import time
        times = []
        
        for i in range(runs):
            start_time = time.time()
            
            for img_path in images:
                results = model(img_path, verbose=False)
            
            end_time = time.time()
            times.append(end_time - start_time)
            
            if self.explain_mode and (i + 1) % 3 == 0:
                print(f"    - ì‹¤í–‰ {i+1}/{runs} ì™„ë£Œ...")
        
        avg_time = np.mean(times)
        std_time = np.std(times)
        fps = len(images) / avg_time
        
        benchmark_results = {
            'avg_time_per_batch': avg_time,
            'std_time': std_time,
            'fps': fps,
            'images_per_batch': len(images)
        }
        
        if self.explain_mode:
            print(f"\nğŸ“Š ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼:")
            print(f"  â€¢ í‰ê·  ë°°ì¹˜ ì²˜ë¦¬ ì‹œê°„: {avg_time:.3f}ì´ˆ")
            print(f"  â€¢ í‘œì¤€í¸ì°¨: {std_time:.3f}ì´ˆ")
            print(f"  â€¢ FPS: {fps:.1f}")
            print(f"  â€¢ ì´ë¯¸ì§€ ìˆ˜: {len(images)}")
        
        return benchmark_results

class YOLOOptimizer:
    """YOLO ëª¨ë¸ ìµœì í™” í´ë˜ìŠ¤"""
    
    def __init__(self, explain_mode=False):
        self.explain_mode = explain_mode
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    def prune_yolo_model(self, model, sparsity=0.5):
        """YOLO ëª¨ë¸ ê°€ì§€ì¹˜ê¸°"""
        if self.explain_mode:
            print(f"\nâœ‚ï¸ YOLO ëª¨ë¸ ê°€ì§€ì¹˜ê¸° (í¬ì†Œì„±: {sparsity*100}%)")
            print("  ğŸ“‹ YOLO ëª¨ë¸ì€ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ ì¤‘ì‹¬ êµ¬ì¡°")
            print("  ğŸ“‹ êµ¬ì¡°ì  ê°€ì§€ì¹˜ê¸°ë¡œ ì±„ë„ ë‹¨ìœ„ ì œê±°")
        
        pytorch_model = model.model
        
        # ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ ì°¾ê¸°
        conv_layers = []
        for name, module in pytorch_model.named_modules():
            if isinstance(module, nn.Conv2d):
                conv_layers.append((name, module))
        
        if self.explain_mode:
            print(f"  ğŸ” ë°œê²¬ëœ Conv2d ë ˆì´ì–´: {len(conv_layers)}ê°œ")
        
        # ê°€ì§€ì¹˜ê¸° ì ìš© (ê°„ë‹¨í•œ magnitude-based pruning)
        import torch.nn.utils.prune as prune
        
        pruned_layers = 0
        for name, module in conv_layers:
            if module.weight.numel() > 100:  # ì‘ì€ ë ˆì´ì–´ëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŒ
                prune.l1_unstructured(module, name='weight', amount=sparsity)
                pruned_layers += 1
                
                if self.explain_mode and pruned_layers <= 5:
                    print(f"    â€¢ {name}: {module.weight.shape} ê°€ì§€ì¹˜ê¸° ì ìš©")
        
        # ê°€ì§€ì¹˜ê¸° ì˜êµ¬ ì ìš©
        for name, module in conv_layers:
            if hasattr(module, 'weight_mask'):
                prune.remove(module, 'weight')
        
        if self.explain_mode:
            print(f"  âœ… {pruned_layers}ê°œ ë ˆì´ì–´ì— ê°€ì§€ì¹˜ê¸° ì™„ë£Œ!")
        
        return model
    
    def knowledge_distillation_yolo(self, teacher_model, student_model_name='yolov8n', images=None, epochs=5):
        """YOLO ì§€ì‹ ì¦ë¥˜"""
        if self.explain_mode:
            print(f"\nğŸ“ YOLO ì§€ì‹ ì¦ë¥˜")
            print(f"  â€¢ êµì‚¬ ëª¨ë¸: í° YOLO ëª¨ë¸")
            print(f"  â€¢ í•™ìƒ ëª¨ë¸: {student_model_name}")
        
        # í•™ìƒ ëª¨ë¸ ë¡œë“œ
        loader = YOLOModelLoader(explain_mode=self.explain_mode)
        student_model = loader.load_model(student_model_name)
        
        if self.explain_mode:
            print(f"  ğŸ“š ì§€ì‹ ì¦ë¥˜ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ í•„ìš”")
            print(f"  ğŸ“š í˜„ì¬ëŠ” ëª¨ë¸ í¬ê¸° ë¹„êµ ì‹œì—°")
        
        # êµì‚¬ì™€ í•™ìƒ ëª¨ë¸ ì •ë³´ ë¹„êµ
        teacher_info = loader.get_model_info(teacher_model)
        student_info = loader.get_model_info(student_model)
        
        if self.explain_mode:
            print(f"\nğŸ“Š í¬ê¸° ë¹„êµ:")
            print(f"  â€¢ êµì‚¬ ëª¨ë¸: {teacher_info['model_size_mb']:.2f} MB")
            print(f"  â€¢ í•™ìƒ ëª¨ë¸: {student_info['model_size_mb']:.2f} MB")
            reduction = (teacher_info['model_size_mb'] - student_info['model_size_mb']) / teacher_info['model_size_mb'] * 100
            print(f"  â€¢ í¬ê¸° ê°ì†Œ: {reduction:.1f}%")
        
        return student_model

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    loader = YOLOModelLoader(explain_mode=True)
    
    # ëª¨ë¸ ë¡œë“œ
    model = loader.load_model('yolov8n')
    
    # ëª¨ë¸ ì •ë³´ ì¶œë ¥
    info = loader.get_model_info(model)
    
    # ìƒ˜í”Œ ì´ë¯¸ì§€ ìƒì„±
    images = loader.create_sample_images(3)
    
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    results = loader.benchmark_inference(model, images, runs=3)