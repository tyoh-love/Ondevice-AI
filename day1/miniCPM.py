
# MiniCPM-V 2.6 ì‹¤ìŠµ
# ì£¼ì˜: ì´ ëª¨ë¸ì€ ë” ë§ì€ ë©”ëª¨ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤

from transformers import AutoModel, AutoTokenizer
import torch
import numpy as np
from PIL import Image
from decord import VideoReader, cpu
import matplotlib.pyplot as plt

class VideoUnderstandingAI:
    """ë¹„ë””ì˜¤ë¥¼ ì´í•´í•˜ëŠ” ë˜‘ë˜‘í•œ AI"""
    
    def __init__(self):
        """MiniCPM-V 2.6 ëª¨ë¸ ì´ˆê¸°í™”"""
        print("ğŸ¬ ë¹„ë””ì˜¤ ì´í•´ AIë¥¼ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        
        model_name = "openbmb/MiniCPM-V-2_6"
        
        # í† í¬ë‚˜ì´ì €: í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True  # ì»¤ìŠ¤í…€ ì½”ë“œ ì‹¤í–‰ í—ˆìš©
        )
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = AutoModel.from_pretrained(
            model_name,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32
        )
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = self.model.to(self.device)
        
        print(f"âœ… ë¹„ë””ì˜¤ AI ì¤€ë¹„ ì™„ë£Œ! (ë””ë°”ì´ìŠ¤: {self.device})")
    
    def extract_frames(self, video_path, num_frames=8):
        """ë¹„ë””ì˜¤ì—ì„œ ê· ë“±í•˜ê²Œ í”„ë ˆì„ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
        print(f"ğŸï¸ ë¹„ë””ì˜¤ì—ì„œ {num_frames}ê°œì˜ í”„ë ˆì„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤...")
        
        # ë¹„ë””ì˜¤ ì½ê¸°
        vr = VideoReader(video_path, ctx=cpu(0))
        total_frames = len(vr)
        
        # ê· ë“±í•œ ê°„ê²©ìœ¼ë¡œ í”„ë ˆì„ ì„ íƒ
        indices = np.linspace(0, total_frames-1, num_frames, dtype=int)
        
        frames = []
        for idx in indices:
            frame = vr[idx].asnumpy()
            frame_pil = Image.fromarray(frame)
            frames.append(frame_pil)
        
        return frames, indices, total_frames
    
    def analyze_video(self, video_path, question="ì´ ë¹„ë””ì˜¤ì—ì„œ ë¬´ìŠ¨ ì¼ì´ ì¼ì–´ë‚˜ê³  ìˆë‚˜ìš”?"):
        """ë¹„ë””ì˜¤ë¥¼ ë¶„ì„í•˜ê³  ì§ˆë¬¸ì— ë‹µí•˜ëŠ” í•¨ìˆ˜"""
        
        # 1. í”„ë ˆì„ ì¶”ì¶œ
        frames, indices, total_frames = self.extract_frames(video_path)
        
        # 2. í”„ë ˆì„ ì‹œê°í™”
        self.visualize_frames(frames, indices, total_frames)
        
        # 3. ëª¨ë¸ì— ì…ë ¥
        print(f"\nğŸ¤” AIê°€ ë¹„ë””ì˜¤ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        print(f"ì§ˆë¬¸: {question}")
        
        # ì…ë ¥ ì¤€ë¹„
        msgs = [
            {
                'role': 'user',
                'content': [
                    *[{'type': 'image', 'image': frame} for frame in frames],
                    {'type': 'text', 'text': question}
                ]
            }
        ]
        
        # ì¶”ë¡ 
        with torch.no_grad():
            answer = self.model.chat(
                image=None,
                msgs=msgs,
                tokenizer=self.tokenizer,
                max_new_tokens=200,
                temperature=0.7
            )
        
        print(f"\nğŸ¤– AIì˜ ë‹µë³€: {answer}")
        
        return answer, frames
    
    def visualize_frames(self, frames, indices, total_frames):
        """ì¶”ì¶œëœ í”„ë ˆì„ì„ ì‹œê°í™”í•˜ëŠ” í•¨ìˆ˜"""
        fig, axes = plt.subplots(2, 4, figsize=(12, 6))
        axes = axes.ravel()
        
        for i, (frame, idx) in enumerate(zip(frames, indices)):
            axes[i].imshow(frame)
            axes[i].set_title(f'í”„ë ˆì„ {idx}/{total_frames}')
            axes[i].axis('off')
        
        plt.suptitle('ë¹„ë””ì˜¤ì—ì„œ ì¶”ì¶œí•œ í”„ë ˆì„ë“¤', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.show()

    def analyze_image(self, image_path, question="ì´ ì´ë¯¸ì§€ì— ë¬´ì—‡ì´ ë³´ì´ë‚˜ìš”?"):
        """ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ê³  ì§ˆë¬¸ì— ë‹µí•˜ëŠ” í•¨ìˆ˜"""
        print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤: {image_path}")
        
        # 1. ì´ë¯¸ì§€ ë¡œë“œ
        try:
            image = Image.open(image_path).convert('RGB')
        except FileNotFoundError:
            print(f"ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - {image_path}")
            return None, None

        # 2. ëª¨ë¸ì— ì…ë ¥
        print(f"\nğŸ¤” AIê°€ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        print(f"ì§ˆë¬¸: {question}")

        msgs = [{'role': 'user', 'content': [image, question]}]

        # ì¶”ë¡ 
        with torch.no_grad():
            answer = self.model.chat(
                image=image,
                msgs=msgs,
                tokenizer=self.tokenizer,
                max_new_tokens=200,
                temperature=0.7
            )

        print(f"\nğŸ¤– AIì˜ ë‹µë³€: {answer}")
        
        # ì´ë¯¸ì§€ ì‹œê°í™”
        plt.imshow(image)
        plt.title("ë¶„ì„ëœ ì´ë¯¸ì§€")
        plt.axis('off')
        plt.show()
        
        return answer, image

# ì‚¬ìš© ì˜ˆì‹œ
video_ai = VideoUnderstandingAI()

# --- ì´ë¯¸ì§€ ë¶„ì„ ì˜ˆì œ ---
image_path = "cat1.jpeg"
image_question = "describe this image in detail like position, color, kind etc."
video_ai.analyze_image(image_path, image_question)

# --- ë¹„ë””ì˜¤ ë¶„ì„ ì˜ˆì œ ---
# video_path = "./v09.mp4"
# video_question = "ì´ ë¹„ë””ì˜¤ì—ì„œ ë¬´ìŠ¨ ì¼ì´ ì¼ì–´ë‚˜ê³  ìˆë‚˜ìš”?"
# video_ai.analyze_video(video_path, video_question)

